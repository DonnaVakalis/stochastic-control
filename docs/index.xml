<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/</link>
    <description>Recent content on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/lqr/</guid>
      <description>Note: To be consistent with the notation used in linear systems, we denote the state and action by lowercase \(x\) and \(u\), even for stochastic systems (unlike the notation used for other models where we use uppercase \(X\) and \(U\) for state and actions to emphasize the fact they are random variables).
We start by considering a determinisitc linear system with state \(x_t \in \reals^n\) and control actions \(u_t \in \reals^m\) and dynamics \[ x_{t+1} = A_t x_t + B_t u_t,\] where \(A_t \in \reals^{n \times n}\) and \(B_t \in \reals^{n \times m}\) are known matrices.</description>
    </item>
    
    <item>
      <title>Overview of adaptive control for linear systems</title>
      <link>https://adityam.github.io/stochastic-control/adaptive-control/linear-adaptive-control/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/adaptive-control/linear-adaptive-control/</guid>
      <description>Adaptive control is a umbrella term which is often used to describe algorithms which are able to control an unknown system (i.e., a system with unknown dynamics). In this section, we focus on adaptive control of linear systems. The simplest setup is as follows.
Consider a standard linear quadratic regulator where the dynamics are \[ x_{t+1} = A_θ x_t + B_θ u_t + w_t \] where the matrices \(A_θ\) and \(B_θ\) are parameterized by a parameter \(θ\) which takes values in a set \(Θ\).</description>
    </item>
    
    <item>
      <title>Prelim: Risk Sensitive Utility</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-utility/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-utility/</guid>
      <description>Risk sensitivity is relative to the idea of utility. The value of a sum of money \(z\) to a decision maker may not be proportional to \(z\) itself but may be some general increasing function \(\mathsf{U}(z)\), known as the utility function. For example, in the example on optimal gambling considered earlier, we had assumed that the utility for wealth \(z\) is \(\log z\). If a decision maker has utility function \(\mathsf{U}\), then the value of a random outcome \(Z\) will be defined by the expected utility \(\EXP[\mathsf{U}(Z)]\).</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic approximation</title>
      <link>https://adityam.github.io/stochastic-control/rl/stochastic-approximation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/rl/stochastic-approximation/</guid>
      <description>Suppose we have a function \(f \colon \reals^d \to \reals^d\) is such that the equation \[ f(θ) = 0 \] has a unique root \(θ = θ^*\). There are many methods for determining the value of \(θ\) by successive approximation where start with an initial guess \(θ_0\) and then recursively obtain a new value \(θ_{k+1}\) as a function of the previously obtained \(θ_0, \dots, θ_{k}\), the values \(f(θ_1), \dots, f(θ_{k})\), and possibly those of the derivatives \(f&amp;#39;(θ_0), \dots, f&amp;#39;(θ_{k})\), etc.</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic optimization</title>
      <link>https://adityam.github.io/stochastic-control/stochastic/stochastic-optimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/stochastic/stochastic-optimization/</guid>
      <description>Let’s start with the simplest optimization problem. A decision maker has to choose an action \(a \in \ALPHABET A\). Upon choosing the action \(a\), the decision maker incurs a cost \(c(a)\). What action should the decision maker pick to minimize the cost?
Formally, the above optimization problem may be written as \[ \begin{equation} \label{eq:basic} \min_{a \in \ALPHABET A} c(a). \end{equation}\]
When the action space \(\ALPHABET A\) is finite, say \(\ALPHABET A = \{1, \dots, m\}\), solving the optimization problem \eqref{eq:basic} is conceptually straight-forward: enumerate the cost of all possible actions, i.</description>
    </item>
    
    <item>
      <title>Prelim: Strategic Games</title>
      <link>https://adityam.github.io/stochastic-control/games/strategic-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/strategic-games/</guid>
      <description>So far, we have considered models with a single decision maker. As mentioned in the notes on stochastic optimization, the simplest decision problem is as follows. A decision maker has to choose an action \(a \in \ALPHABET A\). Upon choosing the action \(a\), the decision maker incurs a cost \(c(a)\) (or, equivalently, receives a payoff or a utility \(u(a)\)). What action should the decision maker pick to minimize the cost (or, equivalently, maximize the payoff).</description>
    </item>
    
    <item>
      <title>Theory: Basic model of a POMDP</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/pomdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/pomdp/</guid>
      <description>So far, we have considered a setup where the decision maker perfectly observes the state of the system. In many applications, the decision maker may not directly observe the state of the system but only observe a noisy version of it. Such systems are modeled as partially observable Markov decision processes (POMDPs). We will describe the simplest model of POMDPs, which builds upon the model of MDPs descibed earlier.
We assume that the system has a state \(S_t \in \ALPHABET S\), control input \(A_t \in \ALPHABET A\), and process noise \(W_t \in \ALPHABET W\).</description>
    </item>
    
    <item>
      <title>Theory: Infinite horizon discounted MDP</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/discounted-mdp/</guid>
      <description>\(\def\ONES{\mathbb{1}}\)
A common way to approximate systems that run for a very large horizon is to assume that they run for an infinite horizon. There is an inherent homogeneity over time for infinite horizon system: the future depends only on the current state and not on the current time. Due to this homogeneity over time, we expect that the optimal policy should also be time-homogeneous. Therefore, the optimal policy for an infinite-horizon system should be easier to implement than the optimal policy for a finite horizon system, especially so when the horizon is large.</description>
    </item>
    
    <item>
      <title>Theory: Optimality of threshold policies in optimal stopping</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/monotonicity-optimal-stopping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/monotonicity-optimal-stopping/</guid>
      <description>Let \(\{X_t\}_{t \ge 1}\) be a Markov chain. At each time \(t\), a decision maker observes the state \(X_t\) of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a continuation cost \(c_t(X_t)\) and the state evolves. If the DM decides to stop, he incurs a stopping cost of \(s_t(X_t)\) and the problem is terminated. The objective is to determine an optimal stopping time \(\tau\) to minimize \[J(\tau) := \EXP\bigg[ \sum_{t=1}^{\tau-1} c_t(X_t) + s_\tau(X_\tau) \bigg].</description>
    </item>
    
    <item>
      <title>Example: Inventory Management (revisited)</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/inventory-management/</guid>
      <description>TL;DR One of the potential benefits of modeling a system as infinite horizon discounted cost MDP is that it can be simpler to identify an optimal policy. We illustrate this using the inventory management example.
Consider the model for inventory management and assume that it runs for an infinite horizon. We assume that the per-step cost is given by \[c(s,a,s_{+}) = p a + γ h(s), \] where \[ h(s) = \begin{cases} c_h s, &amp;amp; \text{if $s \ge 0$} \\ -c_s s, &amp;amp; \text{if $s &amp;lt; 0$}, \end{cases}\] where \(a\) is the per-unit holding cost, \(b\) is the per-unit shortage cost, and \(p\) is the per-unit procurement cost.</description>
    </item>
    
    <item>
      <title>Example: The Newsvendor Problem</title>
      <link>https://adityam.github.io/stochastic-control/stochastic/newsvendor/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/stochastic/newsvendor/</guid>
      <description>Image credit: https://americangallery.wordpress.com/category/cafferty-james-h/ TL;DR The newsvendor problem is a simple model of stochastic optimization problem where a decision has to be made when there is uncertainty about the outcome. It also shows that for some stochastic optimization problems it is possible to obtain the qualitative properties of the nature of optimal solution.
Each morning, a newsvendor has to decide how many newspapers to buy before knowing the demand during the day.</description>
    </item>
    
    <item>
      <title>Infinite horizon Linear Quadratic Regulation (LQR)</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/inf-lqr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/inf-lqr/</guid>
      <description>1 Preliminaries We first start with some properties of deterministic time-homogeous linear system: \[\begin{equation} \label{eq:simple} x_{t+1} = A x_{t-1}. \end{equation}\] The solution to this system is given by \(x_t = A^t x_0\). It obviously has an equilibrium point of \(x = 0\). This will be the unique equilibrium point if \(A\) is non-singular, when the only solution of \(x = Ax\) is \(x = 0\). Suppose this is true. One may now ask whether this equilibrium is stable in that \(x_t \to 0\) with increasing \(t\) for any \(x_0\).</description>
    </item>
    
    <item>
      <title>Theory: Q-learning</title>
      <link>https://adityam.github.io/stochastic-control/rl/q-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/rl/q-learning/</guid>
      <description>Consider an MDP with finite state space \(\ALPHABET S\) and action space \(\ALPHABET A\), where \(|\ALPHABET S| = n\) and \(|\ALPHABET A| = m\). Recall the value iteration algorithm for infinite horizon MDPs. We start with an arbitrary initial \(V_0\) and then recursively compute \(V_{k+1} = \mathcal B V_k\). We first observe that the value iteration can be written in terms of the \(Q\)-function rather than the value function as follows: start with an arbitrary initial \(Q_0\) and the recursively compute \[ \begin{equation} \label{eq:Q} Q_{k+1}(s,a) = c(s,a) + γ \sum_{s&amp;#39; \in \ALPHABET S} P_{ss&amp;#39;}(a) \min_{a&amp;#39; \in \ALPHABET A} Q_k(s&amp;#39;,a&amp;#39;), \qquad s \in \ALPHABET S, a \in \ALPHABET A, k \ge 0 \end{equation} \]</description>
    </item>
    
    <item>
      <title>Theory: Risk Sensitive MDP</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/risk-sensitive-mdp/</guid>
      <description>1 Finite horizon setup Consider the matrix formulation of an MDP with state space \(\ALPHABET X\), action space \(\ALPHABET U\), per-step cost \(c \colon \ALPHABET X × \ALPHABET U \to \reals\), and controlled transition matrix \(P\). However, instead of the risk neutral optimization criteria that we consider previously, we consider a risk-sensitive objective. In particular, the performance of any (possibly non-Markovian) strategy \(g = (g_1, \dots, g_T)\) is given by \[ \bar J_θ(g) = \frac{1}{θ} \log \EXP\Bigl[ \exp\Bigl( θ \sum_{t=1}^T c(X_t, U_t) \Bigr) \Bigr].</description>
    </item>
    
    <item>
      <title>Example: Service Migration in Mobile Edge Computing</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/service-migration-in-mec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/service-migration-in-mec/</guid>
      <description>TL;DR Another benefit of infinite horizon models is that it is possible to prove structural properties of the optimal policy which might not hold for a finite horizon model. We illustrate this using a model for service migration in mobile edge computing.
There are many mobile applications which consist of a front-end component running on a mobile device and a back-end component running on a cloud, where the cloud provides additional data processing and computing resources.</description>
    </item>
    
    <item>
      <title>Example: Sequential hypothesis testing</title>
      <link>https://adityam.github.io/stochastic-control/pomdp/sequential-hypothesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/pomdp/sequential-hypothesis/</guid>
      <description>Consider a decision maker (DM) that makes a series of i.i.d. observations which may be distributed according to PDF \(f_0\) or \(f_1\). Let \(Y_t\) denote the observaion at time \(t\). The DM wants to differentiate between two hypothesis: \[\begin{gather*} h_0 : Y_t \sim f_0 \\ h_1 : Y_t \sim f_1 \end{gather*}\] Typically, we think of \(h_0\) as the normal situation (or the null hypothesis) and \(h_1\) as an anomaly. For example, the hypothesis may be \[ h_0: Y_t \sim {\cal N}(0, σ^2) \quad h_1: Y_t \sim {\cal N}(μ, σ^2) \] or \[ h_0: Y_t \sim \text{Ber}(p) \quad h_1: Y_t \sim \text{Ber}(q).</description>
    </item>
    
    <item>
      <title>Partially observed linear quadratic regulator</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/lqg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/lqg/</guid>
      <description>Consider a stochastic linear system as in the case of LQR. The system has state \(x_t \in \reals^n\) and actions \(u_t \in \reals^m\). The initial state \(x_1\) has zero mean and finite variance \(\Sigma^x_1\). The system dynamics are given by \[ x_{t+1} = A_t x_t + B_t u_t + w_t, \] where \(A_t \in \reals^{n×n}\) and \(B_t \in \reals^{n×m}\) are known matrices and \(\{w_t\}_{t \ge 1}\) is \(\reals^n\)-valued i.i.d. noise process with zero mean and finite variance \(\Sigma^w\).</description>
    </item>
    
    <item>
      <title>Theory: Computational complexity of value iteration</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/complexity-vi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/complexity-vi/</guid>
      <description>How many computations are needed to run the value or policy iteration algorithm to obtain a policy that in within \(ε\) of the optimal? In this section, we provide an answer for this question for the value iteration algorithm.
Conisder an MDP with a finite state space \(\ALPHABET S = \{1, \dots, n \}\) with a finite non-empty set of actions \(\ALPHABET A(s)\) available at each \(s \in \ALPHABET S\). Each action set consists of \(M_s\) actions with a total number of \(M = \sum_{s=1}^n M_s\) actions.</description>
    </item>
    
    <item>
      <title>Theory: Sample complexity</title>
      <link>https://adityam.github.io/stochastic-control/rl/sample-complexity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/rl/sample-complexity/</guid>
      <description>An important consideration in model-based RL is to determine how many samples are needed to identify an \(α\)-approximate solution (for a pre-specified accuracy level \(α\)). This is known as sample complexity of learning and is typically analyzed under the assumption that the learning agent has access to a generative model, i.e., a black box simulator that takes the current state and action profile as input and generates samples of the next state as output.</description>
    </item>
    
    <item>
      <title>Theory: Linear Programming Formulation and Constrained MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/linear-programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/linear-programming/</guid>
      <description>Note: Throughout this section, we assume that \(\ALPHABET S\) and \(\ALPHABET A\) are finite and \(|\ALPHABET S|= n\) and \(|\ALPHABET A| = m\).
We know that if \(V \le \mathcal B V\) then \(V \le V^* = \mathcal B V^*\). Thus, \(V^*\) is the “largest” \(V\) that satisfies the constraint \(V \le \mathcal B V\). This constraint can be written as a finite system of linear equations: \[\begin{equation} \label{eq:constaints} V(s) \le c(s,a) + γ\sum_{z \in \ALPHABET S} P(z|s,a) V(z), \qquad s \in \ALPHABET S, a \in \ALPHABET A.</description>
    </item>
    
    <item>
      <title>Theory: A Martingale Principle of Optimal Control</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/martingale-approach/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/martingale-approach/</guid>
      <description>Consider a discounted cost MDP (with bounded rewards) and fix a Markov policy \(g \colon \ALPHABET X \to \reals\). Suppose for a bounded function \(Φ \colon \ALPHABET X \to \reals\), we define a process \(\{M_t\}_{t \ge 1}\) starting at \(M_1 = 0\) and its increments \(ΔM_t = M_{t+1} - M_t\) given by \[ ΔM_t = c(X_t, g(X_t)) + β Φ(X_{t+1}) - Φ(X_t). \]
Proposition If \(\{M_t\}_{t\ge1}\) is a submartingale for all policies \(g\) and, for some policy \(g^*\), \(\{M_t\}_{t \ge 1}\) is a martingale, then \(g^*\) is an optimal policy and \(Φ(x) = V^{g^*}(x) = V(x)\).</description>
    </item>
    
    <item>
      <title>Example: Optimal choice of the best alternative</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/optimal-choice/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/optimal-choice/</guid>
      <description>A decision maker (DM) wants to select the best alternative from a set of \(T\) alternatives. The DM evaluates the alternatives sequentially. After evaluating alternative \(t\), the DM knows whether alternative \(t\) was the best alternative so far or not. Based on this information, the DM must decide whether to choose alternative \(t\) and stop the search, or to permanently reject alternative \(t\) and evaluate remaining alternatives. The DM may reject the last alternative and not make a choice at all.</description>
    </item>
    
    <item>
      <title>Theory: Basic model of an MDP</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-functional/</guid>
      <description>Markov decision processes (MDP) are the simplest model of a stochastic control system. The dynamic behavior of an MDP is modeled by an equation of the form \[ \begin{equation} S_{t+1} = f_t(S_t, A_t, W_t) \label{eq:state} \end{equation}\] where \(S_t \in \ALPHABET S\) is the state, \(A_t \in \ALPHABET A\) is the control input, and \(W_t \in \ALPHABET W\) is the noise. An agent/controller observes the state and chooses the control input \(A_t\).</description>
    </item>
    
    <item>
      <title>Example: Call options</title>
      <link>https://adityam.github.io/stochastic-control/optimal-stopping/call-options/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/optimal-stopping/call-options/</guid>
      <description>An investor has a call option to buy one share of a stock at a fixed price \(p\) dollars and has \(T\) days to exercise this option. For simplicity, we assume that the investor makes a decision at the beginning of each day.
The investory may decide not to exercise the option but if he does exercise the option when the stock price is \(x\), he effectively gets \((x-p)\).
Assume that the price of the stoch varies with independent increments, i.</description>
    </item>
    
    <item>
      <title>Prelim: Change of Measure</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/change-of-measure/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/change-of-measure/</guid>
      <description>1 Change of measure of a single random variable. Theorem 1 Let \((\Omega, \mathcal F, P)\) be a probability space and \(\Lambda\) be an almost surely non-negative random variable such that \(\EXP[\Lambda] = 1\). For any \(A \in \mathcal F\), define \[ P^\dagger(A) = \int_A \Lambda(\omega) dP(\omega). \] Then,
\(P^\dagger\) is a probability measure. For any random variable \(X\), \[ \EXP^\dagger[X] = \EXP[ \Lambda X]. \] If \(\Lambda\) is almost surely positive, then \[ \EXP[X] = \EXP^\dagger \left[ \frac{X}{\Lambda} \right].</description>
    </item>
    
    <item>
      <title>Theory: Lipschitz MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/lipschitz-mdp/</guid>
      <description>1 Preliminaries 1.1 Lipschitz continuous functions Given two metric spaces \((\ALPHABET X, d_X)\) and \((\ALPHABET Y, d_Y)\), the Lipschitz constant of function \(f \colon \ALPHABET X \to \ALPHABET Y\) is defined by \[ \| f\|_{L} = \sup_{x_1 \neq x_2} \left\{ \frac{ d_Y(f(x_1), f(x_2)) } { d_X(x_1, x_2) } : x_1, x_2 \in \ALPHABET X \right\} \in [0, ∞]. \] The function is called Lipschitz continuous if its Lipschitz constant is finite.</description>
    </item>
    
    <item>
      <title>Theory: Security and zero-sum games</title>
      <link>https://adityam.github.io/stochastic-control/games/zero-sum-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/zero-sum-games/</guid>
      <description>As we saw, the notion of rationalizability is attractive, but it makes strong assumptions about other players and does not always prescribe a solution. We will now describe a notion of rationality of a player that does not rely on rationality of other players, and even makes the most pessimistic assessment of their potential behavior.
As an example, consider the game shown below.
\(\mathsf{L}\) \(\mathsf{R}\) \(\mathsf{T}\) \(2\) \(1\) \(2\) \(-20\) \(\mathsf{M}\) \(3\) \(0\) \(-10\) \(1\) \(\mathsf{B}\) \(-100\) \(2\) \(3\) \(3\) In this game \(\mathsf{T}\) is an attractive strategy for the row player because it guarantees a payoff of \(2\) and also ensures that the “risky” payoffs of \(-10\) and \(-100\) are avoided.</description>
    </item>
    
    <item>
      <title>Theory: State aggregation or discretization or quantization</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/state-aggregation/</guid>
      <description>So far, we have studied exact solutions to the dynamic program. When the state space is large (or possibly continuous), an exact solution is not possible due to computational limitations. So, we need to look at approximate solutions.
The simplest form of approximate solution is state aggregation, in which we partition the state space into equivalence classes and assign one state in each class as a representative element of that class.</description>
    </item>
    
    <item>
      <title>Example: Jamming Games</title>
      <link>https://adityam.github.io/stochastic-control/games/jamming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/jamming/</guid>
      <description>Radio frequenccy (RF) jamming is the act of blocking or causing interference to radio or wireless communication by transmitting noise to decrease the signal to noise ration. Jamming of radio transmission originated in World War II and is even used today in military and civilian conflicts. See wikipedia article on jamming for details.
We will consider a simple model of jamming and obtain a using using tools from zero sum games.</description>
    </item>
    
    <item>
      <title>Example: Optimal Gambling</title>
      <link>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/optimal-gambling/</guid>
      <description>Image credit: http://commons.wikimedia.org/wiki/File:Gambling-ca-1800.jpg TL;DR This stylized model of optimal gambling was introduced by Kelly (1956) to highlight a relationship between channel capacity (which had been proposed recently by Shannon), and gambling. Our motivation for studying this model is to use it as an illustrative example to show that sometimes it is possible to identify the optimal strategy and value function of MDPs in closed form.
Imagine a gambler who goes to a casino with an initial fortune of \(s_1\) dollars and places bets over time and must leave after \(T\) bets.</description>
    </item>
    
    <item>
      <title>Theory: Feature abstraction in MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/feature-abstraction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/feature-abstraction/</guid>
      <description>Consider an MDP with continuous state space \(\ALPHABET X\) and finite action space \(\ALPHABET U\). We denote this MDP by \(M = (\ALPHABET X, \ALPHABET U, c, p)\), where for simplicity we assume that the \(p\) is the density of the transition kernel.
Since the state space is continuous, in general, we cannot compute the value functions exactly. The simplest way to proceed is to discretize the state space \(\ALPHABET X\).</description>
    </item>
    
    <item>
      <title>Example: Inventory Management</title>
      <link>https://adityam.github.io/stochastic-control/mdp/inventory-management/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/inventory-management/</guid>
      <description>Image credit: https://hbr.org/2015/06/inventory-management-in-the-age-of-big-data TL;DR The inventory management example illustrates that a dynamic programming formulation is useful even when a closed form solution does not exist. This model also introduces the idea of post-decision state, which is useful in many contexts.
Imagine a retail store that stockpiles products in its warehouse to meet random demand. Suppose the store procures new stocks at the end of each day (and that there is no lead time and stocks are available next morning).</description>
    </item>
    
    <item>
      <title>Theory: Model approximation in MDPs</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/model-approximation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/model-approximation/</guid>
      <description>There are many instances where we may plan using an approximate model. For example, in many applications with large state spaces, we may construct a simulation model of the system and use it to identify an optimal policy using simulation based methods (such as reinforcement learning). Often, the simulation model is only an approximation of the true model. In such instances, we want to know the error in using the policy obtained from the simulation model in the real world.</description>
    </item>
    
    <item>
      <title>Numerics: Matrix formulation of Markov decision processes</title>
      <link>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/mdp-matrix/</guid>
      <description>Image credit: https://etc.usf.edu/clipart/28400/28480/young_boy_28480.htm 1 MDPs as controlled Markov chains In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with the function model described earlier and assume that \(\ALPHABET S\) and \(\ALPHABET A\) are finite sets and that the cost function and the probability distribution of \(\{W_t\}_{t \ge 1}\) are time-homogeneous. Then, the following is a fundamental property of MDPs:</description>
    </item>
    
    <item>
      <title>Prelim: Stochastic dominance</title>
      <link>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/stochastic-dominance/</guid>
      <description>Stochastic dominance is a partial order on random variables. Let \(\ALPHABET S\) be a totally ordered finite set, say \(\{1, \dots, n\}\) and let \(\Delta(\ALPHABET S)\) denote the state of pmfs on \(\ALPHABET S\).
Definition Suppose \(S^1\) and \(S^2\) are \(\ALPHABET S\) valued random variables where \(S^1 \sim \mu^1\) and \(S^2 \sim \mu^2\). We say \(S^1\) stochastically dominates \(S^2\) if for any \(s \in \ALPHABET S\), \[\begin{equation}\label{eq:inc-prob} \PR(S^1 \ge s) \ge \PR(S^2 \ge s).</description>
    </item>
    
    <item>
      <title>Simplest model of a networked control systems</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/ncs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/ncs/</guid>
      <description>Networked control systems (NCS) refer to systems where there a communication network between different components of the control system. In this section, we consider the simplest setup of a NCS where there is a communication channel between the sensor and the controller. There are multiple ways to model the communication channel and we choose the simplest model—the communication channel is an i.i.d. packet drop channel. The details will be explained later.</description>
    </item>
    
    <item>
      <title>Theory: Approximate dynamic programming</title>
      <link>https://adityam.github.io/stochastic-control/inf-mdp/adp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/inf-mdp/adp/</guid>
      <description>The value and policy iteration algorithms for discounted cost MDPs rely on exact computation of the Bellman update \(W = \mathcal B V\) and the corresponding optimal policy \(g\) such that \(\mathcal B V = \mathcal B_π V\). Suppose we cannot compute these updates exactly, but can find approximate solutions \(W\) and \(g\) such that \[ \NORM{W - \mathcal B V} \le δ \quad\text{and}\quad \NORM{\mathcal B_π V - \mathcal B V} \le ε\] where \(δ\) and \(ε\) are positive constants.</description>
    </item>
    
    <item>
      <title>Theory: Nash Equilibrium</title>
      <link>https://adityam.github.io/stochastic-control/games/nash-equilibrium/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/nash-equilibrium/</guid>
      <description>So far, we have looked at two solution concepts: (i) rationalizable strategies, which don’t exist for all games, and (ii) optimal or minimax strategies for zero-sum games. Now, we look at the most popular solution concept in games, known as Nash Equilibrium.
Definition A Nash equilibrium (in pure strategies) of a strategic game \(\mathscr{G} = \langle \ALPHABET N, (\ALPHABET A_i)_{i \in \ALPHABET N}, (u_i)_{i \in \ALPHABET N} \rangle\) is a strategy profile \(a^* = (a^*_1, \dots, a^*_n) \in \ALPHABET A\) such that for each player \(i \in \ALPHABET N\), and each strategy \(a_i \in \ALPHABET A_i\), we have \[ u_i(a_i^*, a_{-i}^*) \ge u_i(a_i, a_{-i}^*).</description>
    </item>
    
    <item>
      <title>Networked control systems with communication cost</title>
      <link>https://adityam.github.io/stochastic-control/linear-systems/ncs-with-cost/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/linear-systems/ncs-with-cost/</guid>
      <description>In this section, we extend the model of a simple NCS to one where there a cost associated with sending data from the sensor to the controller. This cost may be due to considerations of transmit power at the sensor or it may be a viewed as a Lagrange multiplier if multiple sensors are sharing the same communication channel with limited bandwidth.
As before, we consider a linear system with state \(x_t \in \reals^n\) and control action \(u_t \in \reals^n\) where the initial state \(x_1\) has zero mean and finite variance \(Σ^x_1\).</description>
    </item>
    
    <item>
      <title>Theory: Monotone value functions and policies</title>
      <link>https://adityam.github.io/stochastic-control/mdp/monotonicity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/monotonicity/</guid>
      <description>TL;DR General conditions are presented under which the optimal policy is monotone. Such a structural property is useful because it makes it easy to search and implement the optimal policy.
Consider the matrix formulation of MDPs and suppose the state space \(\ALPHABET S\) is totally ordered. In many applications, it is useful to know if the value function is increasing (or decreasing) in state.
Theorem 1 Consider an MDP where the state space \(\ALPHABET S\) is totally ordered.</description>
    </item>
    
    <item>
      <title>Example: Power-delay trade-off in wireless communication</title>
      <link>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/power-delay-tradeoff/</guid>
      <description>TL;DR This stylized example of power-delay trade-off in wireless communications illustrates that a dynamic programming formulation can be used to identify qualitative properties of the value function and optimal policies.
In a cell phone, higher layer applications such as voicecall, email, browsers, etc. generate data packets. These packets are buffered in a queue and the transmission protocol decides how many packets to transmit at each time depending the number of packets in the queue and the quality of the wireless channel.</description>
    </item>
    
    <item>
      <title>Theory: Evolutionary stable strategies</title>
      <link>https://adityam.github.io/stochastic-control/games/evolutionary-stable-strategies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/evolutionary-stable-strategies/</guid>
      <description>As we saw earlier, one of the interpretations of mixed strategy Nash equlibrium is that it determines the fraction of population playing a certain pure action. Maynard Smith and Price (1973) used this interpretation to propose an evolutionary mechanism via which agents can change their behavior across generations. For a facinating (non-technical) introduction to this area, see Maynard Smith (1982) and Dawkins (1976).
The key idea in what is now called Evolutionary Game theory is that of evolutionary stable strategies, which we discuss here.</description>
    </item>
    
    <item>
      <title>Theory: Reward Shaping</title>
      <link>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/mdp/reward-shaping/</guid>
      <description>What are the conditions under which two MDPs which have the same dynamics but different cost functions have the same optimal policy? This is an important question in reinforcement learning (where one often shapes the reward function to speed up learning) and inverse reinforcement learning (where one learns the reward function from the behavior of an expert). The following result provides a complete answer to this question. These results are typically established for inifinte horizon models.</description>
    </item>
    
    <item>
      <title>Theory: Evolutionary Dynamics</title>
      <link>https://adityam.github.io/stochastic-control/games/evolutionary-dynamics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/evolutionary-dynamics/</guid>
      <description>Consider a two player symmetric \(m × m\) game with \(\ALPHABET A_1 = \ALPHABET A_2 = \ALPHABET B := \{1, \dots, m \}\). Let \[ Δ^m = \bigl\{ (p_1, \dots, p_m) : \sum_{i=1}^m p_i = 1 \text{ and } p_i \in [0, 1] \bigr\}. \] denote the set of mixed strategies. We will use \(p = (p_1, \dots, p_m)\) (instead of \(α\) used earlier) to denote a mixed strategy.
Evolutionary Game Theory provides “revision protocols” that explain how a population state \(p\) evolves over time.</description>
    </item>
    
    <item>
      <title>Theory: Correlated equilibrium</title>
      <link>https://adityam.github.io/stochastic-control/games/correlated-equilibrium/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/correlated-equilibrium/</guid>
      <description>“If there is intelligent life on other planets, in a majority of them, they would have discovered correlated equilibrium before Nash equilibrium.” – Roger Myerson
Cosider the following “traffic stop” game:
\(\mathsf{Stop}\) \(\mathsf{Go}\) \(\mathsf{Stop}\) \(0\) \(0\) \(0\) \(1\) \(\mathsf{Go}\) \(1\) \(0\) \(-100\) \(-100\) There are two pure strategy Nash equilibria: \((\mathsf{Stop}, \mathsf{Go})\) and \((\mathsf{Go}, \mathsf{Stop})\). In both equilibria, one player gets a payoff of~\(1\) and the other gets a payoff of~\(0\).</description>
    </item>
    
    <item>
      <title>Linear Exponential of Quadratic Gaussian (LEQG)</title>
      <link>https://adityam.github.io/stochastic-control/risk-sensitive/leqg/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/risk-sensitive/leqg/</guid>
      <description>1 Preliminaries Lemma 1 Suppose that \(Q(z,w)\) is a quadratic function of vectors \(z\) and \(w\), positive definite in \(w\).
Let \(Q_{ww} = ∂^2 Q(z,w)/∂w^2\). Since \(Q(z,w)\) is a quadratic function, \(Q_{ww}\) does not depend on \(z\). Since \(Q\) is positive definite in \(w\), \(Q_{ww} &amp;gt; 0\).
Suppose \(w \in \reals^r\). Define \(q = \log[ (2π)^{r/2} \det(Q_{ww})^{-1/2}]\). Then, for a fixed value of \(z\) \[ \int \exp\bigl[ -Q(z,w)\bigr] dw = \exp\bigl[ q - \inf_{w \in \reals^r} Q(z,w) \bigr].</description>
    </item>
    
    <item>
      <title>Theory: Bayesian games</title>
      <link>https://adityam.github.io/stochastic-control/games/bayesian-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/games/bayesian-games/</guid>
      <description>So far we have assumed that all players know all the relevant information about each other. Such games are known as games with complete information. In this section, we consider games with incomplete information, i.e., games where players have private information about something relevant to their decision making.
Consider the following variation of battle of sexes. There are two players. As before, player 1 wants to go out with player 2 and prefers to go watch a football game over watching opera.</description>
    </item>
    
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://adityam.github.io/stochastic-control/appendix/rkhs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/rkhs/</guid>
      <description>1 Linear Operators Definition 1 (Linear operator) Let \(\mathcal F\) and \(\mathcal G\) be normed vector spaces over \(\reals\). A function \(A \colon \mathcal F \to \mathcal G\) is called a linear operator if it satisfies the following properties:
Honogeneity: For any \(α \in \reals\) and \(f \in \mathcal F\), \(A(αf) = α (Af)\). Additivity: For any \(f,g \in \mathcal F\), \(A(f + g) = Af + Ag\). The operator norm of a linear operator is defined as \[ \NORM{A} = \sup_{f \in \mathcal F} \frac{ \NORM{A f}_{\mathcal G}} {\NORM{f}}_{\mathcal F}.</description>
    </item>
    
    <item>
      <title>Vectorization</title>
      <link>https://adityam.github.io/stochastic-control/appendix/vectorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/vectorization/</guid>
      <description>Vectorization is a linear transformation that converts a matrix to a column vector. For example, \[\VEC\left(\MATRIX{a &amp;amp; b \\ c &amp;amp; d }\right) = \MATRIX{a \\ c \\ b \\ d}.\]
Vectorization is often used to express matrix multiplication as a linear transformation on matrices. In particular, we have the following three properties:
\(\VEC(ABC) = (C^\TRANS \otimes A) \VEC(B).\) \(\VEC(ABC) = (I \otimes AB)\VEC(C).\) \(\VEC(ABC) = (C^\TRANS B^\TRANS \otimes I)\VEC(A).\) Another useful formulation is the following</description>
    </item>
    
    <item>
      <title>Positive definite matrices</title>
      <link>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</guid>
      <description>[WARNING] Citeproc: citation Abbasi-Yadkori2011 not found 1 Definite and basic properties Definition A \(n \times n\) symmetric matrix \(M\) is called
positive definite (written as \(M &amp;gt; 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x &amp;gt; 0.\]
positive semi definite (written as \(M \ge 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x \ge 0.\]
1.1 Examples \(\MATRIX{ x_1 &amp;amp; x_2 } \MATRIX{ 3 &amp;amp; 0 \\ 0 &amp;amp; 2 } \MATRIX{ x_1 \\ x_2 } = 3 x_1^2 + 2 x_2^2\).</description>
    </item>
    
    <item>
      <title>Singular value decomposition</title>
      <link>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</guid>
      <description>Recall that if \(A\) is a symmetric \(n × n\) matrix, then \(A\) has real eigenvalues \(λ_1, \dots, λ_n\) (possibly repeated) and \(\reals^n\) has an orthonormal basis \(v_1, \dots, v_n\), where each \(v_i\) is an eigenvector of \(A\) with eigenvalue \(λ_i\). Then, \[ A = V D V^{-1} \] where \(V\) is the matrix whose columns are \(v_1, \dots, v_n\) and \(D\) is a diagonal matrix whose diagonals are \(λ_1, \dots, λ_n\).</description>
    </item>
    
    <item>
      <title>Sub-Gaussian random variables</title>
      <link>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</guid>
      <description>1 Prelim: Concentration inequality of sum of Gaussian random variables Let \(\phi(\cdot)\) denote the density of \(\mathcal{N}(0,1)\) Gaussian random variable: \[ \phi(x) = \frac{1}{\sqrt{2π}} \exp\biggl( - \frac{x^2}{2} \biggr). \]
Note that if \(X \sim \mathcal{N}(μ,σ^2)\), then the density of \(X\) is \[ \frac{1}{σ}\phi\biggl( \frac{x-μ}{σ} \biggr) = \frac{1}{\sqrt{2π}\,σ} \exp\biggl( - \frac{(x-μ)^2}{2 σ^2} \biggr). \]
The tails of Gaussian random variables decay fast which can be quantified using the following inequality.
Mills inequality.</description>
    </item>
    
    <item>
      <title>Poisson processes</title>
      <link>https://adityam.github.io/stochastic-control/appendix/poisson-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/poisson-processes/</guid>
      <description>1 Prelude: Bernoulli process A Bernoulli process is a sequence \(\{Z_n\}_{n \ge 1}\) of i.i.d. binary random variables. Let \(p = \PR(Z_n = 1)\) and \(q = 1 - p = \PR(Z_n = 0)\).
An example would be i.i.d. coin tosses where we can think of \(\{Z_n = 1\}\) as the event that the \(h\)-th outcome is a head and \(\{Z_n = 0\}\) to be the event that the \(n\)-th outcome is a tail.</description>
    </item>
    
    <item>
      <title>Markov chain</title>
      <link>https://adityam.github.io/stochastic-control/appendix/markov-chains/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/markov-chains/</guid>
      <description>A time-homogeneous Markov chain \(\{X_n\}_{n \ge 1}\), \(X_n \in \ALPHABET X\), is a stochastic process that satisfies the following property: for all \(x_{1:n+1} \in \ALPHABET X\), \[ \PR(X_{n+1} = x_{n+1} \mid X_{1:n} = x_{1:n}) = \PR(X_{n+1} = x_{n+1} \mid X_n = x_n). \] The transition probability can be written succinctly using a transition matrix \(P\), where \(P_{ij} = \PR(X_{n+1} = j | X_n = i)\).
Let \(π_n\) denote the PMF of the state of the Markov chain at time \(n\).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://adityam.github.io/stochastic-control/coursework/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/coursework/</guid>
      <description>Project The report must not be any longer than 10-15 pages. The grade distribution for different parts of the report are as follows.
Background, presentation of the problem setting, and literature overview: 20% Summary of the results, proof outlines, and examples to illustrate the main results: 50% Critical evaluation of the contributions: 20% Clarity of the presentation: 10% The potential project topics are listed below. You can also pick up on the discussion of the literature in the reference section of the notes.</description>
    </item>
    
    <item>
      <title>Assignment 1</title>
      <link>https://adityam.github.io/stochastic-control/assignments/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/01/</guid>
      <description>Exercise 1 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
Exercise 2 from the notes on stochastic optimization. Write a computer program in any language of your choice to find the optimal policy. You must submit your code along with your solution.
Exercise 3 from the notes on the newsvendor problem.</description>
    </item>
    
    <item>
      <title>Assignment 1 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/01/</guid>
      <description>Problem 1 We first observe a simplification that can be done for this model, which makes the calculations a little easier. In principle, we need to compute \[ Q(s,a) = \EXP[ c(s,a,W) | S = s] = \sum_{w \in \ALPHABET W} P(W = w | S = s) c(s,a,w). \]
This means that we need to compute \(P(W|S)\). However, we can avoid that step by observing that \[P(W|S) = \dfrac{ P(S,W) }{ P(S) }.</description>
    </item>
    
    <item>
      <title>Assignment 2</title>
      <link>https://adityam.github.io/stochastic-control/assignments/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/02/</guid>
      <description>Inventory Management. Consider a variation of the inventory management problem considered in class. Suppose that the inventory \(S_t\) and the actions \(A_t\) takes values in the set \(\mathbb S = \{0, 1, \dots, L \}\), and the dynamics are given by \[ S_{t+1} = \bigl[ S_t + A_t - W_t \bigr]_0^L, \] where \([ s ]_{0}^L\) is a function which clips the values between \(0\) and \(L\), i.e., \[ [ s ]_0^L = \begin{cases} 0, &amp;amp; \text{if $s &amp;lt; 0$} \\ s, &amp;amp; \text{if $0 \le s \le L$} \\ L, &amp;amp; \text{if $s &amp;gt; L$}.</description>
    </item>
    
    <item>
      <title>Assignment 2 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/02/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/02/</guid>
      <description>Problem 1 Problem 2 We prove the result for the case when \(V_{T-1}(x) \le V_T(x)\). The case when \(V_{T-1}(x) \ge V_T(x)\) follows from a similar argument.
As usual, the proof proceeds by backward induction. We know that the result is true for \(t = T-1\). This forms the basis of induction. Assume that the result is true for \(t+1\), i.e., \[\begin{equation} \label{eq:1.1} V_{t+1}(x) \le V_{t+2}(x), \quad \forall x \in \ALPHABET X \end{equation}\]</description>
    </item>
    
    <item>
      <title>Assignment 3</title>
      <link>https://adityam.github.io/stochastic-control/assignments/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/03/</guid>
      <description>Service rate control in queueing systems
Consider a queueing system where \(S_t \in \{0, 1, \dots, n\}\) denotes the number of jobs in the queue at time \(t\), \(W_t \in \integers_{\ge 0}\) denotes the number of new job arrivals in time \(t\), \(Z_t \in \{0,1\}\) denotes whether the job being processed gets completed in time \(t\). The dynamics of the system are given by \[ S_{t+1} = \bigl[ [ S_t - Z_t]^{+} + W_t \bigr]^{n}_{0} \] where \([s]^{+} = \max\{s,0\}\), and \([s]_{0}^n\) is the clip function (defined in Assignment 2).</description>
    </item>
    
    <item>
      <title>Assignment 3 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/03/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/03/</guid>
      <description>0.1 Problem 1 The code for computing the optimal policy is available here. The code is written for ease of reading, not efficiency.
using Distributions, OffsetArrays # Parameters n, m = 8, 3 λ = 0.5 μ = [0.2, 0.5, 0.8] q = [1, 3, 8] h = 1 R = 10 B = 15 # State spaces S = 0:n A = 1:m W = 0:B Z = 0:1 # Dynamics f(s,z,w) = min( max(s-z, 0) + w, n) # Arrival probability Pw(w) = pdf(Poisson(λ),w) # Departure probability Pz(z,s,a) = (s == 0) ?</description>
    </item>
    
    <item>
      <title>Assignment 4</title>
      <link>https://adityam.github.io/stochastic-control/assignments/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/04/</guid>
      <description>Exercise 1 of the notes on power-delay trade-off in wireless communication.
Exercise 2 of notes on stochastic dominance.</description>
    </item>
    
    <item>
      <title>Assignment 4 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/04/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/04/</guid>
      <description>Problem 1 We first note that when the channel is i.i.d., we can simplify the dynamic program as follows:
\[V_{T+1}(x,s) = 0\]
and for \(t \in \{T, \dots, 1\}\), \[\begin{align} H_t(y) &amp;amp;= λ d(y) + \EXP[ V_{t+1}(y + W_t, S_{t+1}) ] \label{eq:1}\\ V_t(x,s) &amp;amp;= \min_{0 \le a \le x} \bigl\{ p(a) q(s) + H_t(x-a) \bigr\} \end{align}\]
Note that due the i.i.d. nature of the channel, we don’t need to condition on the current state in \eqref{eq:1}.</description>
    </item>
    
    <item>
      <title>Assignment 5</title>
      <link>https://adityam.github.io/stochastic-control/assignments/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/05/</guid>
      <description>Consider the infinite horizon version of the machine repair model presented in Section 3 of the notes on matrix formulation of MDPs with \(θ = 0.3\), \(λ = 8\), and \(γ = 0.9\).
Using the results on stochastic monotonicity, show that the optimal policy is weakly decreasing.
Note that there are 4 possible weakly decreasing policies. Evaluate the performance of each of these policies using the policy evaluation formula derived in class.</description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05-v1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05-v1/</guid>
      <description>0.1 Problem 1 For the reward maximization case, the dynamic program is given as follows:
\[V_T(x) = s_T(x)\] and for \(t \in \{T-1,\dots, 1\}\), we have \[V_t(x) = \max\{ s_t(x), c_t(x) + \EXP[V_{t+1}(X_{t+1} | X_t = x]. \]
We define the benefit funciton as before. i.e., \[B_t(x) = c_t(x) + \EXP[ V_{t+1}(X_{t+1}) | X_t = x] - s_t(x). \] Then, it is optimal to stop whenever \(B_t(x) \le 0\).
Now, we write the value function in terms of the benefit function as follows:</description>
    </item>
    
    <item>
      <title>Assignment 5 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/05/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/05/</guid>
      <description>Note that \(c(s,1) = λ\), which is a constant and \(c(\cdot,2) = [0,1,5]\), which is increasing. Thus (C1) is satisfied.
Now we consider stochastic monotonicity of the transition matrices. \(P(1) = \begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 \\ 1 &amp;amp; 0 &amp;amp; 0 \end{bmatrix}\). Since all rows of \(P(1)\) are identical, it is trivially, stochsatic monotone.
\(P(2) = \begin{bmatrix} 1-θ &amp;amp; θ &amp;amp; 0 \\ 0 &amp;amp; 1-θ &amp;amp; θ \\ 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}\).</description>
    </item>
    
    <item>
      <title>Assignment 6</title>
      <link>https://adityam.github.io/stochastic-control/assignments/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/assignments/06/</guid>
      <description>Exercise 1 of the notes on discounted mdp
Consider the infinite horizon version of the machine repair model presented in Section 3 of the notes on matrix formulation of MDPs with \(θ = 0.3\), \(λ = 8\), and \(γ = 0.9\).
Start with an initial guess of \(V_0 = [0, 0, 0]\). Use value iteration to find a policy \(π\) such that \(\NORM{V_π - V^*} \le 10^{-6}\). How many iterations does it take to converge?</description>
    </item>
    
    <item>
      <title>Assignment 6 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/06/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/06/</guid>
      <description>\(\def\ONES{\mathbb{1}}\)
Problem 1 We start by the right side of the inequality. We want to show that \[\begin{equation} \label{eq:zero} V_{k+1} + \bar{\delta}_{k+1}\ONES \leq V_k + \bar{\delta}_k\ONES. \end{equation}\]
From the definition of \(\bar{δ}_k\), we know that for any \(s \in \ALPHABET S\),
\[ V_{k}(s) - V_{k-1}(s) \le \frac{(1-γ)}{γ} \bar δ_k. \] Thus, \[ V_k \le V_{k-1} + \frac{(1-γ)}{γ} \bar δ_k \ONES. \] Applying the Bellman operator on both sides and using the discounting property, we get that \[\begin{equation} V_{k+1} \le V_k + (1-γ) \bar δ_k \ONES \label{eq:one} \end{equation}\]</description>
    </item>
    
    <item>
      <title>Assignment 7 (solution)</title>
      <link>https://adityam.github.io/stochastic-control/solutions/07/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/solutions/07/</guid>
      <description>0.0.1 Problem 1 We are given Bernoulli measures \(μ = a δ_x + (1-a) δ_y\) and \(ν = b δ_x + (1-b)δ_y\). Now consider \[\begin{align*} K(μ, ν) &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| \int_X f dμ - \int_X f dν \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| \int_X f(z)( a δ_x(z) + (1 - a) δ_y(z) ) dz - \int_X f(z)( b δ_x(z) + (1 - b) δ_y(z) ) dz \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } \left| a f(x) + (1-a) f(y) - \big( b f(x) + (1-b) f(y) \big) \right| \\ &amp;amp;= \sup_{ f \colon \| f \|_L \le 1 } | a-b | | f(x) - f(y) | \\ &amp;amp;= |a - b | \sup_{ f \colon \| f \|_L \le 1 } \left| f(x) - f(y) \right | \\ &amp;amp;= |a -b | d_X(x,y).</description>
    </item>
    
    <item>
      <title>Course Notes</title>
      <link>https://adityam.github.io/stochastic-control/notes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/notes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lectures</title>
      <link>https://adityam.github.io/stochastic-control/lectures/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/lectures/</guid>
      <description>Whenever possible, I will post notes on some of the material covered in class, but that is not guaranteed. This is a graduate class and you are responsible for taking notes in class and reading the appropriate chapters of the textbooks.
The notes will be updated as we move along in the course. Please check the dates on the first page to keep track. If you find any typos/mistakes in the notes, please let me know.</description>
    </item>
    
    <item>
      <title>Term Projects for Winter 2020</title>
      <link>https://adityam.github.io/stochastic-control/projects/2020-winter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/projects/2020-winter/</guid>
      <description>Alex Dunyak: An Overview of the Stochastic Dual Dynamic Programming Method
Alper Oker: Review of “Two Characterizations of Optimality in Dynamic Progrmaming
Amro Al Baali: Paper Summary: Adaptive Controllers Over Finite Parameter Sets
Erfan Seyedsalehi: The Options Framework for Temporal Abstraction
Fatih Gurturk: Average cost error bounds of finite state approximations of Markov decision processes
Feras Al Taha: Witsenhausen’s Counterexample
Imen Jendoubi: Average cost MDPs
Imene Romdhane: Convergence of Stochastic Iterative Dynamic Programming Algorithms</description>
    </item>
    
  </channel>
</rss>
