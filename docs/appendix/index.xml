<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Appendix on ECSE 506: Stochastic Control and Decision Theory</title>
    <link>https://adityam.github.io/stochastic-control/appendix/</link>
    <description>Recent content in Appendix on ECSE 506: Stochastic Control and Decision Theory</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://adityam.github.io/stochastic-control/appendix/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Reproducing Kernel Hilbert Space</title>
      <link>https://adityam.github.io/stochastic-control/appendix/rkhs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/rkhs/</guid>
      <description>1 Linear Operators Definition 1 (Linear operator) Let \(\mathcal F\) and \(\mathcal G\) be normed vector spaces over \(\reals\). A function \(A \colon \mathcal F \to \mathcal G\) is called a linear operator if it satisfies the following properties:
Honogeneity: For any \(α \in \reals\) and \(f \in \mathcal F\), \(A(αf) = α (Af)\). Additivity: For any \(f,g \in \mathcal F\), \(A(f + g) = Af + Ag\). The operator norm of a linear operator is defined as \[ \NORM{A} = \sup_{f \in \mathcal F} \frac{ \NORM{A f}_{\mathcal G}} {\NORM{f}}_{\mathcal F}.</description>
    </item>
    
    <item>
      <title>Vectorization</title>
      <link>https://adityam.github.io/stochastic-control/appendix/vectorization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/vectorization/</guid>
      <description>Vectorization is a linear transformation that converts a matrix to a column vector. For example, \[\VEC\left(\MATRIX{a &amp;amp; b \\ c &amp;amp; d }\right) = \MATRIX{a \\ c \\ b \\ d}.\]
Vectorization is often used to express matrix multiplication as a linear transformation on matrices. In particular, we have the following three properties:
\(\VEC(ABC) = (C^\TRANS \otimes A) \VEC(B).\) \(\VEC(ABC) = (I \otimes AB)\VEC(C).\) \(\VEC(ABC) = (C^\TRANS B^\TRANS \otimes I)\VEC(A).\) Another useful formulation is the following</description>
    </item>
    
    <item>
      <title>Positive definite matrices</title>
      <link>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/positive-definite-matrix/</guid>
      <description>[WARNING] Citeproc: citation Abbasi-Yadkori2011 not found 1 Definite and basic properties Definition A \(n \times n\) symmetric matrix \(M\) is called
positive definite (written as \(M &amp;gt; 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x &amp;gt; 0.\]
positive semi definite (written as \(M \ge 0\)) if for all \(x \in \reals^n\), \(x \neq 0\), we have \[x^\TRANS M x \ge 0.\]
1.1 Examples \(\MATRIX{ x_1 &amp;amp; x_2 } \MATRIX{ 3 &amp;amp; 0 \\ 0 &amp;amp; 2 } \MATRIX{ x_1 \\ x_2 } = 3 x_1^2 + 2 x_2^2\).</description>
    </item>
    
    <item>
      <title>Singular value decomposition</title>
      <link>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/singular-value-decomposition/</guid>
      <description>Recall that if \(A\) is a symmetric \(n × n\) matrix, then \(A\) has real eigenvalues \(λ_1, \dots, λ_n\) (possibly repeated) and \(\reals^n\) has an orthonormal basis \(v_1, \dots, v_n\), where each \(v_i\) is an eigenvector of \(A\) with eigenvalue \(λ_i\). Then, \[ A = V D V^{-1} \] where \(V\) is the matrix whose columns are \(v_1, \dots, v_n\) and \(D\) is a diagonal matrix whose diagonals are \(λ_1, \dots, λ_n\).</description>
    </item>
    
    <item>
      <title>Sub-Gaussian random variables</title>
      <link>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/sub-gaussian/</guid>
      <description>1 Prelim: Concentration inequality of sum of Gaussian random variables Let \(\phi(\cdot)\) denote the density of \(\mathcal{N}(0,1)\) Gaussian random variable: \[ \phi(x) = \frac{1}{\sqrt{2π}} \exp\biggl( - \frac{x^2}{2} \biggr). \]
Note that if \(X \sim \mathcal{N}(μ,σ^2)\), then the density of \(X\) is \[ \frac{1}{σ}\phi\biggl( \frac{x-μ}{σ} \biggr) = \frac{1}{\sqrt{2π}\,σ} \exp\biggl( - \frac{(x-μ)^2}{2 σ^2} \biggr). \]
The tails of Gaussian random variables decay fast which can be quantified using the following inequality.
Mills inequality.</description>
    </item>
    
    <item>
      <title>Poisson processes</title>
      <link>https://adityam.github.io/stochastic-control/appendix/poisson-processes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://adityam.github.io/stochastic-control/appendix/poisson-processes/</guid>
      <description>1 Prelude: Bernoulli process A Bernoulli process is a sequence \(\{Z_n\}_{n \ge 1}\) of i.i.d. binary random variables. Let \(p = \PR(Z_n = 1)\) and \(q = 1 - p = \PR(Z_n = 0)\).
An example would be i.i.d. coin tosses where we can think of \(\{Z_n = 1\}\) as the event that the \(h\)-th outcome is a head and \(\{Z_n = 0\}\) to be the event that the \(n\)-th outcome is a tail.</description>
    </item>
    
  </channel>
</rss>
