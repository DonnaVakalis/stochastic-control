<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="infinite horizon,discounted cost,sample complexity,Hoeffding inequality,generative model" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Sample complexity</div>

<p>An important consideration in model-based RL is to determine how many samples are needed to identify an <span class="math inline">\(α\)</span>-approximate solution (for a pre-specified accuracy level <span class="math inline">\(α\)</span>). This is known as <em>sample complexity</em> of learning and is typically analyzed under the assumption that the learning agent has access to a generative model, i.e., a black box simulator that takes the current state and action profile as input and generates samples of the next state as output.</p>
<p>The simplest approach in this setting is to use a <em>plug-in estimator</em> (or the <em>certainty equivalent controller</em>), i.e., estimating the transition matrix using the generated samples and using the optimal policy corresponding to the estimated model in the true system. In this section, we characterize the sample complexity of using the plug-in estimator.</p>
<p>Consider an MDP <span class="math inline">\(\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, P, c, γ \rangle\)</span>. Suppose the components <span class="math inline">\(\langle \ALPHABET S, \ALPHABET A, c, γ \rangle\)</span> are known but the transition matrix <span class="math inline">\(P\)</span> is not known. We assume that Gwe have access to a <em>generative model</em>, i.e., a black box simulator that provides samples <span class="math inline">\(S_+ \sim P(\,\cdot \mid s,a)\)</span> of the next state <span class="math inline">\(S_+\)</span> for any given state-action pair <span class="math inline">\((s,a)\)</span> as input. Suppose we call the simulator <span class="math inline">\(N\)</span> times at each state-action pair and estimate the emperical model <span class="math inline">\(\hat P_N\)</span> as <span class="math inline">\(\hat P_N(s&#39;|s,a) = \text{count}(s&#39;|s,a)/N\)</span>, where <span class="math inline">\(\text{count}(s&#39;|s,a)\)</span> is the number of times <span class="math inline">\(s&#39;\)</span> is sampled with input <span class="math inline">\((s,a)\)</span>. The model <span class="math inline">\(\widehat {\ALPHABET M}_N = \langle \ALPHABET S, \ALPHABET A, \hat P_N, c, γ \rangle\)</span> may be viewed as an approximation of model <span class="math inline">\(\ALPHABET M\)</span>.</p>
<p>We also assume that we have a <em>planning oracle</em>, which takes the approximate model <span class="math inline">\(\widehat {\ALPHABET M}_N\)</span> as input and generates an optimal policy <span class="math inline">\(\hat π_N\)</span>. We are interested in the following question. Given a <span class="math inline">\(α &gt; 0\)</span> and <span class="math inline">\(p &gt; 0\)</span>, how many samples <span class="math inline">\(N\)</span> are needed from the generative model such that the policy <span class="math inline">\(\hat π_N\)</span> is <span class="math inline">\(α\)</span>-optimal for model <span class="math inline">\(\ALPHABET M\)</span> with probability at least <span class="math inline">\(1 - p\)</span>. This is called <em>sample complexity</em> of learning.</p>
<h1 data-number="1" id="hoeffding-type-bound"><span class="header-section-number">1</span> Hoeffding-Type Bound</h1>
<p>We use the policy approximation bounds derived in the notes for <a href="../../inf-mdp/model-approximation#thm:policy-bound2">model approximation</a> by treating model <span class="math inline">\(\widehat {\ALPHABET M}_N\)</span> as an approximation of model <span class="math inline">\(\ALPHABET M\)</span>. Recall the definitions of Bellman mismatch operators given in the <a href="../../inf-mdp/model-approximation#value-error">notes on model approximation</a>. In our setting, since the cost function is known, we can upper bound both as follows. <span class="math display">\[\begin{align}
  Δ \ALPHABET B^π V^* &amp;= \| \ALPHABET B^π V^* - \hat {\ALPHABET B}^π V^* \|_∞
  \notag \\
  &amp;\le γ \max_{s \in \ALPHABET S} \biggl|
  \sum_{a \in \ALPHABET A} π(a|s) 
  \sum_{s&#39; \in \ALPHABET S} \Bigl[
  P(s&#39;|s,a) V^*(s&#39;) - \hat P_N(s&#39;|s,a) V^*(s&#39;)
  \Bigr] \biggr|
  \notag \\
  &amp;\le γ 
  \max_{s \in \ALPHABET S, a \in \ALPHABET A} \biggl|
  \sum_{s&#39; \in \ALPHABET S} \Bigl[
  P(s&#39;|s,a) V^*(s&#39;) - \hat P_N(s&#39;|s,a) V^*(s&#39;)
  \Bigr] \biggr|
  \notag \\
  &amp;=: γ Δ_N
  \label{eq:bound-1}
\end{align}\]</span></p>
<p>By a similar argument, <span class="math display">\[\begin{align}
  Δ \ALPHABET B^* V^* &amp;= \| \ALPHABET B^* V^* - \hat {\ALPHABET B}^* V^* \|_∞
  \notag \\
  &amp;\le γ \max_{s \in \ALPHABET S, a \in \ALPHABET A} \biggl|
  \sum_{s&#39; \in \ALPHABET S} \Bigl[
  P(s&#39;|s,a) V^*(s&#39;) - \hat P_N(s&#39;|s,a) V^*(s&#39;)
  \Bigr] \biggr|
  \notag \\
  &amp;= γ Δ_N
  \label{eq:bound-2}
\end{align}\]</span> where the first inequality follows from the fact that <span class="math inline">\(| \min f(x) - \min g(x) | \le \max | f(x) - g(x) |\)</span>.</p>
<p>Therefore, using the second bound in <a href="../../inf-mdp/model-approximation#thm:policy-bound2">policy approximation bound</a>, we get</p>
<p><span class="math display">\[
  \| V^* - V^{\hat π_N} \| \le \frac{2γ}{(1-γ)^2} Δ_N.
\]</span></p>
<p>Let <span class="math inline">\(H = \text{span}(V^*)\)</span>. By Hoeffding’s inequality, we have that for any given state action pair <span class="math inline">\((s,a)\)</span> and any constant <span class="math inline">\(\bar Δ &gt; 0\)</span>, <span class="math display">\[
\PR\biggl( \Biggl| \sum_{s&#39;\in \ALPHABET S} \hat P_N(s&#39;|s,a) V^*(s&#39;) 
- \sum_{s&#39; \in \ALPHABET S} P(s&#39;|s,a) V^*(s&#39;) \Biggr| \ge c \biggr)
\le 2 \exp\biggl( - \frac{2 N c^2}{H^2} \biggr).
\]</span></p>
<p>Therefore, by union bound, we have <span class="math display">\[
\PR\biggl( \max_{s \in \ALPHABET S, a \in \ALPHABET  A} \Biggl| \sum_{s&#39;\in \ALPHABET S} \hat P_N(s&#39;|s,a) V^*(s&#39;) 
- \sum_{s&#39; \in \ALPHABET S} P(s&#39;|s,a) V^*(s&#39;) \Biggr| \ge c \biggr)
\le 2 |\ALPHABET S|\,|\ALPHABET A| \exp\biggl( - \frac{2 N c^2}{H^2} \biggr).
\]</span></p>


<p class="categories">
This entry 

 was last updated on 17 Nov 2022
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/rl">
  RL</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/infinite-horizon">infinite horizon</a>,
<a href="https://adityam.github.io/stochastic-control/tags/discounted-cost">discounted cost</a>,
<a href="https://adityam.github.io/stochastic-control/tags/sample-complexity">sample complexity</a>,
<a href="https://adityam.github.io/stochastic-control/tags/hoeffding-inequality">hoeffding inequality</a>,
<a href="https://adityam.github.io/stochastic-control/tags/generative-model">generative model</a>.</p>



    </div>
  </body>
</html>


