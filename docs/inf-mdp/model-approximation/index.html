<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="infinite horizon,discounted cost,Lipschitz continuity,approximation bounds,state aggregation" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Theory: Model approximation in MDPs</div>

<p>There are many instances where we may plan using an approximate model. For example, in many applications with large state spaces, we may construct a simulation model of the system and use it to identify an optimal policy using simulation based methods (such as reinforcement learning). Often, the simulation model is only an approximation of the true model. In such instances, we want to know the error in using the policy obtained from the simulation model in the real world. In this section, we present bounds on such <em>sim-to-real</em> transfer.</p>
<p>Consider an MDP <span class="math inline">\(\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, P, c, γ \rangle\)</span>. Suppose the components <span class="math inline">\(\langle \ALPHABET S, \ALPHABET A, γ \rangle\)</span> are known exactly but the components <span class="math inline">\((P,c)\)</span> are known approximately. Consider the approximate MDP <span class="math inline">\(\widehat {\ALPHABET M} = \langle \ALPHABET S, \ALPHABET A, \hat P, \hat c, γ \rangle\)</span>. We will call <span class="math inline">\(\ALPHABET M\)</span> to be the <em>true model</em> and <span class="math inline">\(\widehat {\ALPHABET M}\)</span> to be the <em>approximate model</em>.</p>
<p>Let <span class="math inline">\(V^*\)</span> and <span class="math inline">\(\hat V^*\)</span> denote the optimal value functions of the true model <span class="math inline">\(\ALPHABET M\)</span> and the approximate model <span class="math inline">\(\widehat {\ALPHABET M}\)</span>, respectively. Moreover, let <span class="math inline">\(π^*\)</span> and <span class="math inline">\(\hat π^*\)</span> be optimal policies for the true model <span class="math inline">\(\ALPHABET M\)</span> and the approximate model <span class="math inline">\(\widehat {\ALPHABET M}\)</span>, respectively.</p>
<p>We are interested in two questions:</p>
<ol type="1">
<li><p><strong>Error in value approximation:</strong> What is the error if <span class="math inline">\(\hat V^*\)</span> is used as an approximation for <span class="math inline">\(V^*\)</span>?</p></li>
<li><p><strong>Error in policy approximation:</strong> What is the error if the policy <span class="math inline">\(\hat π^*\)</span> is used instead of the optimal policy <span class="math inline">\(π^*\)</span></p></li>
</ol>
<h1 data-number="1" id="bounds-for-model-approximation"><span class="header-section-number">1</span> Bounds for model approximation</h1>
<h2 data-number="1.1" id="value-error"><span class="header-section-number">1.1</span> Bounding the error for value function approximation</h2>
<p>Let <span class="math inline">\(\ALPHABET B^π\)</span> and <span class="math inline">\(\ALPHABET B^*\)</span> denote the Bellman operator for policy <span class="math inline">\(π\)</span> and the optimality Bellman operator for model <span class="math inline">\(\ALPHABET M\)</span>. Let <span class="math inline">\(\hat {\ALPHABET B}^π\)</span> and <span class="math inline">\(\hat {\ALPHABET B}^*\)</span> denote the corresponding quantiites for model <span class="math inline">\(\widehat {\ALPHABET M}\)</span>. Define the <em>Bellman mismatch operator</em> <span class="math inline">\(\ALPHABET D^π\)</span> and <span class="math inline">\(\ALPHABET D^*\)</span> as follows: <span class="math display">\[\begin{align*}
  \ALPHABET D^π v &amp;= \| \ALPHABET B^π v - \hat {\ALPHABET B}^π v \|_∞, 
  \\
  \ALPHABET D^* v &amp;= \| \ALPHABET B^* v - \hat {\ALPHABET B}^* v \|_∞ .
\end{align*}\]</span></p>
<p>The Bellman mismatch operators can be used to bound the performance difference of a policy between the true and approximate models.</p>
<div class="highlight">
<dl>
<dt><span id="lem:1"></span><span id="lem:bound-policy" class="pandoc-numbering-text lem"><strong>Lemma 1</strong></span></dt>
<dd>
<p>For any (possibly randomized) policy <span class="math inline">\(π\)</span>, <span class="math display">\[\begin{equation}\label{eq:bound-0}
   \| V^{π} - \hat V^{π} \|_∞ \le 
   \frac{1}{1-γ} \min\{ \ALPHABET D^π V^{π}, \ALPHABET D^π \hat V^{π} \}. 
\end{equation}\]</span></p>
</dd>
</dl>
</div>
<details>
<!--{{{ Proof -->
<summary>
<h4 class="unnumbered" id="proof">Proof</h4>
</summary>
<div>
<p>We bound the left hand side of \eqref{eq:bound-0} in two ways. The first way is as follows: <span class="math display">\[\begin{align}
  \| V^{π} - \hat V^{π} \|_∞ 
  &amp;=
  \| \ALPHABET B^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &amp;\le
  \| \ALPHABET B^π V^π - \hat {\ALPHABET  B}^π V^π \|_∞ 
  + 
  \| \hat {\ALPHABET B}^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &amp;\le
  \ALPHABET D^π V^π + γ \| V^π - \hat V^π \|_∞
  \label{eq:ineq-3}
\end{align}\]</span> where the first inequality follows from the triangle inequality, and the second inequality follows from the definition of the Bellman mismatch operator and the contraction property of Bellman operators. Rearranging terms in \eqref{eq:ineq-3} gives us <span class="math display">\[\begin{equation}
\| V^{π} - \hat V^{π} \|_∞ \le \frac{ \ALPHABET D^π V^{π}}{1 - γ}.
\label{eq:ineq-4}\end{equation}\]</span> This gives the first bound.</p>
<p>The second bound is symmetric and obtained by interchaning the roles of <span class="math inline">\(V^π\)</span> and <span class="math inline">\(\hat V^π\)</span>. <span class="math display">\[\begin{align}
  \| V^{π} - \hat V^{π} \|_∞ 
  &amp;=
  \| \ALPHABET B^π V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &amp;\le
  \| \ALPHABET B^π V^π - \ALPHABET  B^π \hat V^π \|_∞ 
  + 
  \| \ALPHABET B^π \hat V^π - \hat {\ALPHABET  B}^π \hat V^π \|_∞ 
  \notag \\
  &amp;\le
  γ \| V^π - \hat V^π \|_∞
  +
  \ALPHABET D^π \hat V^π 
  \label{eq:ineq-13}
\end{align}\]</span> Rearranging terms in \eqref{eq:ineq-13} gives us <span class="math display">\[\begin{equation}
\| V^{π} - \hat V^{π} \|_∞ \le \frac{ \ALPHABET D^π \hat V^{π}}{1 - γ}.
\label{eq:ineq-14}\end{equation}\]</span> This gives the second bound.</p>
</div>
</details>
<!--}}}-->
<p>Similar to the above, we can also bound the difference between the optimal value function of the true and approximate models.</p>
<div class="highlight">
<dl>
<dt><span id="thm:1"></span><span id="thm:value-bound" class="pandoc-numbering-text thm"><strong>Theorem 1</strong></span></dt>
<dd>
<p>Let <span class="math inline">\(V^*\)</span> and <span class="math inline">\(\hat V^*\)</span> denote the optimal value functions for <span class="math inline">\(\ALPHABET M\)</span> and <span class="math inline">\(\widehat {\ALPHABET M}\)</span>, respectively. Then, <span class="math display">\[\begin{equation}\label{eq:bound-1}
    \| V^* - \hat V^* \|_∞ \le 
    \frac{1}{1-γ} \min\{ \ALPHABET D^* V^*, \ALPHABET D^* \hat V^* \} 
\end{equation}\]</span></p>
</dd>
</dl>
</div>
<details>
<!--{{{ Proof -->
<summary>
<h4 class="unnumbered" id="proof-1">Proof</h4>
</summary>
<div>
<p>The proof argument is almost the same as the proof argument for <a href="#lem:bound-policy" title="Lemma 1"><span class="pandoc-numbering-link lem">Lemma 1</span></a>. The first was is as follows: <span class="math display">\[\begin{align}
  \| V^{*} - \hat V^{*} \|_∞ 
  &amp;=
  \| \ALPHABET B^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &amp;\le
  \| \ALPHABET B^* V^* - \hat {\ALPHABET  B}^* V^* \|_∞ 
  + 
  \| \hat {\ALPHABET B}^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &amp;\le
  \ALPHABET D^* V^* + γ \| V^* - \hat V^* \|_∞
  \label{eq:ineq-1}
\end{align}\]</span> where the first inequality follows from the triangle inequality, and the second inequality follows from the definition of the Bellman mismatch operator and the contraction property of Bellman operators. Rearranging terms in \eqref{eq:ineq-3} gives us <span class="math display">\[\begin{equation}
\| V^* - \hat V^* \|_∞ \le \frac{  \ALPHABET D^* V^*}{1 - γ}.
\label{eq:ineq-2}\end{equation}\]</span> This gives the first bound.</p>
<p>The second bound is symmetric and obtained by interchaning the roles of <span class="math inline">\(V^*\)</span> and <span class="math inline">\(\hat V^*\)</span>. <span class="math display">\[\begin{align}
  \| V^{*} - \hat V^{*} \|_∞ 
  &amp;=
  \| \ALPHABET B^* V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &amp;\le
  \| \ALPHABET B^* V^* - \ALPHABET  B^* \hat V^* \|_∞ 
  + 
  \| \ALPHABET B^* \hat V^* - \hat {\ALPHABET  B}^* \hat V^* \|_∞ 
  \notag \\
  &amp;\le
  γ \| V^* - \hat V^* \|_∞
  +
  \ALPHABET D^* \hat V^* 
  \label{eq:ineq-11}
\end{align}\]</span> Rearranging terms in \eqref{eq:ineq-11} gives us <span class="math display">\[\begin{equation}
\| V^{*} - \hat V^{*} \|_∞ \le \frac{ \ALPHABET D^* \hat V^{*}}{1 - γ}.
\label{eq:ineq-12}\end{equation}\]</span> This gives the second bound.</p>
</div>
</details>
<!--}}}-->
<h2 data-number="1.2" id="bounding-the-error-for-policy-approximation-error"><span class="header-section-number">1.2</span> Bounding the error for policy approximation error</h2>
<div class="highlight">
<dl>
<dt><span id="thm:2"></span><span id="thm:policy-bound" class="pandoc-numbering-text thm"><strong>Theorem 2</strong></span></dt>
<dd>
<p>The policy <span class="math inline">\(\hat π^*\)</span> is an <span class="math inline">\(α\)</span>-optimal policy of <span class="math inline">\(\ALPHABET M\)</span> where <span class="math display">\[\begin{equation} \label{eq:bound}
α := \| V^* - V^{\hat π^*} \|_∞ \le
\frac{1}{1 - γ}\bigl[ \ALPHABET D_1 + \ALPHABET D_2 \bigr]
\end{equation}\]</span> where <span class="math display">\[
 \ALPHABET D_1 \le \min\{ \ALPHABET D^* V^*, \ALPHABET D^* \hat V^* \}
 \quad\text{and}\quad
 \ALPHABET D_2 \le \min\{ \ALPHABET D^{\hat π^*} V^{\hat π^*}, \ALPHABET D^{\hat π^*}\hat V^{\hat π^*} \}.
\]</span></p>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-2">Proof</h4>
</summary>
<div>
<p>From triangle inequality we have <span class="math display">\[\begin{equation} \label{eq:triangle-1}
  \| V^* - V^{\hat π^*} \|_∞ \le
  \| V^* - \hat V^{\hat π^*} \|_∞ 
  + 
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞.
\end{equation}\]</span> The result then follows by bounding the first term using <a href="#thm:value-bound" title="Theorem 1"><span class="pandoc-numbering-link thm">Theorem 1</span></a> and the second term using <a href="#lem:bound-policy" title="Lemma 1"><span class="pandoc-numbering-link lem">Lemma 1</span></a>.</p>
</div>
</details>
<p>Note that depending on how we pick <span class="math inline">\(\ALPHABET D_1\)</span> and <span class="math inline">\(\ALPHABET D_2\)</span>, we get four possible upper bounds and can bound <span class="math inline">\(α\)</span> by the tightest of them, i.e., <span class="math display">\[\begin{equation}\label{eq:combined}
  α \le \frac{1}{1 - γ} \min\bigl\{
  \ALPHABET D^* V^* + \ALPHABET D^{\hat π^*} V^{\hat π^*}, \space 
  \ALPHABET D^* V^* + \ALPHABET D^{\hat π^*} \hat V^*, \space
  \ALPHABET D^* \hat V^* + \ALPHABET D^{\hat π^*} V^{\hat π^*}, \space 
  \ALPHABET D^* \hat V^* + \ALPHABET D^{\hat π^*} \hat V^{*} \bigr\}
\end{equation}\]</span> where we have used the fact that <span class="math inline">\(\hat V^{\hat π^*} = \hat V^*\)</span> to simplify some of the terms.</p>
<p>Ideally, we want an upper bound that just depends on the value function of true model or just on the value function of the approximate model. The last bound in \eqref{eq:combined} depends just on the value function of the approximate model. We state it separately for ease of reference.</p>
<div class="highlight">
<dl>
<dt><span id="cor:1"></span><span id="cor:ais-bound" class="pandoc-numbering-text cor"><strong>Corollary 1</strong></span></dt>
<dd>
<p>The policy <span class="math inline">\(\hat π^*\)</span> is an <span class="math inline">\(α\)</span>-optimal policy of <span class="math inline">\(\ALPHABET M\)</span> where <span class="math display">\[
    α := \| V^* - V^{\hat π^*} \|_∞ \le
    \frac{1}{1-γ} \bigl[ \ALPHABET D^* \hat V^* + \ALPHABET D^{\hat
    π^*} \hat V^* \bigr]. 
\]</span> Moreover, if both <span class="math inline">\(\ALPHABET D^{\hat π^*} \hat V^*\)</span> and <span class="math inline">\(\ALPHABET D^* \hat V^*\)</span> are upper bounded by a common upper bound, say <span class="math inline">\(\bar {\ALPHABET D} \hat V^*\)</span>, then, <span class="math display">\[
    α \le \frac{2}{(1-γ)}  \bar {\ALPHABET D}  \hat V^*. 
\]</span></p>
</dd>
</dl>
</div>
<p>In the next lemma, we develop a bound that depends just on the value function true model.</p>
<div class="highlight">
<dl>
<dt><span id="thm:3"></span><span id="thm:policy-bound2" class="pandoc-numbering-text thm"><strong>Theorem 3</strong></span></dt>
<dd>
<p>The policy <span class="math inline">\(\hat π^*\)</span> is an <span class="math inline">\(α\)</span>-optimal policy of <span class="math inline">\(\ALPHABET M\)</span> where <span class="math display">\[
    α := \| V^* - V^{\hat π^*} \|_∞ \le
    \frac{1}{1-γ} \ALPHABET D^{\hat π^*} V^*
    + 
    \frac{(1+γ)}{(1-γ)^2} \ALPHABET D^* V^* .
\]</span></p>
<p>Moreover, if both <span class="math inline">\(\ALPHABET D^{\hat π^*} V^*\)</span> and <span class="math inline">\(\ALPHABET D^* V^*\)</span> are upper bounded by a common upper bound, say <span class="math inline">\(\bar {\ALPHABET D} V^*\)</span>, then, <span class="math display">\[
    α \le \frac{2}{(1-γ)^2}  \bar {\ALPHABET D}  V^*. 
\]</span></p>
</dd>
</dl>
</div>
<details>
<summary>
<h4 class="unnumbered" id="proof-3">Proof</h4>
</summary>
<div>
<p>As before, we consider the following bound by triangle inequality. <span class="math display">\[\begin{equation} \label{eq:triangle-2}
  \| V^* - V^{\hat π^*} \|_∞ \le
  \| V^* - \hat V^{\hat π^*} \|_∞ 
  + 
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞.
\end{equation}\]</span> We will bound the first term of \eqref{eq:triangle-2} by <a href="#thm:value-bound" title="Theorem 1"><span class="pandoc-numbering-link thm">Theorem 1</span></a>. But instead of bounding the second term of \eqref{eq:triangle-2} by <a href="#lem:bound-policy" title="Lemma 1"><span class="pandoc-numbering-link lem">Lemma 1</span></a>, we consider the following: <span class="math display">\[\begin{align}
  \| V^{\hat π^*} - \hat V^{\hat π^*} \|_∞ 
  &amp;= 
  \| V^{\hat π^*} - \hat V^{*} \|_∞ 
  = \| \ALPHABET B^{\hat π^*} V^{\hat π^*} - 
       \hat {\ALPHABET B}^{\hat π^*} \hat V^{*} \|_∞ 
  \notag \\
  &amp;\le \| \ALPHABET B^{\hat π^*} V^{\hat π^*} - 
          \ALPHABET B^{\hat π^*} V^{*} \|_∞
    +  \| \ALPHABET B^{\hat π^*} V^{*} -
       \hat {\ALPHABET B}^{\hat π^*} V^{*} \|_∞ 
    + 
       \| \hat {\ALPHABET B}^{\hat π^*} V^{*} - 
       \hat {\ALPHABET B}^{\hat π^*} \hat V^{*} \|_∞ 
  \notag \\
  &amp;\le γ \| V^* - V^{\hat π^*} \|_∞ + \ALPHABET D^{\hat π^*} V^* 
  + γ \| V^* - \hat V^* \|_∞
  \label{eq:ineq-21}.
\end{align}\]</span> where the first inequality follows from the triangle inequality and the second inequality follows from the definition of Bellman mismatch operator and contraction property of Bellman operator.</p>
<p>Substituting \eqref{eq:ineq-21} in \eqref{eq:triangle-2} and rearranging terms, we get <span class="math display">\[\begin{align}
  \| V^* - V^{\hat π^*} \|_∞ 
  &amp;\le
  \frac{1}{1-γ} \ALPHABET D^{\hat π^*} V^*
  + 
  \frac{1+γ}{1-γ} \| V^* - \hat V^* \|_∞
  \notag \\
  &amp;\le
  \frac{1}{1-γ} \ALPHABET D^{\hat π^*} V^*
  + 
  \frac{(1+γ)}{(1-γ)^2} \ALPHABET D^* V^* .
\end{align}\]</span> where the second inequality follows from <a href="#thm:value-bound" title="Theorem 1"><span class="pandoc-numbering-link thm">Theorem 1</span></a>.</p>
</div>
</details>
<!--}}}-->
<dl>
<dt><span id="rem:1"></span><span id="rem:comparison" class="pandoc-numbering-text rem"><strong>Remark 1</strong></span></dt>
<dd>
<p>Note that the bound of <a href="#cor:ais-bound" title="Corollary 1"><span class="pandoc-numbering-link cor">Corollary 1</span></a> is tighter by a factor of <span class="math inline">\(1/(1-γ)\)</span>, but this bound is in terms of <span class="math inline">\(\hat V^*\)</span>. In some settings, we prefer a bound in terms of <span class="math inline">\(V^*\)</span>. Using <a href="#thm:policy-bound2" title="Theorem 3"><span class="pandoc-numbering-link thm">Theorem 3</span></a> in such settings leads to scaling by <span class="math inline">\(1/(1-γ)\)</span>.</p>
</dd>
</dl>
<h1 data-number="2" id="example-state-aggregation-or-discretization-or-quantization"><span class="header-section-number">2</span> Example: State aggregation (or discretization or quantization)</h1>
<p>Consider an MDP <span class="math inline">\(\ALPHABET M = \langle \ALPHABET S, \ALPHABET A, p, c, γ \rangle\)</span>, with large (or possibly continuous) state space. For simplicity, we assume that <span class="math inline">\(\ALPHABET S\)</span> is continuous (and compact), and that the <span class="math inline">\(p\)</span> is the density of the transition kernel. Note that we are using the term “probability density” in the engineering sense (so, it may be a combination of a continuous function and delta functions) rather than in the precise measure theoretic sense (where it is the Radon-Nikodym derivative with respect to the Lebesque measure).</p>
<p>If exact computations were possible, we can find an optimal solution by solving a dynamic program. However, since the state space is continuous, we cannot compute the value functions exactly. The simplest way to proceed is to discretize or quantize the state space <span class="math inline">\(\ALPHABET S\)</span>. In particular, let <span class="math inline">\(\{\ALPHABET S_1, \dots \ALPHABET S_n\}\)</span> denote a partition of <span class="math inline">\(\ALPHABET S\)</span> (i.e., <span class="math inline">\(\bigcup_{i=1}^n \ALPHABET S_i = \ALPHABET S\)</span> and for any <span class="math inline">\(i \neq j\)</span>, <span class="math inline">\(\ALPHABET S_i \cap \ALPHABET S_j = \emptyset\)</span>). Pick a representative point <span class="math inline">\(\hat s_i \in \ALPHABET S_i\)</span>. We can think of the “grid points” <span class="math inline">\(\hat {\ALPHABET S} = \{\hat s_1, \dots, \hat s_n\}\)</span> as quantization of the state space <span class="math inline">\(\ALPHABET S\)</span>. To simplify the notation, we define a quantization function <span class="math inline">\(\phi \colon \ALPHABET S \to \hat {\ALPHABET S}\)</span> which maps all points in <span class="math inline">\(\ALPHABET S_i\)</span> to the representative element <span class="math inline">\(\hat s_i\)</span>.</p>
<p>We consider a finite state MDP <span class="math inline">\(\widehat {\ALPHABET M} = \langle \hat {\ALPHABET S}, \ALPHABET A, \hat P, \hat c, γ\rangle\)</span>, where <span class="math inline">\(\hat c\)</span> is the restriction of <span class="math inline">\(c\)</span> onto <span class="math inline">\(\hat {\ALPHABET S}\)</span>, and <span class="math inline">\(\hat P\)</span> is given by</p>
<p><span class="math display">\[\hat P(\hat s_j | \hat s_i, a) =
\int_{\ALPHABET S_j} P(s&#39; | \hat s_i, a) dy = P(\ALPHABET S_j | \hat s_i, a).
\]</span></p>
<p>Since model <span class="math inline">\(\widehat {\ALPHABET M}\)</span> is discrete, we can find the optimal value function and policy using dynamic programming. We are interested in the questions of quantifying the errors in value approximation and policy approximation in this setup. Since the quantized model is defined on a state space <span class="math inline">\(\hat {\ALPHABET S}\)</span> that is different from the original state space, the approximation bounds derived above are not directly applicable. Below we present two intermediate models, to be able to compare models <span class="math inline">\(\ALPHABET M\)</span> and <span class="math inline">\(\widehat {\ALPHABET M}\)</span></p>
<h2 data-number="2.1" id="an-intermediate-model"><span class="header-section-number">2.1</span> An intermediate model</h2>
<p>Define a model <span class="math inline">\(\overline {\ALPHABET M} = \langle \ALPHABET S, \ALPHABET A, \bar p, \bar c, γ \rangle\)</span>, where <span class="math display">\[
  \bar c(s,a) = c(\phi(s),a)
\]</span> and <span class="math display">\[
  \bar p(s&#39;|s,a) = p(s&#39;|\phi(s),a).
\]</span></p>
<p>We have the following property for the <span class="math inline">\(\bar p\)</span> dynamics:</p>
<div class="highlight">
<dl>
<dt><span id="lem:2"></span><span id="lem:bar-vs-hat" class="pandoc-numbering-text lem"><strong>Lemma 2</strong></span></dt>
<dd>
<p>For any <span class="math inline">\(\hat V \colon \hat {\ALPHABET S} \to \reals\)</span>, let <span class="math inline">\(V \colon  \ALPHABET S \to \reals\)</span> be its piecewise constant extrapolation from <span class="math inline">\(\hat {\ALPHABET S}\)</span> to <span class="math inline">\(\ALPHABET S\)</span> (i.e., <span class="math inline">\(V(s) = \hat V(\phi(s)))\)</span>. Then, for any <span class="math inline">\(\hat s \in \ALPHABET S\)</span> and <span class="math inline">\(a \in \ALPHABET A\)</span>, we have <span class="math display">\[
\int_{\ALPHABET S_j} \bar p(s&#39;|\hat s, a) V(s&#39;) ds&#39; 
=
\hat P(\hat s_j | \hat s, a) \hat V(\hat s_j).
   \]</span> Therefore, <span class="math display">\[
\int_{\ALPHABET S} \bar p(s&#39;|\hat s, a) V(s&#39;) ds&#39; 
=
\sum_{\hat s&#39; \in \hat {\ALPHABET S}} \hat P(\hat s&#39; | \hat s, a) \hat V(\hat s_j).
   \]</span></p>
</dd>
</dl>
</div>
<details>
<!--{{{ Proof -->
<summary>
<h4 class="unnumbered" id="proof-4">Proof</h4>
</summary>
<div>
<p>Since <span class="math inline">\(V\)</span> is constant over <span class="math inline">\(\ALPHABET S_j\)</span>, we have <span class="math display">\[\begin{align*}
  \int_{\ALPHABET S_j} \bar p(s&#39;|\hat s,a) V(s&#39;) ds&#39; 
  &amp;=
  V(\hat s_j) \int_{\ALPHABET S_j} \bar p(s&#39;|\hat s,a) ds&#39; 
  \notag \\
  &amp;= V(\hat s_j) \bar p(\ALPHABET S_j | \hat s, a)
\end{align*}\]</span> The first result now follows from observing that <span class="math inline">\(\bar p(\ALPHABET S_j | \hat s, a) = p(\ALPHABET S_j | \hat s, a) = \hat P(\hat s_j | \hat s, a)\)</span>. The second result follows from the fact that for any <span class="math inline">\(f \colon \ALPHABET S \to \reals\)</span>, <span class="math display">\[
  \int_{\ALPHABET S} f(s&#39;)  ds&#39; = \int_{\ALPHABET S_1} f(s&#39;) ds&#39; + \cdots +
  \int_{\ALPHABET S_n} f(s&#39;) d(s&#39;).
\]</span></p>
</div>
</details>
<!--}}}-->
<p>Let <span class="math inline">\(\bar V^*\)</span>, and <span class="math inline">\(\hat V^*\)</span> denote the optimal value functions of models <span class="math inline">\(\overline {\ALPHABET M}\)</span> and <span class="math inline">\(\widehat {\ALPHABET M}\)</span>, respectively. Moreover, let <span class="math inline">\(\bar π^*\)</span> and <span class="math inline">\(\hat π^*\)</span> denote optimal policies of models <span class="math inline">\(\widehat {\ALPHABET M}\)</span>, and <span class="math inline">\(\overline {\ALPHABET M}\)</span>, respectively. Then, these are related as follows.</p>
<div class="highlight">
<dl>
<dt><span id="lem:3"></span><span id="lem:equiv-aggregation" class="pandoc-numbering-text lem"><strong>Lemma 3</strong></span></dt>
<dd>
<p><span class="math inline">\(\bar V^*\)</span> is a piecewise constant extrapolation of <span class="math inline">\(\hat V^*\)</span> from <span class="math inline">\(\hat {\ALPHABET S}\)</span> to <span class="math inline">\(\ALPHABET S\)</span>. Moreover, the piecewise constant extrapolation of <span class="math inline">\(\hat π^*\)</span> is optimal for model <span class="math inline">\(\overline {\ALPHABET M}\)</span>.</p>
</dd>
</dl>
</div>
<details>
<!--{{{ Proof -->
<summary>
<h4 class="unnumbered" id="proof-5">Proof</h4>
</summary>
<div>
<p>Let <span class="math inline">\(\bar V\)</span> be the piecewise constant extrapolation of <span class="math inline">\(\hat V^*\)</span> from <span class="math inline">\(\hat {\ALPHABET S}\)</span> to <span class="math inline">\(\ALPHABET S\)</span>. <a href="#lem:bar-vs-hat" title="Lemma 2"><span class="pandoc-numbering-link lem">Lemma 2</span></a> implies that for any <span class="math inline">\(s \in \ALPHABET S\)</span>, the one step Bellman update of <span class="math inline">\(\bar V\)</span> (in model <span class="math inline">\(\overline {\ALPHABET M}\)</span>) is given by <span class="math display">\[\begin{align*}
  \min_{a \in \ALPHABET S} \biggl\{
    c(\phi(s),a) + γ \int_{\ALPHABET S} \bar p(s&#39;|\phi(s),a) \bar V(s&#39;) ds&#39;
  \biggr\}
  &amp;= 
  \min_{a \in \ALPHABET S} \biggl\{
    \hat c(\phi(s),a) + γ \sum_{\hat s&#39; \in \hat {\ALPHABET S}} 
    \hat P(\hat s&#39; | \phi(s), a) \hat V^*(\hat s&#39;)
  \biggr\}
  \\
  &amp;=
  \hat V^*(\phi(s)) 
  \\
  &amp;= \bar V(s)
\end{align*}\]</span> where the first equality follows from <a href="#lem:bar-vs-hat" title="Lemma 2"><span class="pandoc-numbering-link lem">Lemma 2</span></a>, the second equality follows from the fact that <span class="math inline">\(\hat V^*\)</span> is optimal for model <span class="math inline">\(\widehat {\ALPHABET M}\)</span>, and the last equality follows from the definition of <span class="math inline">\(\bar V\)</span>. Thus, <span class="math inline">\(\bar V\)</span> is the fixed point of the Bellman optimality equation for model <span class="math inline">\(\overline {\ALPHABET M}\)</span>. Hence, <span class="math inline">\(\bar V = \bar V^*\)</span>.</p>
<p>The above equation also implies that the policy <span class="math inline">\(\hat π^*(\phi(s))\)</span>, which achieves the arg min in the second equality, is optimal for model <span class="math inline">\(\overline{\ALPHABET M}\)</span>.</p>
</div>
</details>
<!--}}}-->
<p><a href="#lem:equiv-aggregation" title="Lemma 3"><span class="pandoc-numbering-link lem">Lemma 3</span></a> implies that the questions of value and policy approximation for model <span class="math inline">\(\widehat {\ALPHABET M}\)</span> is the same as those for model <span class="math inline">\(\overline {\ALPHABET M}\)</span>. The advantage of working with model <span class="math inline">\(\overline {\ALPHABET M}\)</span> is that it is defined on the same state space as model <span class="math inline">\(\ALPHABET M\)</span>, so we can directly use the results of <a href="#thm:policy-bound" title="Theorem 2"><span class="pandoc-numbering-link thm">Theorem 2</span></a> and <a href="#thm:policy-bound2" title="Theorem 3"><span class="pandoc-numbering-link thm">Theorem 3</span></a>.</p>
<h2 data-number="2.2" id="approximation-bounds"><span class="header-section-number">2.2</span> Approximation bounds</h2>
<p>Now to bound the value error between models <span class="math inline">\(\ALPHABET M\)</span> and <span class="math inline">\(\overline {\ALPHABET M}\)</span>, we need to compute <span class="math inline">\(\ALPHABET D^* V^*\)</span>, which is given by <span class="math display">\[\begin{align*}
  \ALPHABET D^* V^* &amp;= 
  \max_{s \in \ALPHABET S}
  \biggl| 
    \min_{a \in \ALPHABET A} \bigl\{ 
    c(s,a) + γ \int_{\ALPHABET S} p(s&#39;|s,a) V^*(s&#39;)ds&#39;
    \bigr\}
    \notag \\
  &amp;  \hskip 4em -
    \min_{a \in \ALPHABET A} \bigl\{ 
   c(\phi(s), a) + γ \int_{\ALPHABET S} p(s&#39;|\phi(s), a) V^*(s&#39;) ds&#39; 
    \bigr\}
  \biggr|
  \notag \\
  &amp;= \max_{s \in \ALPHABET S} \bigl| V^*(s) - V^*(\phi(s)) \bigr|
  =: H_{\max}.
\end{align*}\]</span></p>
<p>Therefore, from <a href="#thm:value-bound" title="Theorem 1"><span class="pandoc-numbering-link thm">Theorem 1</span></a>, we have</p>
<div class="highlight">
<dl>
<dt><span id="prop:1"></span><span id="prop:value-aggregation" class="pandoc-numbering-text prop"><strong>Proposition 1</strong></span></dt>
<dd>
<p><span class="math display">\[ \| V^* - V^{\bar π^*} \|_∞ \le \frac{H_{\max}}{1-γ}. \]</span></p>
</dd>
</dl>
</div>
<p>To bound the policy error, we also need to compute <span class="math inline">\(\ALPHABET D^{\hat π^*} V^*\)</span>. In order to compute this bound, we assume that the model is <span class="math inline">\((L_c, L_p)\)</span> Lipschitz.</p>
<dl>
<dt><span id="ass:1"></span><span id="ass:lip" class="pandoc-numbering-text ass"><strong>Assumpt. 1</strong></span></dt>
<dd>
<ul>
<li>For every <span class="math inline">\(a \in \ALPHABET A\)</span>, <span class="math inline">\(c(s, a)\)</span> is <span class="math inline">\(L_c\)</span>-Lipschitz in <span class="math inline">\(s\)</span></li>
<li>For every <span class="math inline">\(a \in \ALPHABET A\)</span>, <span class="math inline">\(p(\cdot | s, a)\)</span> is <span class="math inline">\(L_p\)</span>-Lipschitz in <span class="math inline">\(s\)</span> (with respect to the Kantorovich distance on probability measures).</li>
</ul>
</dd>
</dl>
<p>Under this assumption, we have</p>
<p><span class="math display">\[\begin{align*}
  \ALPHABET D^{\hat π^*} V^* &amp;\le \max_{s \in \ALPHABET S}
  \sum_{a \in \ALPHABET A} \bar π^*(a | s) \biggl|
    c(s,a) + γ \int_{\ALPHABET S} p(s&#39;|s,a) V^*(s&#39;)ds&#39;
  \notag \\
  &amp;  \hskip 4em -
    c(\phi(s), a) - γ \int_{\ALPHABET S} p(s&#39;|\phi(s), a) V^*(s&#39;) ds&#39; 
  \biggr|
  \notag \\
  &amp;\le  L_c \max_{s \in \ALPHABET S}d(s, \phi(s)) 
  + 
  γ K(p(\cdot | s, a), p(\cdot | \phi(s), a)) \NORM{V^*}_L )
  \notag \\
  &amp;\le ( L_c  + γ L_p \NORM{V^*}_L ) 
  \underbrace{\max_{s \in \ALPHABET S}d(s, \phi(s)) }_{=: D}
\end{align*}\]</span> where the second inequality uses <a href="../lipschitz-mdp#lemma:Kantorovich">Lemma 1 from the notes on Lipschitz MDP</a>.</p>
<p>Thus, we have the following:</p>
<div class="highlight">
<dl>
<dt><span id="prop:2"></span><span id="prop:policy-aggregation" class="pandoc-numbering-text prop"><strong>Proposition 2</strong></span></dt>
<dd>
<p>Under <a href="#ass:lip" title="Assumpt. 1"><span class="pandoc-numbering-link ass">Assumpt. 1</span></a>, we have <span class="math display">\[\NORM{V^* - V^{\bar π^*}}_∞ \le 
  \frac{D}{1-γ} \biggl[ L_c + γ L_p \NORM{V^*}_L + 
  \frac{1 + γ}{1-γ} \NORM{V^*}_L \biggr]. \]</span></p>
<p>Furthermore, if <span class="math inline">\(γ L_p &lt; 1\)</span>, then from <a href="../lipschitz-mdp#thm:Lipschitz-opt">Theorem 1 of Lipschitz MDPs</a> we know that <span class="math inline">\(\NORM{V^*}_L \le L_c/(1- γ L_p)\)</span>. Thus, <span class="math display">\[\NORM{V^{μ^*} - V^*}_∞ \le 
  \frac{2 D L_c }{ (1-γ)^2 (1-γ L_p) }. \]</span></p>
</dd>
</dl>
</div>
<details>
<!--{{{ Proof -->
<summary>
<h4 class="unnumbered" id="proof-6">Proof</h4>
</summary>
<div>
<p>The first bound follows from <a href="#thm:policy-bound2" title="Theorem 3"><span class="pandoc-numbering-link thm">Theorem 3</span></a>, with the additional observation that <span class="math inline">\(H_{\max} \le \NORM{V^*}_L D\)</span>.</p>
<p>For the second result follows from simple algebra.</p>
</div>
</details>
<!--}}}-->


<p class="categories">
This entry 

 was last updated on 17 Nov 2022
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/infinite-horizon">infinite horizon</a>,
<a href="https://adityam.github.io/stochastic-control/tags/discounted-cost">discounted cost</a>,
<a href="https://adityam.github.io/stochastic-control/tags/lipschitz-continuity">lipschitz continuity</a>,
<a href="https://adityam.github.io/stochastic-control/tags/approximation-bounds">approximation bounds</a>,
<a href="https://adityam.github.io/stochastic-control/tags/state-aggregation">state aggregation</a>.</p>



    </div>
  </body>
</html>


