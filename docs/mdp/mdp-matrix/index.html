<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Aditya Mahajan" />
  <meta name="title" content="ECSE 506: Stochastic Control and Decision Theory" />
  <title>ECSE 506: Stochastic Control and Decision Theory</title>
  
  <meta content="numerical methods" name="keywords" />
  

  <link rel="stylesheet" href="https://adityam.github.io/stochastic-control//css/style.css" type="text/css" /><script type="text/javascript"
    src="https://cdn.jsdelivr.net/npm/mathjs@7.0.1/dist/math.min.js">
  </script><script type="text/javascript"
    src="https://adityam.github.io/stochastic-control/js/mathjax-local.js" defer>
  </script>
  <script type="text/javascript" id="MathJax-script" defer
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
  </script>

  <script type="module" defer
    src="//instant.page/3.0.0"
    integrity="sha384-OeDn4XE77tdHo8pGtE1apMPmAipjoxUQ++eeJa6EtJCfHlvijigWiJpD7VDPWXV1">
  </script>

  <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101261731);</script>
  <script async src="//static.getclicky.com/js"></script>

</head>
<body>
<div id="content">
<div class="title">
  <h1>ECSE 506: Stochastic Control and Decision Theory </h1>
  <h2><a href="http://www.cim.mcgill.ca/~adityam/">Aditya Mahajan</a> <br/>
      Winter 2022
  </h2>
  <h3><a href="https://adityam.github.io/stochastic-control/ ">About</a>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//lectures">Lectures</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//notes">Notes</a></span>
	&nbsp;<small><small>|</small></small>&nbsp;
    <a href="https://adityam.github.io/stochastic-control//coursework">Coursework</a>
</h3>
</div>



<div class="h1-title">Numerics: Matrix formulation of Markov decision processes</div>

<figure>
<img src="/stochastic-control/banners/slate.gif" title="Shut up and compute" style="max-width:30em;;width:100.0%" alt="Image credit: https://etc.usf.edu/clipart/28400/28480/young_boy_28480.htm" />
<figcaption aria-hidden="true">Image credit: https://etc.usf.edu/clipart/28400/28480/young_boy_28480.htm</figcaption>
</figure>
<h1 data-number="1" id="mdps-as-controlled-markov-chains"><span class="header-section-number">1</span> MDPs as controlled Markov chains</h1>
<p>In this section, we present a matrix formulation for finite state finite action MDPs, which is useful for computing the solutions numerically. Let’s start with <a href="../mdp-functional">the function model</a> described earlier and assume that <span class="math inline">\(\ALPHABET S\)</span> and <span class="math inline">\(\ALPHABET A\)</span> are finite sets and that the cost function and the probability distribution of <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span> are time-homogeneous. Then, the following is a fundamental property of MDPs:</p>
<div class="highlight">
<dl>
<dt>Lemma</dt>
<dd>
<p>For any <span class="math inline">\(s_1, s_2, \dots, s_T \in \ALPHABET S\)</span> and <span class="math inline">\(a_1, \dots, a_T \in \ALPHABET A\)</span>, we have <span class="math display">\[ \PR(S_{t+1} = s_{t+1} | S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t})
 = \PR(S_{t+1} = s_{t+1} | S_{t}   = s_t    , A_t = a_t).\]</span> Moreover, the right hand side does not depend on the policy <span class="math inline">\(π\)</span>.</p>
</dd>
</dl>
</div>
<h4 class="unnumbered" id="proof">Proof</h4>
<p>For a given policy <span class="math inline">\(π\)</span> consider <span class="math display">\[\begin{align*}
  \hskip 2em &amp; \hskip -2em
  \PR(S_{t+1} = s_{t+1} | S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t})
  \\
  &amp;\stackrel{(a)}=
  \PR(\{ w_t \in \ALPHABET W : f_t(s_t, a_t, w_t) = s_{t+1} \}
  | S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t}) 
  \\
  &amp;\stackrel{(b)}=
  \PR(\{ w_t \in \ALPHABET W : f_t(s_t, a_t, w_t) = s_{t+1} \})
  \\
  &amp;=
  \PR(S_{t+1} = s_{t+1} | S_{t} = s_{t}, A_{t} = a_{t})
\end{align*}\]</span> where <span class="math inline">\((a)\)</span> follows from the update equation for <span class="math inline">\(S_{t+1}\)</span> and <span class="math inline">\((b)\)</span> follows because the primitive random variables are independent. The expression in the right hand side of <span class="math inline">\((b)\)</span> does not depend on the control strategy <span class="math inline">\(π\)</span>.</p>
<h1 data-number="2" id="matrix-formulation-of-an-mdp"><span class="header-section-number">2</span> Matrix formulation of an MDP</h1>
<p>Let <span class="math inline">\(|\ALPHABET S| = n\)</span> and <span class="math inline">\(|\ALPHABET A| = m\)</span>. The idea behind the matrix formulation of MDP is to think of <span class="math inline">\(\{S_t\}_{t \ge 1}\)</span> as a <em>controlled</em> Markov process and define the <span class="math inline">\(n \times n\)</span> tranistion matrices <span class="math inline">\(\{P(a)\}_{a \in \ALPHABET A}\)</span> as follows: <span class="math display">\[ P_{ss&#39;}(a) = \PR(S_{t+1} = s&#39; | S_t = s, A_t = a).\]</span></p>
<p>Observe that <span class="math display">\[\EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a] 
  = \sum_{s&#39; \in \ALPHABET S} V_{t+1}(s&#39;) \PR(S_{t+1} = s&#39; | S_t = s, A_t = a).
  \]</span> Now, if we think of <span class="math inline">\(V_t\)</span> as a <span class="math inline">\(n \times 1\)</span> column vector, then we can write <span class="math display">\[ \EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a] = [ P(a)V_{t+1} ]_{s}, \]</span> or, equivalently, <span class="math display">\[ \MATRIX{ \EXP[V_{t+1}(S_{t+1}) | S_t = 1, A_t = a] \\ \vdots \\
    \EXP[V_{t+1}(S_{t+1}) | S_t = n, A_t = a] }
    = P(a)V_{t+1}. \]</span></p>
<p>Now, think of <span class="math inline">\(c_t\)</span> and <span class="math inline">\(Q_t\)</span> as <span class="math inline">\(n \times m\)</span> matrices. Then, we can write <span class="math display">\[ Q_t = c_t + \MATRIX { P(1)V_{t+1} &amp; \cdots &amp; P(n) V_{t+1} }. \]</span></p>
<p>Define <span class="math inline">\(\bar P = \MATRIX{P(1) \\ \vdots \\ P(n)}\)</span>, then <span class="math display">\[ Q_t = c_t + \text{reshape}( \bar P \, V_{t+1}, n, m). \]</span> Hence, the key step of dynamic programming can be done using simple linear algebra and therefore implemented efficiently in any programming language.</p>
<h1 data-number="3" id="an-example-machine-repair"><span class="header-section-number">3</span> An Example: Machine repair</h1>
<p>Consider a machine which can be in one of <span class="math inline">\(n\)</span> states <span class="math inline">\(\ALPHABET S = \{1, \dots, n\}\)</span>, where state <span class="math inline">\(1\)</span> denotes the best state and state <span class="math inline">\(n\)</span> denotes the worse state.</p>
<p>There are two actions <span class="math inline">\(\ALPHABET A = \{1, 2\}\)</span>, where <span class="math inline">\(a=1\)</span> means that the current machine is replaced by a brand new one (which starts in state <span class="math inline">\(1\)</span>) and <span class="math inline">\(a=2\)</span> means that the existing machine is used. When action <span class="math inline">\(a=2\)</span> is chosen, the machine state will either remain the same (with probability <span class="math inline">\(1 - θ\)</span>) or deteriorate by one unit (with probability <span class="math inline">\(θ\)</span>).</p>
<p>For example, if <span class="math inline">\(n = 3\)</span>, then <span class="math display">\[ P(1) = \MATRIX{1 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 0},
\qquad
P(2) = \MATRIX{1-θ &amp;  θ &amp;  0\\ 0 &amp;  1-θ &amp; θ\\ 0 &amp;  0 &amp;  1}. \]</span></p>
<p>Now suppose that the replace action <span class="math inline">\(a = 1\)</span> costs <span class="math inline">\(λ\)</span> unit (irrespective of the state) and the do nothing action <span class="math inline">\(a = 2\)</span> costs <span class="math inline">\(0\)</span> in state <span class="math inline">\(1\)</span>, <span class="math inline">\(1\)</span> in state <span class="math inline">\(2\)</span>, and <span class="math inline">\(5\)</span> in state <span class="math inline">\(3\)</span>. <span class="math display">\[ c = \MATRIX{λ &amp; 0 \\ λ &amp; 1 \\ λ &amp; 5 }. \]</span></p>
Then, the value functions and optimal policy for <span class="math inline">\(T = 4\)</span> and specific values of <span class="math inline">\(θ\)</span> and <span class="math inline">\(λ\)</span> are shown below. For visualization we show the replace action by <span class="math inline">\(\bullet\)</span> and the do thing action by <span class="math inline">\(\circ\)</span>. You can play around with the parameters $θ = $ <input id="theta" 
       type="number" step = 0.1
       maxlength="3" size="3"
       min="0" max="1"
       onkeyup="showMatrix()"
       onchange="showMatrix()"
       value="" /> and $λ = $ <input id="lambda" 
       type="number" step = 1
       maxlength="1" size="2"
       min="0" max="20"
       onkeyup="showMatrix()"
       onchange="showMatrix()"
       value="" /> <button onClick="showMatrix()">Update</button>
<p id="machine-repair">
</p>
<script>
if (pagedata == undefined) { var pagedata = { } }

pagedata.machineRepair = { theta : 0.3, lambda : 8 } 

// Set initial values
document.getElementById('theta').value  = pagedata.machineRepair.theta;
document.getElementById('lambda').value = pagedata.machineRepair.lambda;

let promise = Promise.resolve();  // Used to hold chain of typesetting calls

function typeset(code) {
  promise = promise.then(() => {code(); return MathJax.typesetPromise()})
                   .catch((err) => console.log('Typeset failed: ' + err.message));
  return promise;
}

function showMatrix(){
  var theta  = document.getElementById('theta').valueAsNumber;
  var lambda = document.getElementById('lambda').valueAsNumber;

  const P1 = math.matrix([ [1,0,0], [1,0,0], [1,0,0] ]);
  const P2 = math.matrix([ [1-theta, theta, 0], [0, 1-theta, theta], [0, 0, 1] ]);
  const P  = math.concat(P1, P2, 0);

  const c  = math.matrix([[lambda, 0], [lambda, 1], [lambda, 5]]);

  const [n, m] = c.size();
  const T  = 4;
  const V  = [];
  const π  = [];

  V[T+1] = math.zeros(n)
  for (var t = T; t > 0; t--) {
    // math.reshape behaves differently from Julia. It reshapes in 
    // row major format rather than column major. So, we need an extra transpose

    var Q = math.add(c, math.transpose(math.reshape( math.multiply(P, V[t+1]), [m,n] )))
            .toArray();
            
    // There is no argmin function in the math.js library
    // Since there are only two actions, we make the comparison by hand
    π[t] = [];
    V[t] = [];
    for(var s = 0; s < n; s++) {
       if(Q[s][0] < Q[s][1]) {
          V[t][s] = Q[s][0];
          π[t][s] = "\\bullet";
       } else {
          V[t][s] = Q[s][1];
          π[t][s] = "\\circ";
       }
    }
    V[t] = math.matrix(V[t]);
  }

  function VtoTeX(matrix) {
      const arr = matrix.toArray(); // A col vector
      return `\\MATRIX{ ${arr[0].toFixed(2)} \\\\ 
                        ${arr[1].toFixed(2)} \\\\ 
                        ${arr[2].toFixed(2)} }`
  }

  function gtoTeX(arr) {
      return `\\MATRIX{ ${arr[0]} \\\\ 
                        ${arr[1]} \\\\ 
                        ${arr[2]} }`
  }

  typeset(() => { 
      const id = document.getElementById('machine-repair');
      id.innerHTML = `$$\\begin{align*}
        V_4 &= ${VtoTeX(V[4])}, &
        V_3 &= ${VtoTeX(V[3])}, &
        V_2 &= ${VtoTeX(V[2])}, &
        V_1 &= ${VtoTeX(V[1])}. \\\\
        π_4 &= ${gtoTeX(π[4])}, &
        π_3 &= ${gtoTeX(π[3])}, &
        π_2 &= ${gtoTeX(π[2])}, &
        π_1 &= ${gtoTeX(π[1])}. \\\\
       \\end{align*}$$`
      return id;
  })
}

showMatrix()
</script>
<p>Couple of features of the optimal solution are worth noting:</p>
<ul>
<li><p>For <span class="math inline">\(λ\)</span> roughly in the range of <span class="math inline">\((5,15)\)</span> the optimal policy chooses the replace action in state <span class="math inline">\(3\)</span> at time <span class="math inline">\(1\)</span> even though taking the do nothing action incurs less instanteous cost.</p></li>
<li><p>There is a range of values of <span class="math inline">\(λ\)</span> for which the the optimal policy is the same. This argument is sometimes used to illustrate why searching in the policy space can be more efficient than searching in the value space.</p></li>
<li><p>It appears that the optimal policy satisifies a <em>monotonicity</em> property: for low states take “do nothing” action and for higher states take the “replace” action. The actual value of what constitutes a low or a high state depends on the parameters of the model. <a href="../monotonicity">Later in the course</a>, we will study sufficient conditions to establish monotonicity of optimal policy. It is easy to show that the setup described above <a href="../monotonicity/#exercises">satisfies these sufficient conditions</a>, so the optimal policy is monotone for all choices of model parameters.</p></li>
</ul>


<p class="categories">
This entry 

 was last updated on 08 Feb 2022
 and posted in 

<a href="https://adityam.github.io/stochastic-control/categories/mdp">
  MDP</a>
and tagged
<a href="https://adityam.github.io/stochastic-control/tags/numerical-methods">numerical methods</a>.</p>



    </div>
  </body>
</html>


