<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Aditya Mahajan">
<meta name="dcterms.date" content="2023-05-31">
<meta name="keywords" content="MDPs, Markov strategies, dynamic programming, comparison principle, principle of irrelevant information">
<meta name="description" content="ECES 506 (Stochastic Control and Decision Theory)">

<title>Course Notes - 3&nbsp; Finite horizon MDPs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../mdps/gambling.html" rel="next">
<link href="../stochastic-optimization/newsvendor.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script src="../site_libs/quarto-contrib/nutshell-1.0.6/nutshell.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
window.MathJax = {
  tex: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: true,
    tags: "ams",
    macros: {
      PR: "\\mathbb{P}",
      EXP: "\\mathbb{E}",
      IND: "\\mathbb{I}",
      ONES: "\\mathbb{1}",
      reals: "\\mathbb{R}",
      integers: "\\mathbb{Z}",
      BLANK: "\\mathfrak{E}",
      TRANS: "\\intercal",
      BELLMAN: "\\mathcal{B}",
      MISMATCH: "\\mathcal{D}",
      VEC: "\\operatorname{vec}",
      diag: "\\operatorname{diag}",
      ROWS: "\\operatorname{vec}",
      TR: "\\operatorname{Tr}",   
      SPAN: "\\operatorname{sp}",   
      ALPHABET: ["\\mathcal{#1}", 1],
      MATRIX: ["\\begin{bmatrix} #1 \\end{bmatrix}", 1],
      NORM: ["\\left\\lVert #1 \\right\\rVert", 1],
      ABS: ["\\left\\lvert #1 \\right\\rvert", 1],
      GRAD: "\\nabla"
    },
  },
  options: {
    ignoreHtmlClass: 'tex2jax_ignore',
    processHtmlClass: 'tex2jax_process'
  },
};
</script>
<script async="" data-id="101261731" src="//static.getclicky.com/js"></script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="citation_title" content="[3]{.chapter-number}&nbsp; [Finite horizon MDPs]{.chapter-title}">
<meta name="citation_keywords" content="MDPs,Markov strategies,dynamic programming,comparison principle,principle of irrelevant information">
<meta name="citation_author" content="Aditya Mahajan">
<meta name="citation_publication_date" content="2023-05-31">
<meta name="citation_cover_date" content="2023-05-31">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-05-31">
<meta name="citation_fulltext_html_url" content="https://adityam.github.io/stochastic-control//mdps/intro.html">
<meta name="citation_language" content="en">
<meta name="citation_journal_title" content="Course notes for ECSE 506 (Stochastic Control and Decision Theorey)">
<meta name="citation_reference" content="citation_title=A new interpretation of information rate;,citation_author=John L. Kelly;,citation_publication_date=1956-07;,citation_cover_date=1956-07;,citation_year=1956;,citation_issue=4;,citation_doi=10.1002/j.1538-7305.1956.tb03809.x;,citation_volume=35;,citation_journal_title=Bell System Technical Journal;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Dynamic programming and gambling models;,citation_author=Sheldon M. Ross;,citation_publication_date=1974-09;,citation_cover_date=1974-09;,citation_year=1974;,citation_issue=3;,citation_doi=10.2307/1426236;,citation_volume=6;,citation_journal_title=Advances in Applied Probability;,citation_publisher=Applied Probability Trust;">
<meta name="citation_reference" content="citation_title=Optimal inventory policy;,citation_author=Kenneth J Arrow;,citation_author=Theodore Harris;,citation_author=Jacob Marschak;,citation_publication_date=1952-01;,citation_cover_date=1952-01;,citation_year=1952;,citation_issue=1;,citation_doi=10.2307/1907830;,citation_volume=20;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=On the optimal inventory equation;,citation_author=Richard Bellman;,citation_author=Irving Glicksberg;,citation_author=Oliver Gross;,citation_publication_date=1955-10;,citation_cover_date=1955-10;,citation_year=1955;,citation_issue=1;,citation_doi=10.1287/mnsc.2.1.83;,citation_volume=2;,citation_journal_title=Management Science;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Memoryless strategies in finite-stage dynamic programming;,citation_author=David Blackwell;,citation_publication_date=1964-06;,citation_cover_date=1964-06;,citation_year=1964;,citation_issue=2;,citation_doi=10.1214/aoms/1177703586;,citation_volume=35;,citation_journal_title=The Annals of Mathematical Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=On the structure of real-time source coders;,citation_author=Hans S. Witsenhausen;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_issue=6;,citation_volume=58;,citation_journal_title=Bell System Technical Journal;">
<meta name="citation_reference" content="citation_title=Contributions to the theory of optimal control;,citation_author=Rudolf Emil Kalman;,citation_publication_date=1960;,citation_cover_date=1960;,citation_year=1960;,citation_volume=5;,citation_journal_title=Boletin de la Sociedad Matematica Mexicana;">
<meta name="citation_reference" content="citation_title=Dynamic programming under uncertainty with a quadratic criterion function;,citation_author=Herbert A Simon;,citation_publication_date=1956-01;,citation_cover_date=1956-01;,citation_year=1956;,citation_issue=1;,citation_doi=10.2307/1905261;,citation_volume=24;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Econometric models and welfare maximization;,citation_author=Henri Theil;,citation_publication_date=1954;,citation_cover_date=1954;,citation_year=1954;,citation_doi=10.1007/978-94-011-2410-2_1;,citation_volume=72;,citation_journal_title=Wirtschaftliches Archiv;">
<meta name="citation_reference" content="citation_title=A note on certainty equivalence in dynamic planning;,citation_author=Henri Theil;,citation_publication_date=1957;,citation_cover_date=1957;,citation_year=1957;,citation_doi=10.1007/978-94-011-2410-2_3;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Power and delay trade-offs in fading channels;,citation_author=Randall Alexander Berry;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_fulltext_html_url=https://dspace.mit.edu/handle/1721.1/9290;,citation_dissertation_institution=Massachusetts Institute of Technology;">
<meta name="citation_reference" content="citation_title=Communication over fading channels with delay constraints;,citation_author=Randall A Berry;,citation_author=Robert G Gallager;,citation_publication_date=2002-05;,citation_cover_date=2002-05;,citation_year=2002;,citation_issue=5;,citation_doi=10.1109/18.995554;,citation_volume=48;,citation_journal_title=IEEE Transactions on Information Theory;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Optimal power-delay tradeoffs in fading channels—small-delay asymptotics;,citation_author=Randall A Berry;,citation_publication_date=2013-06;,citation_cover_date=2013-06;,citation_year=2013;,citation_issue=6;,citation_doi=10.1109/TIT.2013.2253194;,citation_volume=59;,citation_journal_title=IEEE Transactions on Information Theory;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=Evaluating a call option and optimal timing strategy in the stock market;,citation_author=Howard M. Taylor;,citation_publication_date=1967;,citation_cover_date=1967;,citation_year=1967;,citation_fulltext_html_url=http://www.jstor.org/stable/2628546;,citation_issue=1;,citation_issn=00251909, 15265501;,citation_volume=14;,citation_journal_title=Management Science;,citation_publisher=INFORMS;">
<meta name="citation_reference" content="citation_title=Optimal stopping and applications;,citation_author=Thomas S. Ferguson;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_fulltext_html_url=http://www.math.ucla.edu/~tom/Stopping/Contents.html;">
<meta name="citation_reference" content="citation_title=Who solved the secretary problem?;,citation_author=Thomas S Ferguson;,citation_publication_date=1989;,citation_cover_date=1989;,citation_year=1989;,citation_journal_title=Statistical science;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Characterizing the structure of optimal stopping policies;,citation_author=Sechan Oh;,citation_author=Özalp Özer;,citation_publication_date=2016-07;,citation_cover_date=2016-07;,citation_year=2016;,citation_issue=11;,citation_doi=10.1111/poms.12579;,citation_volume=25;,citation_journal_title=Production and Operations Management;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=Markov decision processes: Discrete stochastic dynamic programming;,citation_author=Martin L Puterman;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_doi=10.1002/9780470316887;">
<meta name="citation_reference" content="citation_title=Optimization over time: Dynamic programming and stochastic control. Vol. 1 and 2;,citation_author=Peter Whittle;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;">
<meta name="citation_reference" content="citation_title=Optimal control: Basics and beyond;,citation_author=Peter Whittle;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;">
<meta name="citation_reference" content="citation_title=Dynamic programming and optimal control;,citation_author=Dimitri P Bertsekas;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_fulltext_html_url=http://www.athenasc.com/dpbook.html;,citation_volume=I and II;">
<meta name="citation_reference" content="citation_title=Abstract dynamic programming;,citation_author=Dimitri P Bertsekas;,citation_publication_date=2013;,citation_cover_date=2013;,citation_year=2013;,citation_fulltext_html_url=https://web.mit.edu/dimitrib/www/abstractdp_MIT.html;">
<meta name="citation_reference" content="citation_title=Introduction to stochastic control theory;,citation_author=Karl J. Aström;,citation_publication_date=1970;,citation_cover_date=1970;,citation_year=1970;">
<meta name="citation_reference" content="citation_title=Policy invariance under reward transformations: Theory and application to reward shaping;,citation_author=Andrew Y Ng;,citation_author=Daishi Harada;,citation_author=Stuart Russell;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;,citation_fulltext_html_url=http://aima.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf;,citation_volume=99;,citation_conference_title=ICML;">
<meta name="citation_reference" content="citation_title=Theoretical and empirical analysis of reward shaping in reinforcement learning;,citation_author=M. Grzes;,citation_author=D. Kudenko;,citation_publication_date=2009-12;,citation_cover_date=2009-12;,citation_year=2009;,citation_doi=10.1109/ICMLA.2009.33;,citation_conference_title=International conference on machine learning and applications;">
<meta name="citation_reference" content="citation_title=Potential based reward shaping tutorial;,citation_author=Sam Devlin;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=http://www-users.cs.york.ac.uk/~devlin/presentations/pbrs-tut.pdf;">
<meta name="citation_reference" content="citation_title=How many parts to make at once;,citation_author=Ford W Harris;,citation_publication_date=1913-02;,citation_cover_date=1913-02;,citation_year=1913;,citation_issue=2;,citation_doi=10.1287/opre.38.6.947;,citation_volume=10;,citation_journal_title=The magazine of management;">
<meta name="citation_reference" content="citation_title=The mathematical theory of banking;,citation_author=Francis Y Edgeworth;,citation_publication_date=1888;,citation_cover_date=1888;,citation_year=1888;,citation_fulltext_html_url=https://www.jstor.org/stable/2979084;,citation_issue=1;,citation_volume=51;,citation_journal_title=Journal of the Royal Statistical Society;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Methods of operations research;,citation_author=P. Morse;,citation_author=G. Kimball;,citation_publication_date=1951;,citation_cover_date=1951;,citation_year=1951;">
<meta name="citation_reference" content="citation_title=The theory of inventory management;,citation_author=S. Whitin;,citation_publication_date=1953;,citation_cover_date=1953;,citation_year=1953;">
<meta name="citation_reference" content="citation_title=Building intuition: Insights from basic operations management models and principles;,citation_author=Evan L. Porteus;,citation_editor=D. Chhajed;,citation_editor=T. J. Lowe;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_doi=10.1007/978-0-387-73699-0;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Selling random wind;,citation_author=Eilyan Bitar;,citation_author=Kameshwar Poolla;,citation_author=Pramod Khargonekar;,citation_author=Ram Rajagopal;,citation_author=Pravin Varaiya;,citation_author=Felix Wu;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_conference_title=2012 45th hawaii international conference on system sciences;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Fundamental performance limits in cross-layer wireless optimization: Throughput, delay, and energy;,citation_author=Edmund M. Yeh;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=1;,citation_doi=10.1561/0100000014;,citation_issn=1567-2190;,citation_volume=9;,citation_journal_title=Foundations and Trends in Communications and Information Theory;">
<meta name="citation_reference" content="citation_title=Energy-efficient scheduling under delay constraints for wireless networks;,citation_author=Randall Berry;,citation_author=Eytan Modiano;,citation_author=Murtaza Zafer;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_issue=2;,citation_doi=10.2200/S00443ED1V01Y201208CNT011;,citation_volume=5;,citation_journal_title=Synthesis Lectures on Communication Networks;">
<meta name="citation_reference" content="citation_title=Stochastic dominance: Investment decision making under uncertainty;,citation_author=Haim Levy;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_doi=10.1007/978-3-319-21708-6;">
<meta name="citation_reference" content="citation_title=Stochastic dominance and expected utility: Survey and analysis;,citation_author=Haim Levy;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_issue=4;,citation_doi=10.1287/mnsc.38.4.555;,citation_volume=38;,citation_journal_title=Management Science;,citation_publisher=INFORMS;">
<meta name="citation_reference" content="citation_title=Stochastically monotone markov chains;,citation_author=D J Daley;,citation_publication_date=1968;,citation_cover_date=1968;,citation_year=1968;,citation_issue=4;,citation_doi=10.1007/BF00531852;,citation_volume=10;,citation_journal_title=Zeitschrift für Wahrscheinlichkeitstheorie und verwandte Gebiete;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Monotone matrices and monotone markov processes;,citation_author=Julian Keilson;,citation_author=Adri Kester;,citation_publication_date=1977;,citation_cover_date=1977;,citation_year=1977;,citation_issue=3;,citation_volume=5;,citation_journal_title=Stochastic Processes and their Applications;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=On monotonicity of the optimal transmission policy in cross-layer adaptive m -QAM modulation;,citation_author=N. Ding;,citation_author=P. Sadeghi;,citation_author=R. A. Kennedy;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=9;,citation_doi=10.1109/TCOMM.2016.2590427;,citation_issn=1558-0857;,citation_volume=64;,citation_journal_title=IEEE Transactions on Communications;">
<meta name="citation_reference" content="citation_title=Supermodularity and complementarity in economics: An elementary survey;,citation_author=Rabah Amir;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_issue=3;,citation_doi=10.2307/20062066;,citation_issn=00384038;,citation_volume=71;,citation_journal_title=Southern Economic Journal;,citation_publisher=Southern Economic Association;">
<meta name="citation_reference" content="citation_title=Supermodularity and complementarity;,citation_author=Donald M. Topkis;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_isbn=9780691032443;">
<meta name="citation_reference" content="citation_title=Sufficient conditions for the value function and optimal strategy to be even and quasi-convex;,citation_author=J. Chakravorty;,citation_author=A. Mahajan;,citation_publication_date=2018-11;,citation_cover_date=2018-11;,citation_year=2018;,citation_issue=11;,citation_doi=10.1109/TAC.2018.2800796;,citation_issn=2334-3303;,citation_volume=63;,citation_journal_title=IEEE Transactions on Automatic Control;">
<meta name="citation_reference" content="citation_title=On the locality of action domination in sequential decision making;,citation_author=Emmanuel Rachelson;,citation_author=Michail G Lagoudakis;,citation_publication_date=2010-01;,citation_cover_date=2010-01;,citation_year=2010;,citation_fulltext_html_url=https://oatao.univ-toulouse.fr/17977/;,citation_conference_title=Proceedings of 11th international symposium on artificial intelligence and mathematics;">
<meta name="citation_reference" content="citation_title=Lipschitz continuity of value functions in Markovian decision processes;,citation_author=Karl Hinderer;,citation_publication_date=2005;,citation_cover_date=2005;,citation_year=2005;,citation_issue=1;,citation_doi=10.1007/s00186-005-0438-1;,citation_volume=62;,citation_journal_title=Mathematical Methods of Operations Research;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=DeepMDP: Learning continuous latent space models for representation learning;,citation_author=Carles Gelada;,citation_author=Saurabh Kumar;,citation_author=Jacob Buckman;,citation_author=Ofir Nachum;,citation_author=Marc G. Bellemare;,citation_editor=Kamalika Chaudhuri;,citation_editor=Ruslan Salakhutdinov;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=http://proceedings.mlr.press/v97/gelada19a.html;,citation_volume=97;,citation_conference_title=Proceedings of the 36th international conference on machine learning;,citation_conference=PMLR;,citation_series_title=Proceedings of machine learning research;">
<meta name="citation_reference" content="citation_title=Convergence of discretization procedures in dynamic programming;,citation_author=Demitri Bertsekas;,citation_publication_date=1975-06;,citation_cover_date=1975-06;,citation_year=1975;,citation_issue=3;,citation_doi=10.1109/TAC.1975.1100984;,citation_issn=2334-3303;,citation_volume=20;,citation_journal_title=IEEE Transactions on Automatic Control;">
<meta name="citation_reference" content="citation_title=Towards a unified theory of state abstraction for MDPs;,citation_author=Lihong Li;,citation_author=Thomas J Walsh;,citation_author=Michael L Littman;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_fulltext_html_url=http://anytime.cs.umass.edu/aimath06/proceedings/P21.pdf;,citation_conference_title=ISAIM;">
<meta name="citation_reference" content="citation_title=Death and discounting;,citation_author=A. Shwartz;,citation_publication_date=2001-04;,citation_cover_date=2001-04;,citation_year=2001;,citation_fulltext_html_url=https://doi.org/10.1109/9.917668;,citation_issue=4;,citation_doi=10.1109/9.917668;,citation_volume=46;,citation_journal_title=IEEE Transactions on Automatic Control;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Constrained markov decision processes;,citation_author=Eitan. Altman;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;,citation_fulltext_html_url=http://www-sop.inria.fr/members/Eitan.Altman/TEMP/h.pdf;">
<meta name="citation_reference" content="citation_title=Optimal investment policies for the horse race model&amp;amp;amp;quot;;,citation_author=Thomas S. Ferguson;,citation_author=C. Zachary Gilstein;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_fulltext_html_url=https://www.math.ucla.edu/~tom/papers/unpublished/Zach2.pdf;">
<meta name="citation_reference" content="citation_title=Discovery of the kalman filter as a practical tool for aerospace and;,citation_author=Stanley F. Mcgee;,citation_publication_date=1985;,citation_cover_date=1985;,citation_year=1985;,citation_fulltext_html_url=https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19860003843.pdf;,citation_technical_report_institution=National Aeronautics; Space Administration;">
<meta name="citation_reference" content="citation_title=Stochastic systems: Estimation identification and adaptive control;,citation_author=P. R. Kumar;,citation_author=Pravin Varaiya;,citation_publication_date=1986;,citation_cover_date=1986;,citation_year=1986;">
<meta name="citation_reference" content="citation_title=Stochastic dynamic programming and the control of queueing systems;,citation_author=Linn I. Sennott;,citation_publication_date=1999;,citation_cover_date=1999;,citation_year=1999;,citation_isbn=0-471-16120-9;">
<meta name="citation_reference" content="citation_title=Discrete-time controlled Markov processes with average cost criterion - a survey;,citation_author=Aristotle Arapostathis;,citation_author=Vivek S. Borkar;,citation_author=Emmaneul Fernandez-Gaucherand;,citation_author=Mrinak K. Ghosh;,citation_author=Steven I. Marcus;,citation_publication_date=1993-03;,citation_cover_date=1993-03;,citation_year=1993;,citation_issue=2;,citation_volume=31;,citation_journal_title=SIAM Journal of Control and Optimization;">
<meta name="citation_reference" content="citation_title=The optimal control of partially observable markov processes over a finite horizon;,citation_author=Richard D. Smallwood;,citation_author=Edward J. Sondik;,citation_publication_date=1973-10;,citation_cover_date=1973-10;,citation_year=1973;,citation_issue=5;,citation_doi=10.1287/opre.21.5.1071;,citation_volume=21;,citation_journal_title=Operations Research;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Convex analytic methods in markov decision processes;,citation_author=Vivek S. Borkar;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_doi=10.1007/978-1-4615-0805-2_11;,citation_inbook_title=International series in operations research &amp;amp;amp; management science;">
<meta name="citation_reference" content="citation_title=A convex analytic approach to markov decision processes;,citation_author=Vivek S. Borkar;,citation_publication_date=1988-08;,citation_cover_date=1988-08;,citation_year=1988;,citation_issue=4;,citation_doi=10.1007/bf00353877;,citation_volume=78;,citation_journal_title=Probability Theory and Related Fields;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Applications of markov decision processes in communication networks;,citation_author=Eitan Altman;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_doi=10.1007/978-1-4615-0805-2_16;,citation_inbook_title=International series in operations research &amp;amp;amp; management science;">
<meta name="citation_reference" content="citation_title=Water reservoir applications of markov decision processes;,citation_author=Bernard F. Lamond;,citation_author=Abdeslem Boukhtouta;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;,citation_doi=10.1007/978-1-4615-0805-2_17;,citation_inbook_title=International series in operations research &amp;amp;amp; management science;">
<meta name="citation_reference" content="citation_title=Asymptotically efficient adaptive allocation rules;,citation_author=T. L Lai;,citation_author=Herbert Robbins;,citation_publication_date=1985;,citation_cover_date=1985;,citation_year=1985;,citation_issue=1;,citation_doi=http://dx.doi.org/10.1016/0196-8858(85)90002-8;,citation_issn=0196-8858;,citation_volume=6;,citation_journal_title=Advances in Applied Mathematics;">
<meta name="citation_reference" content="citation_title=Dynamic programming;,citation_author=Richard Bellman;,citation_publication_date=1957;,citation_cover_date=1957;,citation_year=1957;">
<meta name="citation_reference" content="citation_title=A dynamic allocation index for the discounted multiarmed bandit problem;,citation_author=J. C. Gittins;,citation_author=D. M. Jones;,citation_publication_date=1974;,citation_cover_date=1974;,citation_year=1974;,citation_volume=9;,citation_inbook_title=Progress in statistics;">
<meta name="citation_reference" content="citation_title=Bandit processes and dynamic allocation indices;,citation_author=John C Gittins;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_issue=2;,citation_volume=41;,citation_journal_title=Journal of the Royal Statistical Society. Series B (Methodological);,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Multi-armed bandits and the Gittins index;,citation_abstract=A plausible conjecture (C) has the implication that a relationship (12) holds between the maximal expected rewards for a multi-project process and for a one-project process (F and Ï&amp;amp;amp;lt;sub&amp;gt;i&amp;lt;/sub&amp;gt; respectively), if the option of retirement with reward M is available. The validity of this relation and optimality of Gittins’ index rule are verified simultaneously by dynamic programming methods. These results are partially extended to the case of so-called &amp;quot;bandit superprocesses&amp;quot;.;,citation_author=P. Whittle;,citation_publication_date=1980;,citation_cover_date=1980;,citation_year=1980;,citation_issue=2;,citation_issn=00359246;,citation_volume=42;,citation_journal_title=Journal of the Royal Statistical Society. Series B (Methodological);,citation_publisher=[Royal Statistical Society, Wiley];">
<meta name="citation_reference" content="citation_title=Four proofs of Gittins’ multiarmed bandit theorem;,citation_abstract=We study four proofs that the Gittins index priority rule is optimal for alternative bandit processes. These include Gittins’ original exchange argument, Weber’s prevailing charge argument, Whittle’s Lagrangian dual approach, and Bertsimas and Niño-Mora’s proof based on the achievable region approach and generalized conservation laws. We extend the achievable region proof to infinite countable state spaces, by using infinite dimensional linear programming theory.;,citation_author=Esther Frostig;,citation_author=Gideon Weiss;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_fulltext_html_url=http://dx.doi.org/10.1007/s10479-013-1523-0;,citation_issue=1;,citation_doi=10.1007/s10479-013-1523-0;,citation_issn=1572-9338;,citation_volume=241;,citation_journal_title=Annals of Operations Research;">
<meta name="citation_reference" content="citation_title=The multi-armed bandit problem: Decomposition and computation;,citation_author=Michael N Katehakis;,citation_author=Arthur F Veinott;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_issue=2;,citation_volume=12;,citation_journal_title=Mathematics of Operations Research;,citation_publisher=INFORMS;">
<meta name="citation_reference" content="citation_title=Multi-armed bandits under general deprecation and commitment;,citation_author=Wesley Cowan;,citation_author=Michael N. Katehakis;,citation_publication_date=2015-10;,citation_cover_date=2015-10;,citation_year=2015;,citation_issue=1;,citation_doi=10.1017/s0269964814000217;,citation_volume=29;,citation_journal_title=Probability in the Engineering and Informational Sciences;,citation_publisher=Cambridge University Press (CUP);">
<meta name="citation_reference" content="citation_title=Restless bandits: Activity allocation in a changing world;,citation_author=Peter Whittle;,citation_publication_date=1988;,citation_cover_date=1988;,citation_year=1988;,citation_issue=A;,citation_volume=25;,citation_journal_title=Journal of applied probability;,citation_publisher=Cambridge Univ Press;">
<meta name="citation_reference" content="citation_title=Foundations and applications of sensor management;,citation_author=A. Mahajan;,citation_author=D. Teneketzis;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Two characterizations of optimality in dynamic programming;,citation_author=Ioannis Karatzas;,citation_author=William D. Sudderth;,citation_publication_date=2010-11;,citation_cover_date=2010-11;,citation_year=2010;,citation_issue=3;,citation_doi=10.1007/s00245-009-9093-x;,citation_volume=61;,citation_journal_title=Applied Mathematics and Optimization;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Structural properties of stochastic dynamic programs;,citation_author=James E. Smith;,citation_author=Kevin F. McCardle;,citation_publication_date=2002-10;,citation_cover_date=2002-10;,citation_year=2002;,citation_issue=5;,citation_doi=10.1287/opre.50.5.796.365;,citation_volume=50;,citation_journal_title=Operations Research;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Monotonicity in multidimensional markov decision processes for the batch dispatch problem;,citation_author=Katerina Papadaki;,citation_author=Warren B. Powell;,citation_publication_date=2007-03;,citation_cover_date=2007-03;,citation_year=2007;,citation_issue=2;,citation_doi=10.1016/j.orl.2006.03.013;,citation_volume=35;,citation_journal_title=Operations Research Letters;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Basic ideas for event-based optimization of markov systems;,citation_author=Xi-Ren Cao;,citation_publication_date=2005-06;,citation_cover_date=2005-06;,citation_year=2005;,citation_issue=2;,citation_doi=10.1007/s10626-004-6211-4;,citation_volume=15;,citation_journal_title=Discrete Event Dynamic Systems;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Monotonicity in markov reward and decision chains: Theory and applications;,citation_author=Ger Koole;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_issue=1;,citation_doi=10.1561/0900000002;,citation_volume=1;,citation_journal_title=Foundations and Trends in Stochastic Systems;,citation_publisher=Now Publishers;">
<meta name="citation_reference" content="citation_title=Stochastic learning and optimization;,citation_author=Xi-Ren Cao;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_doi=10.1007/978-0-387-69082-7;">
<meta name="citation_reference" content="citation_title=Optimality and approximation with policy gradient methods in markov decision processes;,citation_abstract=Policy gradient methods are among the most effective methods in challenging reinforcement learning problems with large state and/or action spaces. However, little is known about even their most basic theoretical convergence properties, including: if and how fast they converge to a globally optimal solution (say with a sufficiently rich policy class); how they cope with approximation error due to using a restricted class of parametric policies; or their finite sample behavior. Such characterizations are important not only to compare these methods to their approximate value function counterparts (where such issues are relatively well understood, at least in the worst case), but also to help with more principled approaches to algorithm design. This work provides provable characterizations of computational, approximation, and sample size issues with regards to policy gradient methods in the context of discounted Markov Decision Processes (MDPs). We focus on both: 1) &amp;amp;amp;quot;tabular&amp;quot; policy parameterizations, where the optimal policy is contained in the class and where we show global convergence to the optimal policy, and 2) restricted policy classes, which may not contain the optimal policy and where we provide agnostic learning results. One insight of this work is in formalizing the importance how a favorable initial state distribution provides a means to circumvent worst-case exploration issues. Overall, these results place policy gradient methods under a solid theoretical footing, analogous to the global convergence guarantees of iterative value function based algorithms.;,citation_author=Alekh Agarwal;,citation_author=Sham M. Kakade;,citation_author=Jason D. Lee;,citation_author=Gaurav Mahajan;,citation_publication_date=2019-08-01;,citation_cover_date=2019-08-01;,citation_year=2019;,citation_fulltext_html_url=https://arxiv.org/abs/1908.00261v2;">
<meta name="citation_reference" content="citation_title=Analysis of stochastic dual dynamic programming method;,citation_author=Alexander Shapiro;,citation_publication_date=2011-02;,citation_cover_date=2011-02;,citation_year=2011;,citation_issue=1;,citation_doi=10.1016/j.ejor.2010.08.007;,citation_volume=209;,citation_journal_title=European Journal of Operational Research;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Multi-stage stochastic optimization applied to energy planning;,citation_author=M. V. F. Pereira;,citation_author=L. M. V. G. Pinto;,citation_publication_date=1991-05;,citation_cover_date=1991-05;,citation_year=1991;,citation_issue=1-3;,citation_doi=10.1007/bf01582895;,citation_volume=52;,citation_journal_title=Mathematical Programming;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Approximate information state for partially observed systems;,citation_author=Jayakumar Subramanian;,citation_author=Aditya Mahajan;,citation_publication_date=2019-12;,citation_cover_date=2019-12;,citation_year=2019;,citation_doi=10.1109/cdc40024.2019.9029898;,citation_conference_title=2019 IEEE 58th conference on decision and control (CDC);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Approximate information state for approximate planning and reinforcement learning in partially observed systems;,citation_author=Jayakumar Subramanian;,citation_author=Amit Sinha;,citation_author=Raihan Seraj;,citation_author=Aditya Mahajan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=http://jmlr.org/papers/v23/20-1165.html;,citation_issue=12;,citation_volume=23;,citation_journal_title=Journal of Machine Learning Research;">
<meta name="citation_reference" content="citation_title=On policy independence of conditional expectation;,citation_author=Hans S. Witsenhausen;,citation_publication_date=1975;,citation_cover_date=1975;,citation_year=1975;,citation_volume=28;,citation_journal_title=Information and Control;">
<meta name="citation_reference" content="citation_title=Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes;,citation_author=Anthony Cassandra;,citation_author=Michael L. Littman;,citation_author=Nevin L. Zhang;,citation_publication_date=1997;,citation_cover_date=1997;,citation_year=1997;,citation_conference_title=Proceedings of the thirteenth conference on uncertainty in artificial intelligence;">
<meta name="citation_reference" content="citation_title=Partially observable Markov decision processes: A geometric technique and analysis;,citation_author=H. Zhang;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_journal_title=Operations Research;">
<meta name="citation_reference" content="citation_title=Algorithms for partially observable markov decision processes;,citation_author=Hsien-Te Cheng;,citation_publication_date=1988;,citation_cover_date=1988;,citation_year=1988;,citation_dissertation_institution=University of British Columbia;">
<meta name="citation_reference" content="citation_title=Acting optimally in partially observable stochastic domains;,citation_author=Anthony R Cassandra;,citation_author=Leslie Pack Kaelbling;,citation_author=Michael L Littman;,citation_publication_date=1994;,citation_cover_date=1994;,citation_year=1994;,citation_volume=94;,citation_conference_title=AAAI;">
<meta name="citation_reference" content="citation_title=Planning in stochastic domains: Problem characteristics and approximation;,citation_author=N Zhang;,citation_author=W Liu;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_technical_report_institution=Hong Kong Univeristy of Science; Technology;,citation_technical_report_number=HKUST-CS96-31;">
<meta name="citation_reference" content="citation_title=Sequential tests of statistical hypotheses;,citation_author=A. Wald;,citation_publication_date=1945-06;,citation_cover_date=1945-06;,citation_year=1945;,citation_issue=2;,citation_doi=10.1214/aoms/1177731118;,citation_volume=16;,citation_journal_title=The Annals of Mathematical Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Bayes and minimax solutions of sequential decision problems;,citation_author=K. J. Arrow;,citation_author=D. Blackwell;,citation_author=M. A. Girshick;,citation_publication_date=1949-07;,citation_cover_date=1949-07;,citation_year=1949;,citation_issue=3/4;,citation_doi=10.2307/1905525;,citation_volume=17;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Optimal statistical decisions;,citation_author=Morris DeGroot;,citation_publication_date=1970;,citation_cover_date=1970;,citation_year=1970;,citation_isbn=047168029X;">
<meta name="citation_reference" content="citation_title=On a test whether two samples are from the same population;,citation_author=A. Wald;,citation_author=J. Wolfowitz;,citation_publication_date=1940-06;,citation_cover_date=1940-06;,citation_year=1940;,citation_issue=2;,citation_doi=10.1214/aoms/1177731909;,citation_volume=11;,citation_journal_title=The Annals of Mathematical Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Optimum character of the sequential probability ratio test;,citation_author=A. Wald;,citation_author=J. Wolfowitz;,citation_publication_date=1948-09;,citation_cover_date=1948-09;,citation_year=1948;,citation_issue=3;,citation_doi=10.1214/aoms/1177730197;,citation_volume=19;,citation_journal_title=The Annals of Mathematical Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=A stochastic sensor selection scheme for sequential hypothesis testing with multiple sensors;,citation_author=Cheng-Zong Bai;,citation_author=Vaibhav Katewa;,citation_author=Vijay Gupta;,citation_author=Yih-Fang Huang;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_issue=14;,citation_volume=63;,citation_journal_title=IEEE transactions on signal processing;,citation_publisher=IEEE;">
<meta name="citation_reference" content="citation_title=A discrete markov chain representation of the sequential probability ratio test;,citation_author=Willam H. Woodall;,citation_author=Marion R. Reynolds;,citation_publication_date=1983-01;,citation_cover_date=1983-01;,citation_year=1983;,citation_issue=1;,citation_doi=10.1080/07474948308836025;,citation_volume=2;,citation_journal_title=Communications in Statistics. Part C: Sequential Analysis;,citation_publisher=Informa UK Limited;">
<meta name="citation_reference" content="citation_title=Mean-field games with a major player;,citation_author=Jean-Michel Lasry;,citation_author=Pierre-Louis Lions;,citation_publication_date=2018-08;,citation_cover_date=2018-08;,citation_year=2018;,citation_issue=8;,citation_doi=10.1016/j.crma.2018.06.001;,citation_volume=356;,citation_journal_title=Comptes Rendus Mathematique;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Remote estimation over a packet-drop channel with markovian state;,citation_author=Jhelum Chakravorty;,citation_author=Aditya Mahajan;,citation_publication_date=2020-05;,citation_cover_date=2020-05;,citation_year=2020;,citation_issue=5;,citation_doi=10.1109/tac.2019.2926160;,citation_volume=65;,citation_journal_title=IEEE Transactions on Automatic Control;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Optimal state estimation in the presence of communication costs and packet drops;,citation_author=Gabriel M. Lipsa;,citation_author=Nuno C. Martins;,citation_publication_date=2009-09;,citation_cover_date=2009-09;,citation_year=2009;,citation_doi=10.1109/allerton.2009.5394899;,citation_conference_title=Annual allerton conference on communication, control, and computing (allerton);,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Remote state estimation with communication costs for first-order LTI systems;,citation_author=G. M. Lipsa;,citation_author=N. C. Martins;,citation_publication_date=2011-09;,citation_cover_date=2011-09;,citation_year=2011;,citation_issue=9;,citation_doi=10.1109/tac.2011.2139370;,citation_volume=56;,citation_journal_title=IEEE Transactions on Automatic Control;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Policy improvement and the newton-raphson algorithm;,citation_author=P. Whittle;,citation_author=N. Komarova;,citation_publication_date=1988-04;,citation_cover_date=1988-04;,citation_year=1988;,citation_issue=2;,citation_doi=10.1017/s0269964800000760;,citation_volume=2;,citation_journal_title=Probability in the Engineering and Informational Sciences;,citation_publisher=Cambridge University Press (CUP);">
<meta name="citation_reference" content="citation_title=Optimal adaptive control of linear-quadratic-gaussian systems;,citation_author=P. R. Kumar;,citation_publication_date=1983-03;,citation_cover_date=1983-03;,citation_year=1983;,citation_issue=2;,citation_doi=10.1137/0321009;,citation_volume=21;,citation_journal_title=SIAM Journal on Control and Optimization;,citation_publisher=Society for Industrial &amp;amp;amp; Applied Mathematics (SIAM);">
<meta name="citation_reference" content="citation_title=The optimal inventory policy for batch ordering;,citation_author=Arthur F. Veinott;,citation_publication_date=1965-06;,citation_cover_date=1965-06;,citation_year=1965;,citation_issue=3;,citation_doi=10.1287/opre.13.3.424;,citation_volume=13;,citation_journal_title=Operations Research;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Elements of game theory;,citation_author=Ye S. Venttsel;,citation_publication_date=1980;,citation_cover_date=1980;,citation_year=1980;,citation_fulltext_html_url=https://archive.org/details/ElementsOfGameTheorylittleMathematicsLibrary/;">
<meta name="citation_reference" content="citation_title=Periodic review inventory systems with continuous demand and discrete order sizes;,citation_author=John N. Tsitsiklis;,citation_publication_date=1984-10;,citation_cover_date=1984-10;,citation_year=1984;,citation_issue=10;,citation_doi=10.1287/mnsc.30.10.1250;,citation_volume=30;,citation_journal_title=Management Science;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Risk sensitivity, A strangely pervasive concept;,citation_author=Peter Whittle;,citation_publication_date=2002-02;,citation_cover_date=2002-02;,citation_year=2002;,citation_issue=1;,citation_doi=10.1017/s1365100502027025;,citation_volume=6;,citation_journal_title=Macroeconomic Dynamics;,citation_publisher=Cambridge University Press (CUP);">
<meta name="citation_reference" content="citation_title=Risk-sensitive markov decision processes;,citation_author=Ronald A. Howard;,citation_author=James E. Matheson;,citation_publication_date=1972-03;,citation_cover_date=1972-03;,citation_year=1972;,citation_issue=7;,citation_doi=10.1287/mnsc.18.7.356;,citation_volume=18;,citation_journal_title=Management Science;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Inequalities: Theory of majorization and its applications;,citation_author=Albert W. Marshall;,citation_author=Ingram Olkin;,citation_author=Barry C. Arnold;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_doi=10.1007/978-0-387-68276-1;">
<meta name="citation_reference" content="citation_title=Risk sensitive control of markov processes in countable state space;,citation_author=Daniel Hernandez-Hernández;,citation_author=Steven I. Marcus;,citation_publication_date=1996-11;,citation_cover_date=1996-11;,citation_year=1996;,citation_issue=3;,citation_doi=10.1016/s0167-6911(96)00051-5;,citation_volume=29;,citation_journal_title=Systems &amp;amp;amp; Control Letters;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Existence of risk-sensitive optimal stationary policies for controlled markov processes;,citation_author=D. Hernández-Hernández;,citation_publication_date=1999-11;,citation_cover_date=1999-11;,citation_year=1999;,citation_issue=3;,citation_doi=10.1007/s002459900126;,citation_volume=40;,citation_journal_title=Applied Mathematics and Optimization;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Convex risk measures;,citation_author=Hans Föllmer;,citation_author=Alexander Schied;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470061602.eqf15003;,citation_doi=10.1002/9780470061602.eqf15003;,citation_isbn=9780470061602;,citation_inbook_title=Encyclopedia of quantitative finance;">
<meta name="citation_reference" content="citation_title=Updating the inverse of a matrix;,citation_author=William W. Hager;,citation_publication_date=1989-06;,citation_cover_date=1989-06;,citation_year=1989;,citation_issue=2;,citation_doi=10.1137/1031049;,citation_volume=31;,citation_journal_title=SIAM Review;,citation_publisher=Society for Industrial &amp;amp;amp; Applied Mathematics (SIAM);">
<meta name="citation_reference" content="citation_title=A stochastic approximation method;,citation_author=Herbert Robbins;,citation_author=Sutton Monro;,citation_publication_date=1951-09;,citation_cover_date=1951-09;,citation_year=1951;,citation_issue=3;,citation_doi=10.1214/aoms/1177729586;,citation_volume=22;,citation_journal_title=The Annals of Mathematical Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=The o.d.e. Method for convergence of stochastic approximation and reinforcement learning;,citation_author=V. S. Borkar;,citation_author=S. P. Meyn;,citation_publication_date=2000-01;,citation_cover_date=2000-01;,citation_year=2000;,citation_issue=2;,citation_doi=10.1137/s0363012997331639;,citation_volume=38;,citation_journal_title=SIAM Journal on Control and Optimization;,citation_publisher=Society for Industrial &amp;amp;amp; Applied Mathematics (SIAM);">
<meta name="citation_reference" content="citation_title=Q-learning;,citation_author=Christopher J. C. H. Watkins;,citation_author=Peter Dayan;,citation_publication_date=1992-05;,citation_cover_date=1992-05;,citation_year=1992;,citation_issue=3-4;,citation_doi=10.1007/bf00992698;,citation_volume=8;,citation_journal_title=Machine Learning;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Asynchronous stochastic approximation and q-learning;,citation_author=John N. Tsitsiklis;,citation_publication_date=1994-09;,citation_cover_date=1994-09;,citation_year=1994;,citation_issue=3;,citation_doi=10.1007/bf00993306;,citation_volume=16;,citation_journal_title=Machine Learning;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=An analog scheme for fixed point computation. I. theory;,citation_author=V. S. Borkar;,citation_author=K. Soumyanatha;,citation_publication_date=1997-04;,citation_cover_date=1997-04;,citation_year=1997;,citation_issue=4;,citation_doi=10.1109/81.563625;,citation_volume=44;,citation_journal_title=IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=On the convergence of stochastic iterative dynamic programming algorithms;,citation_author=Tommi Jaakkola;,citation_author=Michael I. Jordan;,citation_author=Satinder P. Singh;,citation_publication_date=1994-11;,citation_cover_date=1994-11;,citation_year=1994;,citation_issue=6;,citation_doi=10.1162/neco.1994.6.6.1185;,citation_volume=6;,citation_journal_title=Neural Computation;,citation_publisher=MIT Press - Journals;">
<meta name="citation_reference" content="citation_title=Dynamic programming and markov processes;,citation_author=Ronald A. Howard;,citation_publication_date=1960;,citation_cover_date=1960;,citation_year=1960;">
<meta name="citation_reference" content="citation_title=Concentration inequalities and martingale inequalities: A survey;,citation_author=Fan Chung;,citation_author=Linyuan Lu;,citation_publication_date=2006;,citation_cover_date=2006;,citation_year=2006;,citation_fulltext_html_url=https://projecteuclid.org:443/euclid.im/1175266369;,citation_issue=1;,citation_volume=3;,citation_journal_title=Internet Math.;,citation_publisher=A K Peters, Ltd.;">
<meta name="citation_reference" content="citation_title=On the fenchel duality between strong convexity and lipschitz continuous gradient;,citation_abstract=We provide a simple proof for the Fenchel duality between strong convexity and Lipschitz continuous gradient. To this end, we first establish equivalent conditions of convexity for a general function that may not be differentiable. By utilizing these equivalent conditions, we can directly obtain equivalent conditions for strong convexity and Lipschitz continuous gradient. Based on these results, we can easily prove Fenchel duality. Beside this main result, we also identify several conditions that are implied by strong convexity or Lipschitz continuous gradient, but are not necessarily equivalent to them. This means that these conditions are more general than strong convexity or Lipschitz continuous gradient themselves.;,citation_author=Xingyu Zhou;,citation_publication_date=2018-03-17;,citation_cover_date=2018-03-17;,citation_year=2018;,citation_fulltext_html_url=https://arxiv.org/abs/1803.06573v1;">
<meta name="citation_reference" content="citation_title=Optimal control of markov processes with incomplete state information;,citation_author=K. J Åström;,citation_publication_date=1965-02;,citation_cover_date=1965-02;,citation_year=1965;,citation_issue=1;,citation_doi=10.1016/0022-247x(65)90154-x;,citation_volume=10;,citation_journal_title=Journal of Mathematical Analysis and Applications;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Dynamic service migration in mobile edge computing based on Markov decision process;,citation_author=Shiqiang Wang;,citation_author=Rahul Urgaonkar;,citation_author=Murtaza Zafer;,citation_author=Ting He;,citation_author=Kevin Chan;,citation_author=Kin K. Leung;,citation_publication_date=2019-06;,citation_cover_date=2019-06;,citation_year=2019;,citation_issue=3;,citation_doi=10.1109/tnet.2019.2916577;,citation_volume=27;,citation_journal_title=IEEE/ACM Transactions on Networking;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Dynamic service migration and workload scheduling in edge-clouds;,citation_author=Rahul Urgaonkar;,citation_author=Shiqiang Wang;,citation_author=Ting He;,citation_author=Murtaza Zafer;,citation_author=Kevin Chan;,citation_author=Kin K. Leung;,citation_publication_date=2015-09;,citation_cover_date=2015-09;,citation_year=2015;,citation_doi=10.1016/j.peva.2015.06.013;,citation_volume=91;,citation_journal_title=Performance Evaluation;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=The role and use of the stochastic linear-quadratic-gaussian problem in control system design;,citation_author=M. Athans;,citation_publication_date=1971-12;,citation_cover_date=1971-12;,citation_year=1971;,citation_issue=6;,citation_doi=10.1109/tac.1971.1099818;,citation_volume=16;,citation_journal_title=IEEE Transactions on Automatic Control;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Complexity bounds for approximately solving discounted MDPs by value iterations;,citation_author=Eugene A. Feinberg;,citation_author=Gaojin He;,citation_publication_date=2020-07;,citation_cover_date=2020-07;,citation_year=2020;,citation_doi=10.1016/j.orl.2020.07.001;,citation_journal_title=Operations Research Letters;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=How does the value function of a markov decision process depend on the transition probabilities?;,citation_author=Alfred Müller;,citation_publication_date=1997-11;,citation_cover_date=1997-11;,citation_year=1997;,citation_issue=4;,citation_doi=10.1287/moor.22.4.872;,citation_volume=22;,citation_journal_title=Mathematics of Operations Research;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=What is RKHS?;,citation_author=Dino Sejdinovic;,citation_author=Arthur Gretton;,citation_fulltext_html_url=http://www.stats.ox.ac.uk/~sejdinov/teaching/atml14/Theory_2014.pdf;">
<meta name="citation_reference" content="citation_title=Martingale methods in stochastic control;,citation_author=M. H. A. Davis;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_doi=10.1007/bfb0009377;,citation_inbook_title=Stochastic control theory and stochastic differential systems;">
<meta name="citation_reference" content="citation_title=Concentration inequalities and model selection;,citation_author=Jean Picard;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_doi=10.1007/978-3-540-48503-2;">
<meta name="citation_reference" content="citation_title=High-dimensional statistics;,citation_author=Phillippe Rigollet;,citation_publication_date=2015-07;,citation_cover_date=2015-07;,citation_year=2015;,citation_fulltext_html_url=https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/;">
<meta name="citation_reference" content="citation_title=Subgaussian random variables: An expository note;,citation_author=Omar Rivasplata;,citation_publication_date=2012-11;,citation_cover_date=2012-11;,citation_year=2012;,citation_fulltext_html_url=http://stat.cmu.edu/~arinaldo/36788/subgaussians.pdf;">
<meta name="citation_reference" content="citation_title=High-dimensional statistics;,citation_author=Martin J. Wainwright;,citation_publication_date=2019-02;,citation_cover_date=2019-02;,citation_year=2019;,citation_doi=10.1017/9781108627771;">
<meta name="citation_reference" content="citation_title=Selecting computations: Theory and applications;,citation_author=Nicholas Hay;,citation_author=S. Russell;,citation_author=David Tolpin;,citation_author=S. E. Shimony;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_fulltext_html_url=http://www.auai.org/uai2012/papers/123.pdf;,citation_conference_title=UAI;">
<meta name="citation_reference" content="citation_title=On linear control theory;,citation_author=D. Peter Joseph;,citation_author=T. Julius Tou;,citation_publication_date=1961;,citation_cover_date=1961;,citation_year=1961;,citation_issue=4;,citation_doi=10.1109/tai.1961.6371743;,citation_volume=80;,citation_journal_title=Transactions of the American Institute of Electrical Engineers, Part II: Applications and Industry;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=On the separation theorem of stochastic control;,citation_author=W. M. Wonham;,citation_publication_date=1968-05;,citation_cover_date=1968-05;,citation_year=1968;,citation_issue=2;,citation_doi=10.1137/0306023;,citation_volume=6;,citation_journal_title=SIAM Journal on Control;,citation_publisher=Society for Industrial &amp;amp;amp; Applied Mathematics (SIAM);">
<meta name="citation_reference" content="citation_title=Some comments on a theorem of Hardy and Littlewood;,citation_author=R. Sznajder;,citation_author=J. A. Filar;,citation_publication_date=1992-10;,citation_cover_date=1992-10;,citation_year=1992;,citation_issue=1;,citation_doi=10.1007/bf00939913;,citation_volume=75;,citation_journal_title=Journal of Optimization Theory and Applications;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Notes on the theory of series (XVI): Two Tauberian theorems;,citation_author=G. H. Hardy;,citation_author=J. E. Littlewood;,citation_publication_date=1931-10;,citation_cover_date=1931-10;,citation_year=1931;,citation_issue=4;,citation_doi=10.1112/jlms/s1-6.4.281;,citation_volume=s1-6;,citation_journal_title=Journal of the London Mathematical Society;,citation_publisher=Wiley;">
<meta name="citation_reference" content="citation_title=The Hardy-Littlewood theorems;,citation_author=Jacob Korevaar;,citation_publication_date=2004;,citation_cover_date=2004;,citation_year=2004;,citation_doi=10.1007/978-3-662-10225-1_1;,citation_inbook_title=Tauberian theory: A century of developments;">
<meta name="citation_reference" content="citation_title=Monotone optimal policies for markov decision processes;,citation_author=Richard F. Serfozo;,citation_publication_date=1976;,citation_cover_date=1976;,citation_year=1976;,citation_doi=10.1007/bfb0120752;,citation_inbook_title=Mathematical programming studies;">
<meta name="citation_reference" content="citation_title=Cross-layer communication over fading channels with adaptive decision feedback;,citation_author=Borna Sayedana;,citation_author=Aditya Mahajan;,citation_author=Edmund Yeh;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International symposium on modeling and optimization in mobile, ad hoc, and wireless networks (WiOPT);">
<meta name="citation_reference" content="citation_title=Counterexamples on the monotonicity of delay optimal strategies for energy harvesting transmitters;,citation_author=Borna Sayedana;,citation_author=Aditya Mahajan;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_doi=10.1109/lwc.2020.2981066;,citation_journal_title=IEEE Wireless Communications Letters;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=Q-learning for MDPs with general spaces: Convergence and near optimality via quantization under weak continuity;,citation_abstract=Reinforcement learning algorithms often require finiteness of state and action spaces in Markov decision processes (MDPs) and various efforts have been made in the literature towards the applicability of such algorithms for continuous state and action spaces. In this paper, we show that under very mild regularity conditions (in particular, involving only weak continuity of the transition kernel of an MDP), Q-learning for standard Borel MDPs via quantization of states and actions converge to a limit, and furthermore this limit satisfies an optimality equation which leads to near optimality with either explicit performance bounds or which are guaranteed to be asymptotically optimal. Our approach builds on (i) viewing quantization as a measurement kernel and thus a quantized MDP as a POMDP, (ii) utilizing near optimality and convergence results of Q-learning for POMDPs, and (iii) finally, near-optimality of finite state model approximations for MDPs with weakly continuous kernels which we show to correspond to the fixed point of the constructed POMDP. Thus, our paper presents a very general convergence and approximation result for the applicability of Q-learning for continuous MDPs.;,citation_author=Ali Devran Kara;,citation_author=Naci Saldi;,citation_author=Serdar Yüksel;,citation_publication_date=2021-11-12;,citation_cover_date=2021-11-12;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2111.06781v1;">
<meta name="citation_reference" content="citation_title=Bounds and transformations for discounted finite markov decision chains;,citation_author=Evan L. Porteus;,citation_publication_date=1975-08;,citation_cover_date=1975-08;,citation_year=1975;,citation_issue=4;,citation_doi=10.1287/opre.23.4.761;,citation_volume=23;,citation_journal_title=Operations Research;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Calculus on MDPs: Potential shaping as a gradient;,citation_abstract=In reinforcement learning, different reward functions can be equivalent in terms of the optimal policies they induce. A particularly well-known and important example is potential shaping, a class of functions that can be added to any reward function without changing the optimal policy set under arbitrary transition dynamics. Potential shaping is conceptually similar to potentials, conservative vector fields and gauge transformations in math and physics, but this connection has not previously been formally explored. We develop a formalism for discrete calculus on graphs that abstract a Markov Decision Process, and show how potential shaping can be formally interpreted as a gradient within this framework. This allows us to strengthen results from Ng et al. (1999) describing conditions under which potential shaping is the only additive reward transformation to always preserve optimal policies. As an additional application of our formalism, we define a rule for picking a single unique reward function from each potential shaping equivalence class.;,citation_author=Erik Jenner;,citation_author=Herke Hoof;,citation_author=Adam Gleave;,citation_publication_date=2022-08-20;,citation_cover_date=2022-08-20;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2208.09570v1;">
<meta name="citation_reference" content="citation_title=Dynamic potential-based reward shaping;,citation_abstract=Potential-based reward shaping can significantly improve the time needed to learn an optimal policy and, in multi-agent systems, the performance of the final joint-policy. It has been proven to not alter the optimal policy of an agent learning alone or the Nash equilibria of multiple agents learning together.However, a limitation of existing proofs is the assumption that the potential of a state does not change dynamically during the learning. This assumption often is broken, especially if the reward-shaping function is generated automatically.In this paper we prove and demonstrate a method of extending potential-based reward shaping to allow dynamic shaping and maintain the guarantees of policy invariance in the single-agent case and consistent Nash equilibria in the multi-agent case.;,citation_author=Sam Devlin;,citation_author=Daniel Kudenko;,citation_publication_date=2012;,citation_cover_date=2012;,citation_year=2012;,citation_isbn=0981738117;,citation_conference_title=Proceedings of the 11th international conference on autonomous agents and multiagent systems;,citation_conference=International Foundation for Autonomous Agents; Multiagent Systems;,citation_series_title=AAMAS ’12;">
<meta name="citation_reference" content="citation_title=Potential-based shaping and q-value initialization are equivalent;,citation_abstract=Shaping has proven to be a powerful but precarious means of improving reinforcement learning performance. Ng, Harada, and Russell (1999) proposed the potential-based shaping algorithm for adding shaping rewards in a way that guarantees the learner will learn optimal behavior.In this note, we prove certain similarities between this shaping algorithm and the initialization step required for several reinforcement learning algorithms. More specifically, we prove that a reinforcement learner with initial Q-values based on the shaping algorithm’s potential function make the same updates throughout learning as a learner receiving potential-based shaping rewards. We further prove that under a broad category of policies, the behavior of these two learners are indistinguishable. The comparison provides intuition on the theoretical properties of the shaping algorithm as well as a suggestion for a simpler method for capturing the algorithm’s benefit. In addition, the equivalence raises previously unaddressed issues concerning the efficiency of learning with potential-based shaping.;,citation_author=Eric Wiewiora;,citation_publication_date=2003-09;,citation_cover_date=2003-09;,citation_year=2003;,citation_issue=1;,citation_issn=1076-9757;,citation_volume=19;,citation_journal_title=Journal of Artificial Intelligence Research;,citation_publisher=AI Access Foundation;">
<meta name="citation_reference" content="citation_title=Behavior of organisms;,citation_author=B. F. Skinner;,citation_publication_date=1938;,citation_cover_date=1938;,citation_year=1938;,citation_isbn=9781583900079;">
<meta name="citation_reference" content="citation_title=John von Neumann’s conception of the minimax theorem: A journey through different mathematical contexts;,citation_author=Tinne Hoff Kjeldsen;,citation_publication_date=2001;,citation_cover_date=2001;,citation_year=2001;,citation_fulltext_html_url=http://www.jstor.org/stable/41134130;,citation_issue=1;,citation_issn=00039519, 14320657;,citation_volume=56;,citation_journal_title=Archive for History of Exact Sciences;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Jamming in multiple independent Gaussian channels as a game;,citation_author=Michail Fasoulakis;,citation_author=Apostolos Traganitis;,citation_author=Anthony Ephremides;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_doi=10.1007/978-3-030-16989-3_1;,citation_inbook_title=Lecture notes of the institute for computer sciences, social informatics and telecommunications engineering;">
<meta name="citation_reference" content="citation_title=A jamming game in wireless networks with transmission cost;,citation_author=Eitan Altman;,citation_author=Konstantin Avrachenkov;,citation_author=Andrey Garnaev;,citation_publication_date=2007-06;,citation_cover_date=2007-06;,citation_year=2007;,citation_fulltext_html_url=https://www-sop.inria.fr/members/Eitan.Altman/PAPERS/andrey-lncs.pdf;,citation_conference_title=EuroFGI international conference on network control and optimization (NET-COOP);,citation_conference=Springer;">
<meta name="citation_reference" content="citation_title=Closed form solutions for symmetric water filling games;,citation_author=Eitan Altman;,citation_author=Konstantin Avrachenkov;,citation_author=Andrey Garnaev;,citation_publication_date=2008;,citation_cover_date=2008;,citation_year=2008;,citation_doi=https://doi.org/10.1109/INFOCOM.2008.117;,citation_conference_title=IEEE INFOCOM conference on computer communications;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Closed form solutions for water-filling problems in optimization and game frameworks;,citation_author=Eitan Altman;,citation_author=Konstantin Avrachenkov;,citation_author=Andrey Garnaev;,citation_publication_date=2011;,citation_cover_date=2011;,citation_year=2011;,citation_volume=47;,citation_journal_title=Telecommunication Systems;,citation_publisher=Springer;">
<meta name="citation_reference" content="citation_title=Jamming in wireless networks: The case of several jammers;,citation_author=Eitan Altman;,citation_author=Konstantin Avrachenkov;,citation_author=Andrey Garnaev;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=2009 international conference on game theory for networks;,citation_conference=IEEE;">
<meta name="citation_reference" content="citation_title=Jamming game with incomplete information about the jammer;,citation_author=Eitan Altman;,citation_author=Konstantin Avrachenkov;,citation_author=Andrey Garnaev;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_conference_title=Proceedings of the fourth international ICST conference on performance evaluation methodologies and tools;">
<meta name="citation_reference" content="citation_title=Equilibrium points in n -person games;,citation_author=John F. Nash;,citation_publication_date=1950-01;,citation_cover_date=1950-01;,citation_year=1950;,citation_issue=1;,citation_doi=10.1073/pnas.36.1.48;,citation_volume=36;,citation_journal_title=Proceedings of the National Academy of Sciences;,citation_publisher=Proceedings of the National Academy of Sciences;">
<meta name="citation_reference" content="citation_title=A further generalization of the Kakutani fixed point theorem, with application to nash equilibrium points;,citation_author=I. L. Glicksberg;,citation_publication_date=1952-02;,citation_cover_date=1952-02;,citation_year=1952;,citation_issue=1;,citation_doi=10.2307/2032478;,citation_volume=3;,citation_journal_title=Proceedings of the American Mathematical Society;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=The logic of animal conflict;,citation_author=J Maynard Smith;,citation_author=G. R. Price;,citation_publication_date=1973-11;,citation_cover_date=1973-11;,citation_year=1973;,citation_issue=5427;,citation_doi=10.1038/246015a0;,citation_volume=246;,citation_journal_title=Nature;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Evolution and the theory of games;,citation_author=John Maynard Smith;,citation_publication_date=1982;,citation_cover_date=1982;,citation_year=1982;,citation_isbn=0521246733;">
<meta name="citation_reference" content="citation_title=The selfish gene;,citation_author=Richard Dawkins;,citation_publication_date=1976;,citation_cover_date=1976;,citation_year=1976;,citation_isbn=019857519X;">
<meta name="citation_reference" content="citation_title=Population games and evolutionary dynamics;,citation_author=William H. Sandholm;,citation_publication_date=2010;,citation_cover_date=2010;,citation_year=2010;,citation_isbn=9780262195874;">
<meta name="citation_reference" content="citation_title=A note on evolutionary stable strategies and game dynamics;,citation_author=Josef Hofbauer;,citation_author=Peter Schuster;,citation_author=Karl Sigmund;,citation_publication_date=1979;,citation_cover_date=1979;,citation_year=1979;,citation_issue=3;,citation_volume=81;,citation_journal_title=Journal of Theoretical Biology;">
<meta name="citation_reference" content="citation_title=Evolutionary stable strategies and game dynamics;,citation_author=Peter D. Taylor;,citation_author=Leo B. Jonker;,citation_publication_date=1978-07;,citation_cover_date=1978-07;,citation_year=1978;,citation_issue=1-2;,citation_doi=10.1016/0025-5564(78)90077-9;,citation_volume=40;,citation_journal_title=Mathematical Biosciences;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Correlated equilibrium in a nutshell;,citation_author=Rabah Amir;,citation_author=Sergei Belkov;,citation_author=Igor V. Evstigneev;,citation_publication_date=2017-06;,citation_cover_date=2017-06;,citation_year=2017;,citation_issue=4;,citation_doi=10.1007/s11238-017-9609-9;,citation_volume=83;,citation_journal_title=Theory and Decision;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Correlated equilibrium as an expression of bayesian rationality;,citation_author=Robert J. Aumann;,citation_publication_date=1987-01;,citation_cover_date=1987-01;,citation_year=1987;,citation_issue=1;,citation_doi=10.2307/1911154;,citation_volume=55;,citation_journal_title=Econometrica;,citation_publisher=JSTOR;">
<meta name="citation_reference" content="citation_title=Subjectivity and correlation in randomized strategies;,citation_author=Robert J. Aumann;,citation_publication_date=1974-03;,citation_cover_date=1974-03;,citation_year=1974;,citation_issue=1;,citation_doi=10.1016/0304-4068(74)90037-8;,citation_volume=1;,citation_journal_title=Journal of Mathematical Economics;,citation_publisher=Elsevier BV;">
<meta name="citation_reference" content="citation_title=Agreeing to disagree;,citation_author=Robert J. Aumann;,citation_publication_date=1976-11;,citation_cover_date=1976-11;,citation_year=1976;,citation_issue=6;,citation_doi=10.1214/aos/1176343654;,citation_volume=4;,citation_journal_title=The Annals of Statistics;,citation_publisher=Institute of Mathematical Statistics;">
<meta name="citation_reference" content="citation_title=Computing correlated equilibria in multi-player games;,citation_author=Christos H. Papadimitriou;,citation_author=Tim Roughgarden;,citation_publication_date=2008-07;,citation_cover_date=2008-07;,citation_year=2008;,citation_issue=3;,citation_doi=10.1145/1379759.1379762;,citation_volume=55;,citation_journal_title=Journal of the ACM;,citation_publisher=Association for Computing Machinery (ACM);">
<meta name="citation_reference" content="citation_title=Games with incomplete information played by &amp;amp;amp;quot;bayesian&amp;quot; players, iIII part i. The basic model;,citation_author=John C. Harsanyi;,citation_publication_date=1967-11;,citation_cover_date=1967-11;,citation_year=1967;,citation_issue=3;,citation_doi=10.1287/mnsc.14.3.159;,citation_volume=14;,citation_journal_title=Management Science;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Games with incomplete information played by &amp;amp;amp;quot;bayesian&amp;quot; players part II. Bayesian equilibrium points;,citation_author=John C. Harsanyi;,citation_publication_date=1968-01;,citation_cover_date=1968-01;,citation_year=1968;,citation_issue=5;,citation_doi=10.1287/mnsc.14.5.320;,citation_volume=14;,citation_journal_title=Management Science;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Games with incomplete information played by “bayesian” players, part III. The basic probability distribution of the game;,citation_author=John C. Harsanyi;,citation_publication_date=1968-03;,citation_cover_date=1968-03;,citation_year=1968;,citation_issue=7;,citation_doi=10.1287/mnsc.14.7.486;,citation_volume=14;,citation_journal_title=Management Science;,citation_publisher=Institute for Operations Research; the Management Sciences (INFORMS);">
<meta name="citation_reference" content="citation_title=Game theory for applied economists;,citation_author=Robert Gibbons;,citation_publication_date=1992;,citation_cover_date=1992;,citation_year=1992;,citation_isbn=0691043086;">
<meta name="citation_reference" content="citation_title=Computer science theory for the information age;,citation_author=John Hopcroft;,citation_author=Ravi Kannan;,citation_publication_date=2012-01;,citation_cover_date=2012-01;,citation_year=2012;,citation_fulltext_html_url=https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/hopcroft-kannan-feb2012.pdf;">
<meta name="citation_reference" content="citation_title=Theory of self-adaptive control systems;,citation_author=H. Kwakernaak;,citation_publication_date=1965;,citation_cover_date=1965;,citation_year=1965;,citation_inbook_title=undefined;">
<meta name="citation_reference" content="citation_title=Information pattern for linear discrete-time models with stochastic coefficients;,citation_author=T. Bohlin;,citation_publication_date=1970-02;,citation_cover_date=1970-02;,citation_year=1970;,citation_issue=1;,citation_volume=15;,citation_journal_title=IEEE Transactions on Automatic Control (TAC);">
<meta name="citation_reference" content="citation_title=Information states for linear stochastic systems;,citation_author=M. H. A Davis;,citation_author=P. P Varaiya;,citation_publication_date=1972-02;,citation_cover_date=1972-02;,citation_year=1972;,citation_issue=2;,citation_volume=37;,citation_journal_title=Journal of Mathematical Analysis and Applications;">
<meta name="citation_reference" content="citation_title=Sufficient statistics in the optimal control of stochastic systems;,citation_author=Charlotte Striebel;,citation_publication_date=1965;,citation_cover_date=1965;,citation_year=1965;,citation_volume=12;,citation_journal_title=Journal of Mathematical Analysis and Applications;">
<meta name="citation_reference" content="citation_title=Some remarks on the concept of state;,citation_author=Hans S. Witsenhausen;,citation_editor=Y. C. Ho;,citation_editor=S. K. Mitter;,citation_publication_date=1976;,citation_cover_date=1976;,citation_year=1976;,citation_inbook_title=Directions in large-scale systems;">
<meta name="citation_reference" content="citation_title=Linear automaton transformations;,citation_author=A. Nerode;,citation_publication_date=1958;,citation_cover_date=1958;,citation_year=1958;,citation_volume=9;,citation_journal_title=Proceedings of American Mathematical Society;">
<meta name="citation_reference" content="citation_title=A convergence theorem for non-negative almost supermartingales and some applications;,citation_author=H. Robbins;,citation_author=D. Siegmund;,citation_publication_date=1971;,citation_cover_date=1971;,citation_year=1971;,citation_doi=10.1016/b978-0-12-604550-5.50015-8;,citation_inbook_title=Optimizing methods in statistics;">
<meta name="citation_reference" content="citation_title=Discrete parameter martingales;,citation_author=J. Neveu;,citation_publication_date=1975;,citation_cover_date=1975;,citation_year=1975;">
<meta name="citation_reference" content="citation_title=A user’s guide to measure theoretic probability;,citation_author=David Pollard;,citation_publication_date=2002;,citation_cover_date=2002;,citation_year=2002;">
<meta name="citation_reference" content="citation_title=Convergence of stochastic approximation via martingale and converse Lyapunov methods;,citation_author=M. Vidyasagar;,citation_publication_date=2023-01;,citation_cover_date=2023-01;,citation_year=2023;,citation_issue=2;,citation_doi=10.1007/s00498-023-00342-9;,citation_volume=35;,citation_journal_title=Mathematics of Control, Signals, and Systems;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=On stochastic approximation;,citation_author=E. G. Gladyshev;,citation_publication_date=1965-01;,citation_cover_date=1965-01;,citation_year=1965;,citation_issue=2;,citation_doi=10.1137/1110031;,citation_volume=10;,citation_journal_title=Theory of Probability and Its Applications;,citation_publisher=Society for Industrial &amp;amp;amp; Applied Mathematics (SIAM);">
<meta name="citation_reference" content="citation_title=Gradient convergence in gradient methods with errors;,citation_author=Dimitri P. Bertsekas;,citation_author=John N. Tsitsiklis;,citation_publication_date=2000-01;,citation_cover_date=2000-01;,citation_year=2000;,citation_issue=3;,citation_doi=10.1137/s1052623497331063;,citation_volume=10;,citation_journal_title=SIAM Journal on Optimization;,citation_publisher=Society for Industrial &amp;amp;amp; Applied Mathematics (SIAM);">
<meta name="citation_reference" content="citation_title=Stochastic processes;,citation_author=Joseph T. Chang;,citation_publication_date=2007-02;,citation_cover_date=2007-02;,citation_year=2007;,citation_fulltext_html_url=http://www.stat.yale.edu/~pollard/Courses/251.spring2013/Handouts/Chang-notes.pdf;">
<meta name="citation_reference" content="citation_title=Lyapunov criterion for stochastic systems and its applications in distributed computation;,citation_author=Yuzhen Qin;,citation_author=Ming Cao;,citation_author=Brian D. O. Anderson;,citation_publication_date=2020-02;,citation_cover_date=2020-02;,citation_year=2020;,citation_issue=2;,citation_doi=10.1109/tac.2019.2910948;,citation_volume=65;,citation_journal_title=IEEE Transactions on Automatic Control;,citation_publisher=Institute of Electrical; Electronics Engineers (IEEE);">
<meta name="citation_reference" content="citation_title=An upper bound on the loss from approximate optimal-value functions;,citation_author=Satinder P. Singh;,citation_author=Richard C. Yee;,citation_publication_date=1994-09;,citation_cover_date=1994-09;,citation_year=1994;,citation_issue=3;,citation_doi=10.1007/bf00993308;,citation_volume=16;,citation_journal_title=Machine Learning;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=Feature-based methods for large scale dynamic programming;,citation_author=John N. Tsitsiklis;,citation_author=Benjamin Roy;,citation_publication_date=1996-03;,citation_cover_date=1996-03;,citation_year=1996;,citation_issue=1-3;,citation_doi=10.1007/bf00114724;,citation_volume=22;,citation_journal_title=Machine Learning;,citation_publisher=Springer Science; Business Media LLC;">
<meta name="citation_reference" content="citation_title=A graphical derivation of the legendre transform;,citation_author=Sam Kennerly;,citation_publication_date=2011-04;,citation_cover_date=2011-04;,citation_year=2011;,citation_fulltext_html_url=http://einstein.drexel.edu/~skennerly/maths/Legendre.pdf;">
<meta name="citation_reference" content="citation_title=Variational analysis;,citation_author=R Tyrrell Rockafellar;,citation_author=Roger J-B Wets;,citation_publication_date=2009;,citation_cover_date=2009;,citation_year=2009;,citation_volume=317;">
<meta name="citation_reference" content="citation_title=Entropy, large deviations, and statistical mechanics;,citation_author=Richard S. Ellis;,citation_publication_date=1985;,citation_cover_date=1985;,citation_year=1985;,citation_doi=10.1007/978-1-4613-8533-2;">
<meta name="citation_reference" content="citation_title=A theory of regularized Markov decision processes;,citation_abstract=Many recent successful (deep) reinforcement learning algorithms make use of regularization, generally based on entropy or Kullback-Leibler divergence. We propose a general theory of regularized Markov Decision Processes that generalizes these approaches in two directions: we consider a larger class of regularizers, and we consider the general modified policy iteration approach, encompassing both policy iteration and value iteration. The core building blocks of this theory are a notion of regularized Bellman operator and the Legendre-Fenchel transform, a classical tool of convex optimization. This approach allows for error propagation analyses of general algorithmic schemes of which (possibly variants of) classical algorithms such as Trust Region Policy Optimization, Soft Q-learning, Stochastic Actor Critic or Dynamic Policy Programming are special cases. This also draws connections to proximal convex optimization, especially to Mirror Descent.;,citation_author=Matthieu Geist;,citation_author=Bruno Scherrer;,citation_author=Olivier Pietquin;,citation_editor=Kamalika Chaudhuri;,citation_editor=Ruslan Salakhutdinov;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://proceedings.mlr.press/v97/geist19a.html;,citation_volume=97;,citation_conference_title=Proceedings of the 36th international conference on machine learning;,citation_conference=PMLR;,citation_series_title=Proceedings of machine learning research;">
</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../stochastic-control/index.html" rel="" target="">
 <span class="menu-text">Stochastic Control</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../multi-agent-systems/index.html" rel="" target="">
 <span class="menu-text">Multi-Agent Systems</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
    <a href="https://github.com/adityam/stochastic-control" rel="" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../mdps/intro.html">MDPs</a></li><li class="breadcrumb-item"><a href="../mdps/intro.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite horizon MDPs</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About the course</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Stochastic Optimization</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stochastic-optimization/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../stochastic-optimization/newsvendor.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The newsvendor problem</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">MDPs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/intro.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite horizon MDPs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/gambling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimal gambling</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/inventory-management.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Inventory Management</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/monotone-mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Monotonicity of value function and optimal policies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/power-delay-tradeoff.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Power-delay tradeoff in wireless communication</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/reward-shaping.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Reward Shaping</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/inf-horizon.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Infinite horizon MDPs</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/mdp-algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">MDP algorithms</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/inventory-management-revisited.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Inventory management (revisted)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/mobile-edge-computing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Service Migration in Mobile edge computing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/computational-complexity-vi.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Computational complexity of value interation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/linear-programming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Linear programming formulation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../mdps/lipschitz-mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Lipschitz MDPs</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">POMDPs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pomdps/intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../pomdps/sequential-hypothesis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Sequential hypothesis testing</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Approx DP</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../approx-mdps/approx-DP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Approximate dynamic programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../approx-mdps/policy-loss.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Upper bounds on policy loss</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../approx-mdps/model-approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Model approximation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Risk sensitive MDPs</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../risk-sensitive/risk-sensitive-utility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Risk Sensitive Utility</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../risk-sensitive/risk-sensitive-mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Risk Sensitive MDPs</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">LQ systems</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">RL</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../rl/stochastic-approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Stochastic approximation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true">
 <span class="menu-text">Probability Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-8" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-8" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability/convergence.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Convergence of random variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability/sub-gaussian.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Sub-Gaussian random variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability/change-of-measure.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Change of Measure</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability/martingales.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Martingales</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../probability/stochastic-stability.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Stochastic stability</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true">
 <span class="menu-text">Linear Algebra Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-9" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-9" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../linear-algebra/postive-definite-matrix.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">Positive definite matrices</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../linear-algebra/svd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Singular value decomposition</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../linear-algebra/rkhs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Reproducing Kernel Hilbert Space</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true">
 <span class="menu-text">Convexity Appendix</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-10" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-10" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../convexity/convexity.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Convex sets and convex functions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../convexity/duality.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Duality</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="true">
 <span class="menu-text">Assignments</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-11" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-11" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../assignments/01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Assignment 1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#performance" id="toc-performance" class="nav-link active" data-scroll-target="#performance"><span class="header-section-number">3.1</span> Performance of Markov strategies</a></li>
  <li><a href="#DP" id="toc-DP" class="nav-link" data-scroll-target="#DP"><span class="header-section-number">3.2</span> Dynamic Programming Decomposition</a></li>
  <li><a href="#variations-of-a-theme" id="toc-variations-of-a-theme" class="nav-link" data-scroll-target="#variations-of-a-theme"><span class="header-section-number">3.3</span> Variations of a theme</a>
  <ul class="collapse">
  <li><a href="#cost-depends-on-next-state" id="toc-cost-depends-on-next-state" class="nav-link" data-scroll-target="#cost-depends-on-next-state"><span class="header-section-number">3.3.1</span> Cost depends on next state</a></li>
  <li><a href="#discounted-cost" id="toc-discounted-cost" class="nav-link" data-scroll-target="#discounted-cost"><span class="header-section-number">3.3.2</span> Discounted cost</a></li>
  <li><a href="#multiplicative-cost" id="toc-multiplicative-cost" class="nav-link" data-scroll-target="#multiplicative-cost"><span class="header-section-number">3.3.3</span> Multiplicative cost</a></li>
  <li><a href="#exponential-cost" id="toc-exponential-cost" class="nav-link" data-scroll-target="#exponential-cost"><span class="header-section-number">3.3.4</span> Exponential cost function</a></li>
  <li><a href="#optimal-stopping" id="toc-optimal-stopping" class="nav-link" data-scroll-target="#optimal-stopping"><span class="header-section-number">3.3.5</span> Optimal stopping</a></li>
  <li><a href="#minimax-setup" id="toc-minimax-setup" class="nav-link" data-scroll-target="#minimax-setup"><span class="header-section-number">3.3.6</span> Minimax setup</a></li>
  </ul></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/adityam/stochastic-control/edit/quarto/mdps/intro.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Finite horizon MDPs</span></h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="http://www.cim.mcgill.ca/~adityam">Aditya Mahajan</a> </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="http://www.mcgill.ca/ece">
            McGill University
            </a>
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Updated</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 31, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Markov decision processes (MDP) are the simplest model of a stochastic control system. The dynamic behavior of an MDP is modeled by an equation of the form <span class="math display">\[ \begin{equation}
  S_{t+1} = f_t(S_t, A_t, W_t) \label{eq:state}
\end{equation}\]</span> where <span class="math inline">\(S_t \in \ALPHABET S\)</span> is the state, <span class="math inline">\(A_t \in \ALPHABET A\)</span> is the control input, and <span class="math inline">\(W_t \in \ALPHABET W\)</span> is the noise. An agent/controller observes the state and chooses the control input <span class="math inline">\(A_t\)</span>.</p>
<p>Eq.&nbsp;\eqref{eq:state} is a <em>non-linear</em> <em>τtochastic</em> state-space model—<em>non-linear</em> because <span class="math inline">\(f_t\)</span> can be any nonlinear function; <em>τtochastic</em> because the system is driven by stochastic noise <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span>.</p>
<p>The controller can be as sophisticated as we want. In principle, it can analyze the entire history of observations and control actions to choose the current control action. Thus, the control action can be written as <span class="math display">\[ A_t = π_t(S_{1:t}, A_{1:t-1}),\]</span> where <span class="math inline">\(S_{1:t}\)</span> is a shorthand for <span class="math inline">\((S_1, \dots, S_t)\)</span> and a similar interpretation holds for <span class="math inline">\(A_{1:t-1})\)</span>. The function <span class="math inline">\(π_t\)</span> is called the <em>control law</em> (or <em>policy</em> or <em>τtrategy</em>) at time <span class="math inline">\(t\)</span>.</p>
<p>At each time, the system incurs a cost that may depend on the current state and control action. This cost is denoted by <span class="math inline">\(c_t(S_t, A_t)\)</span>. The system operates for a time horizon <span class="math inline">\(T\)</span>. During this time, it incurs a total cost <span class="math display">\[ \sum_{t=1}^T c_t(S_t, A_t). \]</span></p>
<p>The initial state <span class="math inline">\(S_1\)</span> and the noise process <span class="math inline">\(\{W_t\}_{t \ge 1}\)</span> are random variables defined on a common probability space (these are called <em>primitive random variables</em>) and are mutually independent. This seemingly benign assumption is critical for the theory that we present to go through.</p>
<p>Suppose we have to design such a controller. We are told the probability distribution of the initial state and the noise. We are also told the system update functions <span class="math inline">\((f_1, \dots, f_T)\)</span> and the cost functions <span class="math inline">\((c_1, \dots, c_T)\)</span>. We are asked to choose a <em>control strategy</em> <span class="math inline">\(π = (π_1, \dots, π_T)\)</span> to minimize the expected total cost <span class="math display">\[ J(π) := \EXP\bigg[ \sum_{t=1}^T c_t(S_t, A_t) \bigg]. \]</span> How should we proceed?</p>
<p>At first glance, the problem looks intimidating. It appears that we have to design a very sophisticated controller: one that analyzes all past data to choose a control input. However, this is not the case. A remarkable result is that the optimal control station can discard all past data and choose the control input based only on the current state of the system. Formally, we have the following:</p>
<div id="thm-MDP-markov" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 </strong></span>Optimality of Markov strategies. For the system model described above, there is no loss of optimality in chosing the control action according to <span class="math display">\[ A_t = π_t(S_t), \quad t=1, \dots, T.\]</span> Such a control strategy is called a <em>Markov strategy</em>.</p>
</div>
<p>The above result claims that the cost incurred by the best Markovian strategy is the same as the cost incurred by the best history dependent strategy. This appears to be a tall claim, so lets see how we can prove it. The main idea of the proof is to repeatedly apply <a href="../stochastic-optimization/intro.html#blackwells-principle-of-irrelevant-information">Blackwell’s principle of irrelevant information</a> <span class="citation" data-cites="Blackwell1964">(<a href="../references.html#ref-Blackwell1964" role="doc-biblioref">Blackwell 1964</a>)</span></p>
<div id="lem-MDP-two-step-lemma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3.1 (Two-Step Lemma) </strong></span>Consider an MDP that operates for two steps (<span class="math inline">\(T=2\)</span>). Then there is no loss of optimality in restricting attention to a Markov control strategy at time <span class="math inline">\(t=2\)</span>.</p>
</div>
<p>Note that <span class="math inline">\(π_1\)</span> is Markov because it can only depend <span class="math inline">\(S_1\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Fix <span class="math inline">\(π_1\)</span> and look at the problem of optimizing <span class="math inline">\(π_2\)</span>. The total cost is <span class="math display">\[ \EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) ]\]</span> The choice of <span class="math inline">\(π_2\)</span> does not influence the first term. So, for a fixed <span class="math inline">\(π_1\)</span>, minimizing the total cost is the equivalent to minimizing the second term. Now, from Blackwell’s principle of irrelevant information, there exists a <span class="math inline">\(π_2^* \colon S_2 \mapsto A_2\)</span> such that for any <span class="math inline">\(π_2\)</span> <span class="math display">\[\EXP[c_2(S_2, π_2^*(S_2) ] \le \EXP[c_2(S_2, π_2(S_{1:2}, A_2) ].\]</span></p>
</div>
</div>
</div>
<div id="lem-three-step-lemma" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3.2 (Three-Step Lemma) </strong></span>Consider an MDP that operates for three steps (<span class="math inline">\(T=3\)</span>). Assume that the control law <span class="math inline">\(π_3\)</span> at time <span class="math inline">\(t=3\)</span> is Markov, i.e., <span class="math inline">\(A_3 = π_3(S_3)\)</span>. Then, there is no loss of optimality in restricting attention to Markov control law at time <span class="math inline">\(t=2\)</span>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Fix <span class="math inline">\(π_1\)</span> and <span class="math inline">\(π_3\)</span> and look at optimizing <span class="math inline">\(π_2\)</span>. The total cost is <span class="math display">\[ \EXP[ c_1(S_1, π_1(S_1)) + c_2(S_2, π_2(S_{1:2}, A_1)) + c_3(S_3, π_3(S_3)].\]</span></p>
<p>The choice of <span class="math inline">\(π_2\)</span> does not affect the first term. So, for a fixed <span class="math inline">\(π_1\)</span> and <span class="math inline">\(π_3\)</span>, minimizing the total cost is the same as minimizing the last two terms. Let us look at the last term carefully. Bu the law of iterated expectations, we have <span class="math display">\[ \EXP[ c_3(S_3, π_3(S_3) ] = \EXP[ \EXP[ c_3(S_3, π_3(S_3)) | S_2, A_2 ] ]. \]</span> Now, <span class="math display">\[\begin{align*}
  \EXP[ c_3(S_3, π_3(S_3)) | S_2 = s_2, A_2 = a_2 ] &amp;=
  \sum_{s_3 \in \ALPHABET S} c_3(s_3, π_3(s_3)) \\
  &amp;= \PR( w_2 \in \ALPHABET W : f_2(s_2, a_2, w_2) = s_3 )
  \\
  &amp;=: h_2(s_2, a_2).
\end{align*}\]</span> The key point is that <span class="math inline">\(h_2(s_2, a_2)\)</span> does not depend on <span class="math inline">\(π_1\)</span> or <span class="math inline">\(π_2\)</span>.</p>
<p>Thus, the total expected cost affected by the choice of <span class="math inline">\(π_2\)</span> can be written as <span class="math display">\[\begin{align*}
  \EXP[ c_2(S_2, A_2) + c_3(S_3, A_3) ] &amp;= \EXP[ c_2(S_2, A_2) + h_2(S_2, A_2)
  ] \\
  &amp;=: \EXP[ \tilde c_2(S_2, A_2) ].
\end{align*}\]</span> Now, by Blackwell’s principle of irrelevant information, there exists a <span class="math inline">\(π_2^* : S_2 \mapsto A_2\)</span> such that for any <span class="math inline">\(π_2\)</span>, we have <span class="math display">\[ \EXP[ \tilde c_2(S_2, π_2^*(S_2))] \le  \EXP[ \tilde c_2(S_2, π_2(S_{1:2},
A_1) ].\]</span></p>
</div>
</div>
</div>
<p>Now we have enough background to present the proof of optimality of Markov strategies.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of optimality of Markov strategies
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The main idea is that any system can be thought of as a two- or three-step system by aggregating time. Suppose that the system operates for <span class="math inline">\(T\)</span> steps. It can be thought of as a two-step system where <span class="math inline">\(t \in \{1, \dots, T - 1\}\)</span> corresponds to step 1 and <span class="math inline">\(t = T\)</span> corresponds to step 2. From the two-step lemma, there is no loss of optimality in restricting attention to Markov control law at step 2 (i.e., at time <span class="math inline">\(t=T\)</span>), i.e., <span class="math display">\[ A_T = π_T(S_T). \]</span></p>
<p>Now consider a system where we are using a Markov strategy at time <span class="math inline">\(t=T\)</span>. This system can be thought of as a three-step system where <span class="math inline">\(t \in \{1, \dots, T-2\}\)</span> corresponds to step 1, <span class="math inline">\(t = T-1\)</span> corresponds to step 2, and <span class="math inline">\(t=T\)</span> corresponds to step 3. Since the controller at time <span class="math inline">\(T\)</span> is Markov, the assumption of the three step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time <span class="math inline">\(t=T-1\)</span>), i.e., <span class="math display">\[A_{T-1} = π_{T-1}(S_{T-1}).\]</span></p>
<p>Now consider a system where we are using a Markov strategy at time <span class="math inline">\(t \in \{T-1, T\}\)</span>. This can be thought of as a three-step system where <span class="math inline">\(t \in \{1, \dots, T - 3\}\)</span> correspond to step 1, <span class="math inline">\(t = T-2\)</span> correspond to step 2, and <span class="math inline">\(t \in \{T-1, T\}\)</span> correspond to step 3. Since the controllers at time <span class="math inline">\(t \in \{T-1, T\}\)</span> are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at step 2 (i.e., at time <span class="math inline">\(t=T-2\)</span>), i.e., <span class="math display">\[A_{T-2} = π_{T-2}(S_{T-2}).\]</span></p>
<p>Proceeding this way, we continue to think of the system as a three step system by different relabeling of time. Once we have shown that the controllers at times <span class="math inline">\(t \in \{s+1, s+2, \dots, T\}\)</span> are Markov, we relabel time as follows: <span class="math inline">\(t=\{1, \dots, s-1\}\)</span> corresponds to step 1, <span class="math inline">\(t = s\)</span> corresponds to step 2, and <span class="math inline">\(t \in \{s+1, \dots, T\}\)</span> corresponds to step 3. Since the controllers at time <span class="math inline">\(t \in \{s+1, \dots, T\}\)</span> are Markov, the assumption of the three-step lemma is satisfied. Thus, by that lemma, there is no loss of optimality in restricting attention to Markov controllers at stage 2 (i.e.&nbsp;at time <span class="math inline">\(s\)</span>), i.e., <span class="math display">\[A_τ = π_τ(S_τ).\]</span></p>
<p>Proceeding until <span class="math inline">\(s=2\)</span>, completes the proof.</p>
</div>
</div>
</div>
<section id="performance" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="performance"><span class="header-section-number">3.1</span> Performance of Markov strategies</h2>
<p>We have shown that there is no loss of optimality to restrict attention to Markov strategies. One of the advantages of Markov strategies is that their performance can be computed recursively. In particular, given any Markov strategy <span class="math inline">\(π = (π_1, \dots, π_T)\)</span>, define <em>the cost-to-go functions</em> as follows: <span class="math display">\[V^{π}_t(s) = \EXP^π \bigg[ \sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \biggm| S_t =
s\bigg]. \]</span> Note that <span class="math inline">\(V^{π}_t(s)\)</span> only depends on the future strategy <span class="math inline">\((π_t, \dots, π_T)\)</span>. These functions can be computed recursively as follows: <span class="math display">\[\begin{align*}
  V^{π}_t(s) &amp;= \EXP^π \bigg[ \sum_{τ = t}^{T} c_τ(S_τ, π_τ(S_τ)) \biggm| S_t =
  s \bigg] \\
  &amp;= \EXP^π \bigg[ c_t(s, π_t(s)) + \EXP^π \bigg[ \sum_{τ = t+1}^T
    c_τ(S_τ, π_τ(S_τ)) \biggm| S_{t+1} \bigg] \biggm| S_t = s \bigg]
  \\
  &amp;= \EXP^π\big[ c_t(s, π_t(s)) + V_{t+1}(S_{t+1}; π) \big| S_t = s \big].
\end{align*}\]</span></p>
</section>
<section id="DP" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="DP"><span class="header-section-number">3.2</span> Dynamic Programming Decomposition</h2>
<p>Now we are ready to state the main result of MDPs</p>
<div id="thm-MDP-DP" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Dynamic program) </strong></span>Recursive define <em>value functions</em> <span class="math inline">\(\{V_t\}_{t = 1}^{T+1} \colon \ALPHABET S \to \reals\)</span> as follows: <span class="math display">\[ \begin{equation} \label{eq:DP-1}
  V_{T+1}(s) = 0
\end{equation} \]</span> and for <span class="math inline">\(t \in \{T, \dots, 1\}\)</span>: <span class="math display">\[\begin{align}
   Q_t(s,a) &amp;= c(s,a) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a]
   \nonumber \\
   &amp;= c(s,a) + \EXP[ V_{t+1}(f_t(s,a,W_t)) ], \label{eq:DP-2}
\end{align}\]</span> and define <span class="math display">\[ \begin{equation} \label{eq:DP-3}
  V_t(s) = \min_{a \in \ALPHABET A} Q_t(s,a).
\end{equation} \]</span> Then, a Markov policy is optimal if and only if it satisfies <span class="math display">\[ \begin{equation} \label{eq:verification}
  π_t^*(s) = \arg \min_{a \in \ALPHABET A} Q_t(s,a).
\end{equation} \]</span></p>
</div>
<p>Instead of proving the above result, we prove a related result</p>
<div id="thm-comparison-principle" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (The comparison principle) </strong></span>For any Markov strategy <span class="math inline">\(π\)</span> <span class="math display">\[ V^{π}_t(s) \ge V_t(s) \]</span> with equality at <span class="math inline">\(t\)</span> if and only if the <em>future strategy</em> <span class="math inline">\(π_{t:T}\)</span> satisfies the verification step \eqref{eq:verification}.</p>
</div>
<p>Note that the comparison principle immediately implies that the strategy obtained using dynamic programming is optimal.</p>
<p>The comparison principle also allows us to interpret the value functions. The value function at time <span class="math inline">\(t\)</span> is the minimum of all the cost-to-go functions over all future strategies. The comparison principle also allows us to interpret the optimal policy (the interpretation is due to Bellman and is colloquially called Bellman’s principle of optimality).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Bellman’s principle of optimality.
</div>
</div>
<div class="callout-body-container callout-body">
<p>An optimal policy has the property that whatever the initial state and the initial decisions are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Proof of the comparison principle
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The proof proceeds by backward induction. Consider any Markov strategy <span class="math inline">\(π = (π_1, \dots, π_T)\)</span>. For <span class="math inline">\(t = T\)</span>, <span class="math display">\[ \begin{align*}
  V_T(s) &amp;= \min_{a \in \ALPHABET A} Q_T(s,a) \\
  &amp;\stackrel{(a)}= \min_{a \in \ALPHABET A} c_T(s,a) \\
  &amp;\stackrel{(b)}\le c_T(s, π_T(s)) \\
  &amp;\stackrel{(c)}= V^{π}_T(s),
\end{align*} \]</span> where <span class="math inline">\((a)\)</span> follows from the definition of <span class="math inline">\(Q_T\)</span>, <span class="math inline">\((b)\)</span> follows from the definition of minimization, and <span class="math inline">\((c)\)</span> follows from the definition of <span class="math inline">\(J_T\)</span>. Equality holds in <span class="math inline">\((b)\)</span> iff the policy <span class="math inline">\(π_T\)</span> is optimal. This result forms the basis of induction.</p>
<p>Now assume that the statement of the theorem is true for <span class="math inline">\(t+1\)</span>. Then, for <span class="math inline">\(t\)</span> <span class="math display">\[ \begin{align*}
  V_t(s) &amp;= \min_{a \in \ALPHABET A} Q_t(s,a) \\
  &amp;\stackrel{(a)}= \min_{a \in \ALPHABET A} \Big\{
  c_t(s,a) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a]
  \Big\}
  \\
  &amp;\stackrel{(b)}\le  \Big\{
  c_t(s,π_t(s)) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = π_t(s)]
  \Big\} \\
  &amp;\stackrel{(c)}\le  \Big\{
  c_t(s,π_t(s)) + \EXP[ J_{t+1}(S_{t+1}; π) | S_t = s, A_t = π_t(s)]
  \Big\} \\
  &amp;\stackrel{(d)}= V^{π}_t(s),
\end{align*} \]</span> where <span class="math inline">\((a)\)</span> follows from the definition of <span class="math inline">\(Q_t\)</span>, <span class="math inline">\((b)\)</span> follows from the definition of minimization, <span class="math inline">\((c)\)</span> follows from the induction hypothesis, and <span class="math inline">\((d)\)</span> follows from the definition of <span class="math inline">\(J_t\)</span>. We have equality in step <span class="math inline">\((b)\)</span> iff <span class="math inline">\(π_t\)</span> satisfies the verification step \eqref{eq:verification} and have equality in step <span class="math inline">\((c)\)</span> iff <span class="math inline">\(π_{t+1:T}\)</span> is optimal (this is part of the induction hypothesis). Thus, the result is true for time <span class="math inline">\(t\)</span> and, by the principle of induction, is true for all time.</p>
</div>
</div>
</div>
</section>
<section id="variations-of-a-theme" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="variations-of-a-theme"><span class="header-section-number">3.3</span> Variations of a theme</h2>
<section id="cost-depends-on-next-state" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="cost-depends-on-next-state"><span class="header-section-number">3.3.1</span> Cost depends on next state</h3>
<p>In the basic model that we have considered above, we assumed that the per-step cost depends only on the current state and current actions. In some applications, such as the <a href="inventory-management.html">inventory management</a> model considered in class, it is more natural to have a cost function where the cost depends on the current state, current action, and the next state. Conceptually, such problems can be treated in the same way as the standard model.</p>
<p>In particular, suppose we have a per-step cost given by <span class="math inline">\(c_t(S_t,A_t,S_{t+1})\)</span>, where the objective is to minimize <span class="math display">\[ J(π) = \EXP\Bigl[ \sum_{t=1}^T c_t(S_t, A_t, S_{t+1}) \Bigr]. \]</span></p>
<p>Define <span class="math display">\[ \tilde c_t(s, a) = \EXP[ c_t(s, a, S_{t+1}) | S_t = s, A_t = a ]
= \EXP[ c_t(s,a, f_t(s,a, W_t) ]. \]</span> Then, by the towering property of conditional expectation, we can write</p>
<p><span class="math display">\[ \begin{align*}
J(π) &amp;= \EXP\Bigl[ \sum_{t=1}^T \EXP[ c_t(S_t, A_t, S_{t+1}) | S_t, A_t] \Bigr] \\
&amp;= \EXP\Bigl[ \sum_{t=1}^T \tilde c_t(S_t, A_t) \Bigr].
\end{align*} \]</span></p>
<p>Thus, we can equivalently consider this as our standard model with the per-step cost given by <span class="math inline">\(\tilde c_t(S_t, A_t)\)</span>. We can write the recursive step of the dynamic program as follows: <span class="math display">\[ Q_t(s,a) = \EXP[ c_t(s,a, S_{t+1}) + V_{t+1}(S_{t+1}) | S_t = s, A_t = a ].\]</span></p>
<p>For numerically solving the dynamic program when the cost is time-homogeneous (i.e., does not depend on <span class="math inline">\(t\)</span>), it is more efficient to compute <span class="math inline">\(\tilde c\)</span> once and recuse that in the dynamic program recursion.</p>
</section>
<section id="discounted-cost" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="discounted-cost"><span class="header-section-number">3.3.2</span> Discounted cost</h3>
<p>In some applications, it is common to consider a discounted expected cost given by <span class="math display">\[ J(π) = \EXP\Bigl[ \sum_{t=1}^T γ^{t-1} c_t(S_t, A_t) \Bigr] \]</span> where <span class="math inline">\(γ \in (0,1)\)</span> is called the discount factor.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Discount factor
</div>
</div>
<div class="callout-body-container callout-body">
<p>There are two interpretations of the discount factor <span class="math inline">\(γ\)</span>. The first interpretation is an economic interpretation to determine the <em>present value</em> of a utility that will be received in the future. For example, suppose a decision maker is indifferent between receiving 1 dollar today or <span class="math inline">\(s\)</span> dollars tomorrow. This means that the decision maker discounts the future at a rate <span class="math inline">\(1/s\)</span>, so <span class="math inline">\(γ = 1/s\)</span>.</p>
<p>The second interpretation is that of an absorbing state. Suppose we are operating a machine that generates a value of $1 each day. However, there is a probability <span class="math inline">\(p\)</span> that the machine will break down at the end of the day. Thus, the expected return for today is $1 while the expected return for tomorrow is <span class="math inline">\((1-p)\)</span> (which is the probability that the machine is still working tomorrow). In this case, the discount factor is defined as <span class="math inline">\((1-p)\)</span>. See <span class="citation" data-cites="Shwartz2001">Shwartz (<a href="../references.html#ref-Shwartz2001" role="doc-biblioref">2001</a>)</span> for a detailed discussion of this alternative.</p>
</div>
</div>
<p>The recursive step of the dynamic program for such models can be written as <span class="math display">\[ Q_t(s,a) = c_t(s,a) + γ \, \EXP[ V_{t+1}( S_{t+1}) | S_t = s, A_t = a ].\]</span></p>
</section>
<section id="multiplicative-cost" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="multiplicative-cost"><span class="header-section-number">3.3.3</span> Multiplicative cost</h3>
<p>So far, we have assumed that the cost is additive. The dynamic proramming decomposition also works for models with multiplicative cost. In particular, suppose that the performance of any policy is given by <span class="math display">\[ J(π) = \EXP\Bigl[ \prod_{t=1}^T c_t(S_t, A_t) \Bigr] \]</span> where the per-step cost function is positive. Then, it can be shown that the optimal policy is given by the following dynamic program.</p>
<div id="prp-DP-multiplicative" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.1 (Dynamic Program for multiplicative cost) </strong></span>Initialize <span class="math inline">\(V_{T+1}(s) = 1\)</span> and recursively compute <span class="math display">\[ \begin{align*}
Q_t(s,a) &amp;= c_t(s,a) \EXP[ V_{t+1}(S_{t+1}) | S_t = s, A_t = a ], \\
V_t(s) &amp;= \min_{a \in \ALPHABET A} Q_t(s,a).
\end{align*} \]</span></p>
</div>
</section>
<section id="exponential-cost" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="exponential-cost"><span class="header-section-number">3.3.4</span> Exponential cost function</h3>
<p>A special class of multiplicative cost function is exponential of sum: <span class="math display">\[J(π) = \EXP\Bigl[ \exp\Bigl( \theta \sum_{t=1}^T c_t(S_t, A_t) \Bigr) \Bigr]. \]</span></p>
<p>When <span class="math inline">\(\theta &gt; 0\)</span>, the above captures risk-averse preferences and when <span class="math inline">\(\theta &lt; 0\)</span>, it corresponds to risk-seeking preferences. This is equivalent to a multiplicative cost <span class="math display">\[J(π) = \EXP\Bigl[ \prod_{t=1}^T \exp( \theta c_t(S_t, A_t)) \Bigr]. \]</span> Therefore, the dynamic program for multiplicative cost is also applicable for this model.</p>
</section>
<section id="optimal-stopping" class="level3" data-number="3.3.5">
<h3 data-number="3.3.5" class="anchored" data-anchor-id="optimal-stopping"><span class="header-section-number">3.3.5</span> Optimal stopping</h3>
<p>Let <span class="math inline">\(\{S_t\}_{t \ge 1}\)</span> be a Markov chain. At each time <span class="math inline">\(t\)</span>, a decision maker observes the state <span class="math inline">\(S_t\)</span> of the Markov chain and decides whether to continue or stop the process. If the decision maker decides to continue, he incurs a <em>continuation cost</em> <span class="math inline">\(c_t(S_t)\)</span> and the state evolves. If the DM decides to stop, he incurs a <em>τtopping cost</em> of <span class="math inline">\(d_t(S_t)\)</span> and the problem is terminated. The objective is to determine an optimal <em>τtopping time</em> <span class="math inline">\(\tau\)</span> to minimize <span class="math display">\[J(\tau) := \EXP\bigg[ \sum_{t=1}^{\tau-1} c_t(S_t) + d_\tau(S_\tau)
\bigg].\]</span></p>
<p>Such problems are called <em>Optimal stopping problems</em>.</p>
<p>Define the <em>cost-to-go function</em> of any stopping rule as <span class="math display">\[V^{\tau}_t(s) = \EXP\bigg[ \sum_{τ = t}^{\tau - 1} c_{\tau}(S_t) +
d_\tau(S_\tau) \,\bigg|\, \tau &gt; t \bigg]\]</span> and the <em>value function</em> as <span class="math display">\[V_t(s) = \inf_{\tau} V^{\tau}_t(s). \]</span> Then, it can be shown that the value functions satisfy the following recursion:</p>
<div id="prp-DP-stopping" class="theorem proposition">
<p><span class="theorem-title"><strong>Proposition 3.2 </strong></span><strong>Dynamic Program for optimal stopping</strong> <span class="math display">\[ \begin{align*}
V_T(s) &amp;= s_T(s) \\
V_t(s) &amp;= \min\{ s_t(s), c_t(s) + \EXP[ V_{t+1}(S_{t+1}) | S_t = s].
\end{align*}\]</span></p>
</div>
<p>For more details on the optimal stopping problems, see <span class="citation" data-cites="Ferguson:book">Ferguson (<a href="../references.html#ref-Ferguson:book" role="doc-biblioref">2008</a>)</span>.</p>
</section>
<section id="minimax-setup" class="level3" data-number="3.3.6">
<h3 data-number="3.3.6" class="anchored" data-anchor-id="minimax-setup"><span class="header-section-number">3.3.6</span> Minimax setup</h3>
<p><em>To be written</em></p>
</section>
</section>
<section id="notes" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="notes">Notes</h2>
<p>The proof idea for the optimality of Markov strategies is based on a proof by <span class="citation" data-cites="Witsenhausen1979">Witsenhausen (<a href="../references.html#ref-Witsenhausen1979" role="doc-biblioref">1979</a>)</span> on the structure of optimal coding strategies for real-time communication. Note that the proof does not require us to find a dynamic programming decomposition of the problem. This is in contrast with the standard textbook proof where the optimality of Markov strategies is proved as part of the dynamic programming decomposition.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list" style="display: none">
<div id="ref-Blackwell1964" class="csl-entry" role="listitem">
<span class="smallcaps">Blackwell, D.</span> 1964. Memoryless strategies in finite-stage dynamic programming. <em>The Annals of Mathematical Statistics</em> <em>35</em>, 2, 863–865. DOI: <a href="https://doi.org/10.1214/aoms/1177703586">10.1214/aoms/1177703586</a>.
</div>
<div id="ref-Ferguson:book" class="csl-entry" role="listitem">
<span class="smallcaps">Ferguson, T.S.</span> 2008. Optimal stopping and applications. Available at: <a href="http://www.math.ucla.edu/~tom/Stopping/Contents.html">http://www.math.ucla.edu/~tom/Stopping/Contents.html</a>.
</div>
<div id="ref-Shwartz2001" class="csl-entry" role="listitem">
<span class="smallcaps">Shwartz, A.</span> 2001. Death and discounting. <em><span>IEEE</span> Transactions on Automatic Control</em> <em>46</em>, 4, 644–647. DOI: <a href="https://doi.org/10.1109/9.917668">10.1109/9.917668</a>.
</div>
<div id="ref-Witsenhausen1979" class="csl-entry" role="listitem">
<span class="smallcaps">Witsenhausen, H.S.</span> 1979. On the structure of real-time source coders. <em>Bell System Technical Journal</em> <em>58</em>, 6, 1437–1451.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../stochastic-optimization/newsvendor.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">The newsvendor problem</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../mdps/gambling.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Optimal gambling</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>