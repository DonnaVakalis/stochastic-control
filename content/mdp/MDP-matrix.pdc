---
title: "Numerics: Matrix formulation of Markov decision processes"
weight: 14
categories:
  - MDP
tags:
  - numerical methods
scripts:
  - math
---

![Image credit: https://etc.usf.edu/clipart/28400/28480/young_boy_28480.htm][calculations]

[calculations]: /stochastic-control/banners/slate.gif "Shut up and
compute" { width=100% style='max-width:30em;'}

# MDPs as controlled Markov chains

In this section, we present a matrix formulation for finite state finite
action MDPs, which is useful for computing the solutions numerically. 
Let's start with [the function model][MDP] described earlier and assume that
$\ALPHABET S$ and $\ALPHABET A$ are finite sets and that the cost function and
the probability distribution of $\{W_t\}_{t \ge 1}$ are time-homogeneous.
Then, the following is a fundamental property of MDPs:

[MDP]: ../mdp-functional

::: highlight :::
Lemma

:   For any $s_1, s_2, \dots, s_T \in \ALPHABET S$ and $a_1, \dots, a_T \in
    \ALPHABET A$, we have
    $$ \PR(S_{t+1} = s_{t+1} | S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t})
     = \PR(S_{t+1} = s_{t+1} | S_{t}   = s_t    , A_t = a_t).$$
    Moreover, the right hand side does not depend on the policy $π$.
:::

#### Proof {-}
For a given policy $π$ consider
$$\begin{align*}
  \hskip 2em & \hskip -2em
  \PR(S_{t+1} = s_{t+1} | S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t})
  \\
  &\stackrel{(a)}=
  \PR(\{ w_t \in \ALPHABET W : f_t(s_t, a_t, w_t) = s_{t+1} \}
  | S_{1:t} = s_{1:t}, A_{1:t} = a_{1:t}) 
  \\
  &\stackrel{(b)}=
  \PR(\{ w_t \in \ALPHABET W : f_t(s_t, a_t, w_t) = s_{t+1} \})
  \\
  &=
  \PR(S_{t+1} = s_{t+1} | S_{t} = s_{t}, A_{t} = a_{t})
\end{align*}$$
where $(a)$ follows from the update equation for $S_{t+1}$ and $(b)$ follows
because the primitive random variables are independent. The expression in the
right hand side of $(b)$ does not depend on the control strategy $π$. 

# Matrix formulation of an MDP

Let $|\ALPHABET S| = n$ and $|\ALPHABET A| = m$. The idea behind the matrix
formulation of MDP is to think of $\{S_t\}_{t \ge 1}$ as a _controlled_ Markov
process and define the $n \times n$ tranistion matrices $\{P(a)\}_{a \in
\ALPHABET A}$ as follows:
$$ P_{ss'}(a) = \PR(S_{t+1} = s' | S_t = s, A_t = a).$$

Observe that
$$\EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a] 
  = \sum_{s' \in \ALPHABET S} V_{t+1}(s') \PR(S_{t+1} = s' | S_t = s, A_t = a).
  $$
Now, if we think of $V_t$ as a $n \times 1$ column vector, then we can write
$$ \EXP[V_{t+1}(S_{t+1}) | S_t = s, A_t = a] = [ P(a)V_{t+1} ]_{s}, $$
or, equivalently,
$$ \MATRIX{ \EXP[V_{t+1}(S_{t+1}) | S_t = 1, A_t = a] \\ \vdots \\
    \EXP[V_{t+1}(S_{t+1}) | S_t = n, A_t = a] }
    = P(a)V_{t+1}. $$

Now, think of $c_t$ and $Q_t$ as $n \times m$ matrices. Then, we can write
$$ Q_t = c_t + \MATRIX { P(1)V_{t+1} & \cdots & P(n) V_{t+1} }. $$

Define $\bar P = \MATRIX{P(1) \\ \vdots \\ P(n)}$, then 
$$ Q_t = c_t + \text{reshape}( \bar P \, V_{t+1}, n, m). $$
Hence, the key step of dynamic programming can be done using simple linear
algebra and therefore implemented efficiently in any programming language. 

# An Example: Machine repair

Consider a machine which can be in one of $n$ states $\ALPHABET S = \{1,
\dots, n\}$, where state $1$ denotes the best state and state $n$ denotes the
worse state. 

There are two actions $\ALPHABET A = \{1, 2\}$, where $a=1$ means that the
current machine is replaced by a brand new one (which starts in state $1$)
and $a=2$ means that the existing machine is used. When action $a=2$ is
chosen, the machine state will either remain the same (with probability $1
- θ$) or deteriorate by one unit (with probability $θ$). 

For example, if $n = 3$, then 
$$ P(1) = \MATRIX{1 & 0 & 0\\ 1 & 0 & 0\\ 1 & 0 & 0},
\qquad
P(2) = \MATRIX{1-θ &  θ &  0\\ 0 &  1-θ & θ\\ 0 &  0 &  1}. $$

Now suppose that the replace action $a = 1$ costs $λ$ unit (irrespective of
the state) and the do nothing action $a = 2$ costs $0$ in state $1$, $1$ in
state $2$, and $5$ in state $3$. 
$$ c = \MATRIX{λ & 0 \\ λ & 1 \\ λ & 5 }. $$

Then, the value functions and optimal policy for $T = 4$ and specific values of
$θ$ and $λ$ are shown below. For visualization we show the replace action by
$\bullet$ and the do thing action by $\circ$. You can play around with the
parameters $θ = $
<input id="theta" 
       type="number" step = 0.1
       maxlength="3" size="3"
       min="0" max="1"
       onkeyup="showMatrix()"
       onchange="showMatrix()"
       value="" />
and
$λ = $
<input id="lambda" 
       type="number" step = 1
       maxlength="1" size="2"
       min="0" max="20"
       onkeyup="showMatrix()"
       onchange="showMatrix()"
       value="" />
<button onClick="showMatrix()">Update</button>
<p id="machine-repair"></p>

<script>
if (pagedata == undefined) { var pagedata = { } }

pagedata.machineRepair = { theta : 0.3, lambda : 8 } 

// Set initial values
document.getElementById('theta').value  = pagedata.machineRepair.theta;
document.getElementById('lambda').value = pagedata.machineRepair.lambda;

let promise = Promise.resolve();  // Used to hold chain of typesetting calls

function typeset(code) {
  promise = promise.then(() => {code(); return MathJax.typesetPromise()})
                   .catch((err) => console.log('Typeset failed: ' + err.message));
  return promise;
}

function showMatrix(){
  var theta  = document.getElementById('theta').valueAsNumber;
  var lambda = document.getElementById('lambda').valueAsNumber;

  const P1 = math.matrix([ [1,0,0], [1,0,0], [1,0,0] ]);
  const P2 = math.matrix([ [1-theta, theta, 0], [0, 1-theta, theta], [0, 0, 1] ]);
  const P  = math.concat(P1, P2, 0);

  const c  = math.matrix([[lambda, 0], [lambda, 1], [lambda, 5]]);

  const [n, m] = c.size();
  const T  = 4;
  const V  = [];
  const π  = [];

  V[T+1] = math.zeros(n)
  for (var t = T; t > 0; t--) {
    // math.reshape behaves differently from Julia. It reshapes in 
    // row major format rather than column major. So, we need an extra transpose

    var Q = math.add(c, math.transpose(math.reshape( math.multiply(P, V[t+1]), [m,n] )))
            .toArray();
            
    // There is no argmin function in the math.js library
    // Since there are only two actions, we make the comparison by hand
    π[t] = [];
    V[t] = [];
    for(var s = 0; s < n; s++) {
       if(Q[s][0] < Q[s][1]) {
          V[t][s] = Q[s][0];
          π[t][s] = "\\bullet";
       } else {
          V[t][s] = Q[s][1];
          π[t][s] = "\\circ";
       }
    }
    V[t] = math.matrix(V[t]);
  }

  function VtoTeX(matrix) {
      const arr = matrix.toArray(); // A col vector
      return `\\MATRIX{ ${arr[0].toFixed(2)} \\\\ 
                        ${arr[1].toFixed(2)} \\\\ 
                        ${arr[2].toFixed(2)} }`
  }

  function gtoTeX(arr) {
      return `\\MATRIX{ ${arr[0]} \\\\ 
                        ${arr[1]} \\\\ 
                        ${arr[2]} }`
  }

  typeset(() => { 
      const id = document.getElementById('machine-repair');
      id.innerHTML = `$$\\begin{align*}
        V_4 &= ${VtoTeX(V[4])}, &
        V_3 &= ${VtoTeX(V[3])}, &
        V_2 &= ${VtoTeX(V[2])}, &
        V_1 &= ${VtoTeX(V[1])}. \\\\
        π_4 &= ${gtoTeX(π[4])}, &
        π_3 &= ${gtoTeX(π[3])}, &
        π_2 &= ${gtoTeX(π[2])}, &
        π_1 &= ${gtoTeX(π[1])}. \\\\
       \\end{align*}$$`
      return id;
  })
}

showMatrix()
</script>

Couple of features of the optimal solution are worth noting:

* For $λ$ roughly in the range of $(5,15)$ the optimal policy chooses the replace
  action in state $3$ at time $1$ even though taking the do nothing action
  incurs less instanteous cost. 

* There is a range of values of $λ$ for which the the optimal policy is the
  same. This argument is sometimes used to illustrate why searching in the
  policy space can be more efficient than searching in the value space. 

* It appears that the optimal policy satisifies a _monotonicity_ property: for
  low states take "do nothing" action and for higher states take the "replace"
  action. The actual value of what constitutes a low or a high state depends
  on the parameters of the model. [Later in the course](../monotonicity),
  we will study sufficient conditions to establish monotonicity of optimal
  policy. It is easy to show that the setup described above [satisfies these
  sufficient conditions](../monotonicity/#exercises), so the optimal policy is
  monotone for all choices of model parameters. 
